[
  {
    "objectID": "research/articles/seta-2023/index.html",
    "href": "research/articles/seta-2023/index.html",
    "title": "Synthetic Ethnography: Field Devices for the Qualitative Study of Generative Models",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/seta-2023/index.html#citation",
    "href": "research/articles/seta-2023/index.html#citation",
    "title": "Synthetic Ethnography: Field Devices for the Qualitative Study of Generative Models",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/knuutila-2022/index.html",
    "href": "research/articles/knuutila-2022/index.html",
    "title": "Who Is Afraid of Fake News? Modeling Risk Perceptions of Misinformation in 142 Countries",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/knuutila-2022/index.html#citation",
    "href": "research/articles/knuutila-2022/index.html#citation",
    "title": "Who Is Afraid of Fake News? Modeling Risk Perceptions of Misinformation in 142 Countries",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/knuutila-2021/index.html",
    "href": "research/articles/knuutila-2021/index.html",
    "title": "A Dataset of COVID-Related Misinformation Videos and Their Spread on Social Media",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/knuutila-2021/index.html#citation",
    "href": "research/articles/knuutila-2021/index.html#citation",
    "title": "A Dataset of COVID-Related Misinformation Videos and Their Spread on Social Media",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/herasimenka-2023/index.html",
    "href": "research/articles/herasimenka-2023/index.html",
    "title": "Misinformation and Professional News on Largely Unmoderated Platforms: The Case of Telegram",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/herasimenka-2023/index.html#citation",
    "href": "research/articles/herasimenka-2023/index.html#citation",
    "title": "Misinformation and Professional News on Largely Unmoderated Platforms: The Case of Telegram",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Fall 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Fall 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-1",
    "href": "teaching/index.html#section-1",
    "title": "Teaching",
    "section": "2022–23",
    "text": "2022–23\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2023\n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Comparative Public Administration \n            \n            \n                PMAP 8441 | \n                Georgia State University\n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n            \n                 Spring 2023\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Bayesian Statistics Readings \n            \n            \n                PMAP 8911 | \n                Georgia State University\n            \n            Independent readings course on Bayesian statistics with R and Stan\n            \n                 Spring 2023\n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Introduction to Nonprofits \n            \n            \n                PMAP 3210 | \n                Georgia State University\n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n            \n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2023 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-2",
    "href": "teaching/index.html#section-2",
    "title": "Teaching",
    "section": "2021–22",
    "text": "2021–22\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2022\n                 Fall 2021\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Introduction to Nonprofits \n            \n            \n                PMAP 3210 | \n                Georgia State University\n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n            \n                 Spring 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2022 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2022 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-3",
    "href": "teaching/index.html#section-3",
    "title": "Teaching",
    "section": "2020–21",
    "text": "2020–21\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2021 (asynchronous online)\n                 Fall 2020 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2021 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2021 (asynchronous online)\n                 Spring 2021 (asynchronous online)\n                 Fall 2020 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-4",
    "href": "teaching/index.html#section-4",
    "title": "Teaching",
    "section": "2019–20",
    "text": "2019–20\n\n\n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 May 2020 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2020\n                 Fall 2019\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2020 (asynchronous online)\n                 Fall 2019\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-5",
    "href": "teaching/index.html#section-5",
    "title": "Teaching",
    "section": "2017–19",
    "text": "2017–19\n\n\n    \n        \n            \n        \n        \n            \n                Economy, Society, and Public Policy \n            \n            \n                MPA 612 | \n                Brigham Young University\n            \n            \n            \n                 Winter 2019\n                 Winter 2019 (executive MPA)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Science and Statistics for Public Management \n            \n            \n                MPA 630 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization \n            \n            \n                MPA 635 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Economic Decision Making for Managers \n            \n            \n                MPA 612 | \n                Brigham Young University\n            \n            \n            \n                 Winter 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Telling Stories with Data \n            \n            \n                Bus M 491R | \n                Brigham Young University\n            \n            \n            \n                 Fall 2017\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization \n            \n            \n                MPA 635 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2017\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-6",
    "href": "teaching/index.html#section-6",
    "title": "Teaching",
    "section": "2007–15",
    "text": "2007–15\n\n\n    \n        \n            \n        \n        \n            \n                Stats in Real Life \n            \n            \n                PubPol 590 | \n                Duke University\n                | TA, as PhD student\n            \n            \n            \n                 Fall 2014\n                 Spring 2014\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Policy Analysis \n            \n            \n                PubPol 803/807 | \n                Duke University\n                | TA, as PhD student\n            \n            \n            \n                 Fall 2015\n                 Fall 2013\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Statistical Analysis for Public Administrators \n            \n            \n                PMGT 630 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Summer 2012\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                International Development Field Study \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Winter and Spring 2012\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Decision Modeling and Analysis \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Winter 2012\n                 Fall 2011\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Print Publishing \n            \n            \n                CHum 260 | \n                Brigham Young University\n                | Instructor, as undergraduate\n            \n            \n            \n                 Winter 2008\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                First-year Arabic \n            \n            \n                Arabic 101 | \n                Brigham Young University\n                | Instructor, as undergraduate\n            \n            \n            \n                 Winter 2007\n                 Fall 2006\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research has spans political communication and political communities online, as well as various online harms such as misinformation and abuse directed at politicians. My more recent work is focused on how to work with large datasets, for instance large volumes of images, in the humanities and social sciences."
  },
  {
    "objectID": "research/index.html#articles",
    "href": "research/index.html#articles",
    "title": "Research",
    "section": "Articles",
    "text": "Articles\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "August 15, 2023\n        \n        \n            Manually generate predicted values for logistic regression with matrix multiplication in R\n            \n                \n                    r\n                \n                \n                    statistics\n                \n                \n                    regression\n                \n            \n            This is like basic stats stuff, but I can never remember how to do it—here's how to use matrix multiplication to replicate the results of `predict()`\n             10.59350/qba9a-b3561\n        \n        \n            \n        \n    \n    \n                  \n            August 12, 2023\n        \n        \n            The ultimate practical guide to multilevel multinomial conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models\n             10.59350/2mz75-rrc46\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "August 15, 2023\n        \n        \n            Manually generate predicted values for logistic regression with matrix multiplication in R\n            \n                \n                    r\n                \n                \n                    statistics\n                \n                \n                    regression\n                \n            \n            This is like basic stats stuff, but I can never remember how to do it—here's how to use matrix multiplication to replicate the results of `predict()`\n             10.59350/qba9a-b3561\n        \n        \n            \n        \n    \n    \n                  \n            August 12, 2023\n        \n        \n            The ultimate practical guide to multilevel multinomial conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models\n             10.59350/2mz75-rrc46\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2022",
    "text": "2022\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2021",
    "text": "2021\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2018",
    "text": "2018\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-6",
    "href": "blog/index.html#section-6",
    "title": "Blog",
    "section": "2017",
    "text": "2017\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-7",
    "href": "blog/index.html#section-7",
    "title": "Blog",
    "section": "2016",
    "text": "2016\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-8",
    "href": "blog/index.html#section-8",
    "title": "Blog",
    "section": "2013",
    "text": "2013\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-9",
    "href": "blog/index.html#section-9",
    "title": "Blog",
    "section": "2012",
    "text": "2012\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-10",
    "href": "blog/index.html#section-10",
    "title": "Blog",
    "section": "2011",
    "text": "2011\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-11",
    "href": "blog/index.html#section-11",
    "title": "Blog",
    "section": "2010",
    "text": "2010\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-12",
    "href": "blog/index.html#section-12",
    "title": "Blog",
    "section": "2009",
    "text": "2009\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-13",
    "href": "blog/index.html#section-13",
    "title": "Blog",
    "section": "2007",
    "text": "2007\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "",
    "text": "I recently posted a guide (mostly for future-me) about how to analyze conjoint survey data with R. I explore two different estimands that social scientists are interested in—causal average marginal component effects (AMCEs) and descriptive marginal means—and show how to find them with R, with both frequentist and Bayesian approaches.\nHowever, that post is a little wrong. It’s not wrong wrong, but it is a bit oversimplified.\nWhen political scientists, psychologists, economists, and other social scientists analyze conjoint data, they overwhelmingly do it with ordinary least squares (OLS) regression, or just standard linear regression (lm(y ~ x) in R; reg y x in Stata). Even if the outcome is binary, they’ll use OLS and call it a linear probability model. The main R package for working with conjoint data in a frequentist way ({cregg}) uses OLS and linear probability models. Social scientists (and economists in particular) adore OLS.\nIn my earlier guide, I showed how to analyze the data with logistic regression, but even that is still overly simplified. In reality, conjoint choice-based experiments are more complex than what regular old OLS regression—or even logistic regression—can handle (though I’m sure some econometrician somewhere has a proof showing that OLS works just fine for multinomial conjoint data :shrug:).\nA recent paper published in Political Science Research and Methods (Jensen et al. 2021) does an excellent job explaining the problem with using plain old OLS to estimate AMCEs and marginal means with conjoint data (access the preprint here). Their main argument boils down to this: OLS throws away too much useful information about (1) the relationships and covariance between the different combinations of feature levels offered to respondents, and (2) individual-specific differences in how respondents react to different feature levels.\nJensen et al. explain three different approaches to analyzing data that has a natural hierarchical structure like conjoint data (where lots of choices related to different “products” are nested within individuals). This also is the same argument from chapter 15 of Bayes Rules!.\nBy using a multilevel hierarchical model, Jensen et al. (2021) show that we can still find AMCEs and causal effects, just like in my previous guide, but we can take advantage of the far richer heterogeneity that we get from these complex statements. We can make cool statements like this (in an experiment that varied policies related to unions):\nUsing hierarchical models for conjoint experiments in political science is new and exciting and revolutionary and neat. That’s the whole point of Jensen et al.’s paper—it’s a call to stop using OLS for everything.\nI’ve been working on a conjoint experiment with my coauthors Marc Dotson and Suparna Chaudhry. Suparna and I are political scientists and this multilevel stuff in general is still relatively new and wildly underused in the discipline. Marc, though, is a marketing scholar. The marketing world has been using hierarchical models for conjoint experiments for a long time and it’s standard practice in that discipline. There’s a whole textbook about the hierarchical model approach in marketing (Chapman and Feit 2019), and these fancy conjoint multilevel models are used widely throughout the marketing industry.\nlol at political science, just now discovering this.\nSo, I need to expand my previous conjoint guide. That’s what this post is for.\nI’ll do three things in this guide:\nThroughout this example, I’ll use data from two different simulated conjoint choice experiments. You can download these files and follow along:\nAdditionally, in Part 3, I fit a huge Stan model with {brms} that takes ≈30 minutes to run on my fast laptop. If you want to follow along and not melt your CPU for half an hour, you can download an .rds file of that fitted model that I stuck in an OSF project. The code for brm() later in this guide will load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\nLet’s load some libraries, create some helper functions, load the data, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(broom)            # Convert model objects to tidy data frames\nlibrary(parameters)       # Show model results as nice tables\nlibrary(survey)           # Panel-ish frequentist regression models\nlibrary(mlogit)           # Frequentist multinomial regression models\nlibrary(dfidx)            # Structure data for {mlogit} models\nlibrary(scales)           # Nicer labeling functions\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(ggforce)          # For facet_col()\nlibrary(brms)             # The best formula-based interface to Stan\nlibrary(tidybayes)        # Manipulate Stan results in tidy ways\nlibrary(ggdist)           # Fancy distribution plots\nlibrary(patchwork)        # Combine ggplot plots\nlibrary(rcartocolor)      # Color palettes from CARTOColors (https://carto.com/carto-colors/)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://github.com/intel/clear-sans\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Clear Sans\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Clear Sans\", face = \"bold\"),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 1),\n          strip.text = element_text(family = \"Clear Sans\", face = \"bold\",\n                                    size = rel(0.75), hjust = 0),\n          strip.background = element_rect(fill = \"grey90\", color = NA))\n}\n\ntheme_set(theme_nice())\n\nclrs &lt;- carto_pal(name = \"Prism\")\n\n# Functions for formatting things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\nchocolate &lt;- read_csv(\"data/choco_candy.csv\") %&gt;% \n  mutate(\n    dark = case_match(dark, 0 ~ \"Milk\", 1 ~ \"Dark\"),\n    dark = factor(dark, levels = c(\"Milk\", \"Dark\")),\n    soft = case_match(soft, 0 ~ \"Chewy\", 1 ~ \"Soft\"),\n    soft = factor(soft, levels = c(\"Chewy\", \"Soft\")),\n    nuts = case_match(nuts, 0 ~ \"No nuts\", 1 ~ \"Nuts\"),\n    nuts = factor(nuts, levels = c(\"No nuts\", \"Nuts\"))\n  )\n\nminivans &lt;- read_csv(\"data/rintro-chapter13conjoint.csv\") %&gt;% \n  mutate(\n    across(c(seat, cargo, price), factor),\n    carpool = factor(carpool, levels = c(\"no\", \"yes\")),\n    eng = factor(eng, levels = c(\"gas\", \"hyb\", \"elec\"))\n  )"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these kinds of candies they’d want to buy. Respondents only see this question one time and all possible options are presented simultaneously.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\n\n\n\n\nChocolate\nMilk\nMilk\nMilk\nMilk\nDark\nDark\nDark\nDark\n\n\nCenter\nChewy\nChewy\nSoft\nSoft\nChewy\nChewy\nSoft\nSoft\n\n\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment looks like this, with one row for each possible alternative (so eight rows per person, or subj), with the alternative that was selected marked as 1 in choice. Here, Subject 1 chose option E (dark, chewy, no nuts). There were 10 respondents, with 8 rows each, so there are 10 × 8 = 80 rows.\n\nchocolate\n## # A tibble: 80 × 6\n##     subj choice alt   dark  soft  nuts   \n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  \n##  1     1      0 A     Milk  Chewy No nuts\n##  2     1      0 B     Milk  Chewy Nuts   \n##  3     1      0 C     Milk  Soft  No nuts\n##  4     1      0 D     Milk  Soft  Nuts   \n##  5     1      1 E     Dark  Chewy No nuts\n##  6     1      0 F     Dark  Chewy Nuts   \n##  7     1      0 G     Dark  Soft  No nuts\n##  8     1      0 H     Dark  Soft  Nuts   \n##  9     2      0 A     Milk  Chewy No nuts\n## 10     2      0 B     Milk  Chewy Nuts   \n## # ℹ 70 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown eight different options and asked to select one. While this seems like a binary yes/no choice that could work with just regular plain old logistic regression, we want to account for the features and levels in all the unchosen categories too. To do this, we can use multinomial logistic regression, where the outcome variable is an unordered categorical variable with more than two categories. In this case we have eight different possible outcomes: alternatives A through H.\n\nOriginal SAS model as a baseline\n\n\n\n\n\n\nlol SAS\n\n\n\nI know nothing about SAS. I have never opened SAS in my life. It is a mystery to me.\nI copied these results directly from p. 297 in SAS’s massive “Discrete Choice” technical note (Kuhfeld 2010).\nI only have this SAS output here as a baseline reference for what the actual correct coefficients are supposed to be.\n\n\nSAS apparently fits these models with proportional hazard survival-style models, which feels weird, but there’s probably a mathematical or statistical reason for it. You use PROC PHREG to do it:\nproc phreg data=chocs outest=betas;\n   strata subj set;\n   model c*c(2) = dark soft nuts / ties=breslow;\n   run;\nIt gives these results:\n                   Choice of Chocolate Candies\n\n                       The PHREG Procedure\n\n              Multinomial Logit Parameter Estimates\n              \n                      Parameter     Standard\n                 DF    Estimate        Error  Chi-Square   Pr &gt; ChiSq\nDark Chocolate   1      1.38629      0.79057      3.0749       0.0795\nSoft Center      1     -2.19722      1.05409      4.3450       0.0371\nWith Nuts        1      0.84730      0.69007      1.5076       0.2195\n\n\nSurvival model\nEw, enough SAS. Let’s do this with R instead.\nWe can recreate the same proportional hazards model with coxph() from the {survival} package. Again, this feels weird and not like an intended purpose of survival models and not like multinomial logit at all—in my mind it is neither (1) multinomial nor (2) logit, but whatever. People far smarter than me invented these things, so I’ll just trust them.\n\nmodel_chocolate_survival &lt;- coxph(\n  Surv(subj, choice) ~ dark + soft + nuts, \n  data = chocolate, \n  ties = \"breslow\"  # This is what SAS uses\n)\n\nmodel_parameters(model_chocolate_survival, digits = 4, p_digits = 4)\n## Parameter   | Coefficient |     SE |         95% CI |       z |      p\n## ----------------------------------------------------------------------\n## dark [Dark] |      1.3863 | 0.7906 | [-0.16,  2.94] |  1.7535 | 0.0795\n## soft [Soft] |     -2.1972 | 1.0541 | [-4.26, -0.13] | -2.0845 | 0.0371\n## nuts [Nuts] |      0.8473 | 0.6901 | [-0.51,  2.20] |  1.2279 | 0.2195\n\nThe coefficients, standard errors, and p-values are identical to the SAS output! The only difference is the statistic: in SAS they use a chi-square statistic, while survival:coxph() uses a z statistic. There’s probably a way to make coxph() use a chi-square statistic, but I don’t care about that. I never use survival models and I’m only doing this to replicate the SAS output and it just doesn’t matter.\n\n\nPoisson model\nAn alternative way to fit a multinomial logit model without resorting to survival models is to actually (mis?)use another model family. We can use a Poisson model, even though choice isn’t technically count data, because of obscure stats reasons. See here for an illustration of the relationship between multinomial and Poisson distributions; or see this 2011 Biometrika paper about using Poisson models to reduce bias in multinomial logit models. Richard McElreath has a subsection about this in Statistical Rethinking as well: “Multinomial in disguise as Poisson” (11.3.3). Or as he said over on the currently-walled-garden Bluesky, “All count distributions are just one or more Poisson distributions in a trench coat.”\nTo account for the repeated subjects in the data, we’ll use svyglm() from the {survey} package so that the standard errors are more accurate.\n\nmodel_chocolate_poisson &lt;- glm(\n  choice ~ dark + soft + nuts, \n  data = chocolate, \n  family = poisson()\n)\n\nmodel_parameters(model_chocolate_poisson, digits = 4, p_digits = 4)\n## Parameter   | Log-Mean |     SE |         95% CI |       z |      p\n## -------------------------------------------------------------------\n## (Intercept) |  -2.9188 | 0.8628 | [-4.97, -1.49] | -3.3829 | 0.0007\n## dark [Dark] |   1.3863 | 0.7906 | [ 0.00,  3.28] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-5.11, -0.53] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.43,  2.38] |  1.2279 | 0.2195\n\nLovely—the results are the same.\n\n\nmlogit model\nFinally, we can use the {mlogit} package to fit the model. Before using mlogit(), we need to transform our data a bit to specify which column represents the choice (choice) and how the data is indexed: subjects (subj) with repeated alternatives (alt).\n\nchocolate_idx &lt;- dfidx(\n  chocolate,\n  idx = list(\"subj\", \"alt\"),\n  choice = \"choice\",\n  shape = \"long\"\n)\n\nWe can then use this indexed data frame with mlogit(), which uses the familiar R formula interface, but with some extra features separated by |s\noutcome ~ features | individual-level variables | alternative-level variables\nIf we had columns related to individual-level characteristics or alternative-level characteristics, we could include those in the model—and we’ll do precisely that later in this post. (Incorporating individual-level covariates is the whole point of this post!)\nLet’s fit the model:\n\nmodel_chocolate_mlogit &lt;- mlogit(\n  choice ~ dark + soft + nuts | 0 | 0, \n  data = chocolate_idx\n)\n\nmodel_parameters(model_chocolate_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |         95% CI |       z |      p\n## -------------------------------------------------------------------\n## dark [Dark] |   1.3863 | 0.7906 | [-0.16,  2.94] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-4.26, -0.13] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.51,  2.20] |  1.2279 | 0.2195\n\nDelightful. All the results are the same as the survival model and the Poisson model.\n\n\nBayesian model\nWe can also fit this model in a Bayesian way using {brms}. Stan has a categorical distribution family for multinomial models, and we’ll use it in the next example. For now, for the sake of simplicity, we’ll use a Poisson family, since, as we saw above, that’s a legal way of parameterizing multinomial distributions.\nThe data has a natural hierarchical structure to it, with 8 choices (for alternatives A through H) nested inside each of the 10 subjects.\n\n\n\n\n\nMultilevel experimental structure, with candy choices \\(y_{\\text{A}\\dots\\text{H}}\\) nested in subjects\n\n\n\n\nWe want to model candy choice (choice) based on candy characteristics (dark, soft, and nuts). We’ll use the subscript \\(i\\) to refer to individual candy choices and \\(j\\) to refer to subjects.\nSince we can legally pretend that this multinomial selection process is actually Poisson, we’ll model it as a Poisson process that has a rate of \\(\\lambda_{i_j}\\). We’ll model that \\(\\lambda_{i_j}\\) with a log-linked regression model with covariates for each of the levels of each candy feature. To account for the multilevel structure, we’ll include subject-specific offsets (\\(b_{0_j}\\)) from the global average, thus creating random intercepts. We’ll specify fairly wide priors just because.\nHere’s the formal model for all this:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Probability of selection of alternative}_i \\textbf{ in subject}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Poisson}(\\lambda_{i_j}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\log(\\lambda_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Dark}_{i_j} + \\beta_2 \\text{Soft}_{i_j} + \\beta_3 \\text{Nuts}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\qquad\\quad \\text{Subject-specific offsets from global choice probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for global average choice probability} \\\\\n\\beta_1, \\beta_2, \\beta_3 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for candy feature levels} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\qquad \\text{Prior for between-subject variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model:\n\nmodel_chocolate_brms &lt;- brm(\n  bf(choice ~ dark + soft + nuts + (1 | subj)),\n  data = chocolate,\n  family = poisson(),\n  prior = c(\n    prior(normal(0, 3), class = Intercept),\n    prior(normal(0, 3), class = b),\n    prior(exponential(1), class = sd)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_chocolate_brms\"\n)\n\nThe results are roughly the same as what we found with all the other models—they’re slightly off because of random MCMC sampling.\n\nmodel_parameters(model_chocolate_brms)\n## # Fixed Effects\n## \n## Parameter   | Median |         95% CI |     pd |  Rhat |     ESS\n## ----------------------------------------------------------------\n## (Intercept) |  -3.04 | [-5.07, -1.60] |   100% | 1.000 | 3598.00\n## darkDark    |   1.35 | [-0.05,  3.16] | 96.92% | 1.000 | 4238.00\n## softSoft    |  -2.03 | [-4.35, -0.47] | 99.60% | 1.000 | 2867.00\n## nutsNuts    |   0.83 | [-0.40,  2.27] | 90.28% | 1.000 | 4648.00"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the SAS technical note example, they use the model to generated predicted probabilities of the choice of each of the options. In the world of marketing, this can also be seen as the predicted market share for each option. To do this, they plug each of the eight different different combinations of dark, soft, and nuts into the model and calculate the predicted output on the response (i.e. probability) scale. They get these results, where dark, chewy, and nuts is the most likely and popular option (commanding a 50% market share).\n      Choice of Chocolate Candies\n\nObs    Dark    Soft     Nuts       p\n\n  1    Dark    Chewy    Nuts       0.50400\n  2    Dark    Chewy    No Nuts    0.21600\n  3    Milk    Chewy    Nuts       0.12600\n  4    Dark    Soft     Nuts       0.05600\n  5    Milk    Chewy    No Nuts    0.05400\n  6    Dark    Soft     No Nuts    0.02400\n  7    Milk    Soft     Nuts       0.01400\n  8    Milk    Soft     No Nuts    0.00600\nWe can do the same thing with R.\n\nFrequentist predictions\n{mlogit} model objects have predicted values stored in one of their slots (model_chocolate_mlogit$probabilities), but they’re in a weird non-tidy matrix form and I like working with tidy data. I’m also a huge fan of the {marginaleffects} package, which provides a consistent way to calculate predictions, comparisons, and slopes/marginal effects (with predictions(), comparisons(), and slopes()) for dozens of kinds of models, including mlogit() models.\nSo instead of wrangling the built-in mlogit() probabilities, we’ll generate predictions by feeding the model the unique combinations of dark, soft, and nuts to marginaleffects::predictions(), which will provide us with probability- or proportion-scale predictions:\n\npreds_chocolate_mlogit &lt;- predictions(\n  model_chocolate_mlogit, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n)\n\npreds_chocolate_mlogit %&gt;% \n  # predictions() hides a bunch of columns; forcing it to be a tibble unhides them\n  as_tibble() %&gt;% \n  arrange(desc(estimate)) %&gt;% \n  select(group, dark, soft, nuts, estimate, std.error, statistic, p.value)\n## # A tibble: 8 × 8\n##   group dark  soft  nuts    estimate std.error statistic  p.value\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 F     Dark  Chewy Nuts     0.504     0.142       3.56  0.000373\n## 2 E     Dark  Chewy No nuts  0.216     0.112       1.93  0.0540  \n## 3 B     Milk  Chewy Nuts     0.126     0.0849      1.48  0.138   \n## 4 H     Dark  Soft  Nuts     0.0560    0.0551      1.02  0.309   \n## 5 A     Milk  Chewy No nuts  0.0540    0.0433      1.25  0.213   \n## 6 G     Dark  Soft  No nuts  0.0240    0.0258      0.929 0.353   \n## 7 D     Milk  Soft  Nuts     0.0140    0.0162      0.863 0.388   \n## 8 C     Milk  Soft  No nuts  0.00600   0.00743     0.808 0.419\n\nPerfect! They’re identical to the SAS output.\nWe can play around with these predictions to describe the overall market for candy. Chewy candies dominate the market…\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 2 × 2\n##   dark  share\n##   &lt;fct&gt; &lt;dbl&gt;\n## 1 Milk  0.200\n## 2 Dark  0.800\n\n…and dark chewy candies are by far the most popular:\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark, soft) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 4 × 3\n## # Groups:   dark [2]\n##   dark  soft   share\n##   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n## 1 Milk  Chewy 0.180 \n## 2 Milk  Soft  0.0200\n## 3 Dark  Chewy 0.720 \n## 4 Dark  Soft  0.0800\n\n\n\nBayesian predictions\n{marginaleffects} supports {brms} models too, so we can basically run the same predictions() function to generate predictions for our Bayesian model. Magical.\n\n\n\n\n\n\nlol subject offsets\n\n\n\nWhen plugging values into predictions() (or avg_slopes() or any function that calculates predictions from a model), we have to decide how to handle the random subject offsets (\\(b_{0_j}\\)). I have a whole other blog post guide about this and how absolutely maddening the nomenclature for all this is.\nBy default, predictions() and friends will calculate predictions for subjects on average by using the re_formula = NULL argument. This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average subject. When working with slopes, this is also called a marginal effect.\nWe could also use re_formula = NA to calculate predictions for a typical subject, or a subject where the random offset is set to 0. When working with slopes, this is also called a conditional effect.\n\nConditional predictions/effect = average subject = re_formula = NA\nMarginal predictions/effect = subjects on average = re_formula = NULL (default), using existing subject levels or a new simulated subject\n\nAgain, see this guide for way more about these distinctions. In this example here, we’ll just use the default marginal predictions/effects (re_formula = NULL), or the effect for subjects on average.\n\n\nThe predicted proportions aren’t identical to the SAS output, but they’re close enough, given that it’s a completely different modeling approach.\n\npreds_chocolate_brms &lt;- predictions(\n  model_chocolate_brms, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n) \n\npreds_chocolate_brms %&gt;% \n  as_tibble() %&gt;%\n  arrange(desc(estimate)) %&gt;% \n  select(dark, soft, nuts, estimate, conf.low, conf.high)\n## # A tibble: 8 × 6\n##   dark  soft  nuts    estimate conf.low conf.high\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Dark  Chewy Nuts     0.432   0.144       1.09  \n## 2 Dark  Chewy No nuts  0.186   0.0424      0.571 \n## 3 Milk  Chewy Nuts     0.110   0.0168      0.419 \n## 4 Dark  Soft  Nuts     0.0553  0.00519     0.279 \n## 5 Milk  Chewy No nuts  0.0465  0.00552     0.219 \n## 6 Dark  Soft  No nuts  0.0230  0.00182     0.141 \n## 7 Milk  Soft  Nuts     0.0136  0.00104     0.0881\n## 8 Milk  Soft  No nuts  0.00556 0.000326    0.0414\n\n\n\nPlots\nSince predictions() returns a tidy data frame, we can plot these predicted probabilities (or market shares or however we want to think about them) with {ggplot2}:\n\np1 &lt;- preds_chocolate_mlogit %&gt;% \n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = estimate, y = label)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high), color = clrs[7]) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Frequentist {mlogit} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())\n\np2 &lt;- preds_chocolate_brms %&gt;% \n  posterior_draws() %&gt;%  # Extract the posterior draws of the predictions\n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = draw, y = label)) +\n  stat_halfeye(normalize = \"xy\", fill = clrs[7])  +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Bayesian {brms} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +\n  # Make the x-axis match the mlogit plot\n  coord_cartesian(xlim = c(-0.05, 0.78))\n\np1 / p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nThe marketing world doesn’t typically look at coefficients or marginal effects, but the political science world definitely does. In political science, the estimand we often care about the most is the average marginal component effect (AMCE), or the causal effect of moving one feature level to a different value, holding all other features constant. I have a whole in-depth blog post about AMCEs and how to calculate them—go look at that for more details. Long story short—AMCEs are basically the coefficients in a regression model.\nInterpreting the coefficients is difficult with models that aren’t basic linear regression. Here, all these coefficients are on the log scale, so they’re not directly interpretable. The original SAS technical note also doesn’t really interpret any of these , they don’t really interpret these things anyway, since they’re more focused on predictions. All they say is this:\n\nThe parameter estimate with the smallest p-value is for soft center. Since the parameter estimate is negative, chewy is the more preferred level. Dark is preferred over milk, and nuts over no nuts, however only the p-value for Soft is less than 0.05.\n\nWe could exponentiate the coefficients to make them multiplicative (akin to odds ratios in logistic regression). For center = soft, \\(e^{-2.19722}\\) = 0.1111, which means that candies with a soft center are 89% less likely to be chosen than candies with a chewy center, relative to the average candy. But that’s weird to think about.\nSo instead we can turn to {marginaleffects} once again to calculate percentage-point scale estimands that we can interpret far more easily.\n\n\n\n\n\n\nlol marginal effects\n\n\n\nNobody is ever consistent about the word “marginal effect.” Some people use it to refer to averages; some people use it to refer to slopes. These are complete opposites. In calculus, averages = integrals and slopes = derivatives and they’re the inverse of each other.\nI like to think of marginal effects as what happens to the outcome when you move an explanatory variable a tiny bit. With continuous variables, that’s a slope; with categorical variables, that’s an offset in average outcomes. These correspond directly to how you normally interpret regression coefficients. Or returning to my favorite analogy about regression, with numeric variables we care what happens to the outcome when we slide the value up a tiny bit; with categorical variables we care about what happens to the outcome when we switch on a category.\nAdditionally, there are like a billion different ways to calculate marginal effects: average marginal effects (AMEs), group-average marginal effects (G-AMEs), marginal effects at user-specified values, marginal effects at the mean (MEM), and counterfactual marginal effects. See the documentation for {marginaleffects} + this mega blog post for more about these subtle differences.\n\n\n\nBayesian comparisons/contrasts\nWe can use avg_comparisons() to calculate the difference (or average marginal effect) for each of the categorical coefficients on the percentage-point scale, showing the effect of moving from milk → dark, chewy → soft, and nuts → no nuts.\n(Technically we can also use avg_slopes(), even though none of these coefficients are actually slopes. {marginaleffects} is smart enough to show contrasts for categorical variables and partial derivatives/slopes for continuous variables.)\n\navg_comparisons(model_chocolate_brms)\n## \n##  Term       Contrast Estimate    2.5 %  97.5 %\n##  dark Dark - Milk       0.139 -0.00551  0.3211\n##  nuts Nuts - No nuts    0.092 -0.05221  0.2599\n##  soft Soft - Chewy     -0.182 -0.35800 -0.0523\n## \n## Columns: term, contrast, estimate, conf.low, conf.high\n\nWhen holding all other features constant, moving from chewy → soft is associated with a posterior median 18 percentage point decrease in the probability of selection (or drop in market share if you want to think of it that way), on average.\n\n\nFrequentist comparisons/contrasts\nWe went out of order in this section and showed how to use avg_comparisons() with the Bayesian model first instead of the frequentist model. That’s because it was easy. mlogit() models behave strangely with {marginaleffects} because {mlogit} forces its predictions to use every possible value of the alternatives A–H. Accordingly, the estimate for any coefficients in the attributes section of the {mlogit} formula (dark, soft, and nuts here) will automatically be zero. Note how here there are 24 rows of comparisons instead of 3, since we get comparisons in each of the 8 groups, and note how the estimates are all zero:\n\navg_comparisons(model_chocolate_mlogit)\n## \n##  Group Term       Contrast  Estimate Std. Error         z Pr(&gt;|z|)     2.5 %   97.5 %\n##      A dark Dark - Milk    -2.78e-17   7.27e-13 -3.82e-05        1 -1.42e-12 1.42e-12\n##      B dark Dark - Milk     0.00e+00         NA        NA       NA        NA       NA\n##      C dark Dark - Milk     0.00e+00   1.62e-14  0.00e+00        1 -3.17e-14 3.17e-14\n##      D dark Dark - Milk    -6.94e-18   1.73e-13 -4.02e-05        1 -3.38e-13 3.38e-13\n##      E dark Dark - Milk    -2.78e-17   7.27e-13 -3.82e-05        1 -1.42e-12 1.42e-12\n##      F dark Dark - Milk     0.00e+00         NA        NA       NA        NA       NA\n##      G dark Dark - Milk     0.00e+00   1.62e-14  0.00e+00        1 -3.17e-14 3.17e-14\n##      H dark Dark - Milk    -6.94e-18   1.73e-13 -4.02e-05        1 -3.38e-13 3.38e-13\n##      A nuts Nuts - No nuts  1.39e-17         NA        NA       NA        NA       NA\n##      B nuts Nuts - No nuts  1.39e-17         NA        NA       NA        NA       NA\n##      C nuts Nuts - No nuts  0.00e+00   1.41e-14  0.00e+00        1 -2.77e-14 2.77e-14\n##      D nuts Nuts - No nuts  0.00e+00   1.41e-14  0.00e+00        1 -2.77e-14 2.77e-14\n##      E nuts Nuts - No nuts  0.00e+00   9.74e-13  0.00e+00        1 -1.91e-12 1.91e-12\n##      F nuts Nuts - No nuts  0.00e+00   9.74e-13  0.00e+00        1 -1.91e-12 1.91e-12\n##      G nuts Nuts - No nuts  0.00e+00   1.08e-13  0.00e+00        1 -2.11e-13 2.11e-13\n##      H nuts Nuts - No nuts  0.00e+00   1.08e-13  0.00e+00        1 -2.11e-13 2.11e-13\n##      A soft Soft - Chewy    6.94e-18   1.08e-13  6.42e-05        1 -2.12e-13 2.12e-13\n##      B soft Soft - Chewy    1.39e-17   2.16e-13  6.43e-05        1 -4.23e-13 4.23e-13\n##      C soft Soft - Chewy    6.94e-18   1.08e-13  6.42e-05        1 -2.12e-13 2.12e-13\n##      D soft Soft - Chewy    1.39e-17   2.16e-13  6.43e-05        1 -4.23e-13 4.23e-13\n##      E soft Soft - Chewy    1.39e-17   4.33e-13  3.21e-05        1 -8.48e-13 8.48e-13\n##      F soft Soft - Chewy    0.00e+00   4.52e-13  0.00e+00        1 -8.86e-13 8.86e-13\n##      G soft Soft - Chewy    1.39e-17   4.33e-13  3.21e-05        1 -8.48e-13 8.48e-13\n##      H soft Soft - Chewy    0.00e+00   4.52e-13  0.00e+00        1 -8.86e-13 8.86e-13\n## \n## Columns: group, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nIf we had continuous variables, we could work around this by specifying our own tiny amount of marginal change to compare across, but we’re working with categories and can’t do that. Instead, with categorical variables, we can return to predictions() and define custom aggregations of different features and levels.\nBefore making custom aggregations, though, it’ll be helpful to illustrate what exactly we’re looking at when collapsing these results. Remember that earlier we calculated predictions for all the unique combinations of dark, soft, and nuts:\n\npreds_chocolate_mlogit &lt;- predictions(\n  model_chocolate_mlogit, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n) \n\npreds_chocolate_mlogit %&gt;% \n  as_tibble() %&gt;% \n  select(group, dark, soft, nuts, estimate)\n## # A tibble: 8 × 5\n##   group dark  soft  nuts    estimate\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;\n## 1 A     Milk  Chewy No nuts  0.0540 \n## 2 B     Milk  Chewy Nuts     0.126  \n## 3 C     Milk  Soft  No nuts  0.00600\n## 4 D     Milk  Soft  Nuts     0.0140 \n## 5 E     Dark  Chewy No nuts  0.216  \n## 6 F     Dark  Chewy Nuts     0.504  \n## 7 G     Dark  Soft  No nuts  0.0240 \n## 8 H     Dark  Soft  Nuts     0.0560\n\nFour of the groups have dark = Milk and four have dark = Dark, with other varying characteristics across those groups (chewy/soft, nuts/no nuts). If we want the average proportion of all milk and dark chocolate options, we can group and summarize:\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(avg_pred = mean(estimate))\n## # A tibble: 2 × 2\n##   dark  avg_pred\n##   &lt;fct&gt;    &lt;dbl&gt;\n## 1 Milk    0.0500\n## 2 Dark    0.200\n\nThe average market share for milk chocolate candies, holding all other features constant, is 5% (\\(\\frac{0.0540 + 0.126 + 0.006 + 0.014}{2} = 0.05\\)); the average market share for dark chocolate candies is 20% (\\(\\frac{0.216 + 0.504 + 0.024 + 0.056}{2} = 0.2\\)). These values are the averages of the predictions from the four groups where dark is either Milk or Dark.\nInstead of calculating these averages manually (which would also force us to calculate standard errors and p-values manually, which, ugh), we can calculate these aggregate group means with predictions(). To do this, we can feed a little data frame to predictions() with the by argument. The data frame needs to contain columns for the features we want to collapse, and a by column with the labels we want to include in the output. For example, if we want to collapse the eight possible choices into those with milk chocolate and those with dark chocolate, we could create a by data frame like this:\n\nby &lt;- data.frame(dark = c(\"Milk\", \"Dark\"), by = c(\"Milk\", \"Dark\"))\nby\n##   dark   by\n## 1 Milk Milk\n## 2 Dark Dark\n\nIf we use that by data frame in predictions(), we get the same 5% and 20% from before, but now with all of {marginaleffects}’s extra features like standard errors and confidence intervals:\n\npredictions(\n  model_chocolate_mlogit,\n  by = by\n)\n## \n##  Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %   By\n##      0.20     0.0316 6.32   &lt;0.001  0.138  0.262 Dark\n##      0.05     0.0316 1.58    0.114 -0.012  0.112 Milk\n## \n## Columns: estimate, std.error, statistic, p.value, conf.low, conf.high, by\n\nEven better, we can use the hypothesis functionality of predictions() to conduct a hypothesis test and calculate the difference (or contrast) between these two averages, which is exactly what we’re looking for with categorical AMCEs. This shows the average causal effect of moving from milk → dark—holding all other features constant, switching the chocolate type from milk to dark causes a 15 percentage point increase in the probability of selecting the candy, on average.\n\npredictions(\n  model_chocolate_mlogit,\n  by = by,\n  hypothesis = \"revpairwise\"\n)\n## \n##         Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 % 97.5 %\n##  Milk - Dark    -0.15     0.0632 -2.37   0.0177 -0.274 -0.026\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can’t simultaneously specify all the contrasts we’re interested in single by argument, but we can do them separately and combine them into a single data frame:\n\namces_chocolate_mlogit &lt;- bind_rows(\n  dark = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(dark = c(\"Milk\", \"Dark\"), by = c(\"Milk\", \"Dark\")),\n    hypothesis = \"revpairwise\"\n  ),\n  soft = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(soft = c(\"Chewy\", \"Soft\"), by = c(\"Chewy\", \"Soft\")),\n    hypothesis = \"revpairwise\"\n  ),\n  nuts = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(nuts = c(\"No nuts\", \"Nuts\"), by = c(\"No nuts\", \"Nuts\")),\n    hypothesis = \"revpairwise\"\n  ),\n  .id = \"variable\"\n)\namces_chocolate_mlogit\n## \n##            Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 % 97.5 %\n##  Milk - Dark       -0.15     0.0632 -2.37   0.0177 -0.274 -0.026\n##  Soft - Chewy      -0.20     0.0474 -4.22   &lt;0.001 -0.293 -0.107\n##  Nuts - No nuts     0.10     0.0725  1.38   0.1675 -0.042  0.242\n## \n## Columns: variable, term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n\n\nPlots\nPlotting these AMCEs requires a bit of data wrangling, but we get really neat plots, so it’s worth it. I’ve hidden all the code here for the sake of space.\n\n\nExtract variable labels\nchocolate_var_levels &lt;- tibble(\n  variable = c(\"dark\", \"soft\", \"nuts\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- chocolate[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nchocolate_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"dark\",    \"Type of chocolate\",\n  \"soft\",    \"Type of center\",\n  \"nuts\",    \"Nuts\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plot\namces_chocolate_mlogit_split &lt;- amces_chocolate_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    amces_chocolate_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(1, 3, 8)]) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\nCombine full dataset of factor levels with posterior draws and make {brms} plot\n# This is much easier than the mlogit mess because we can use avg_comparisons() directly\nposterior_mfx &lt;- model_chocolate_brms %&gt;% \n  avg_comparisons() %&gt;% \n  posteriordraws() \n\nposterior_mfx_nested &lt;- posterior_mfx %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\n# Combine full dataset of factor levels with model results\nplot_data_bayes &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    posterior_mfx_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  guides(fill = \"none\") +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(1, 3, 8)]) +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 | p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these minivans they’d want to buy, based on four different features/attributes with different levels:\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nPassengers\n6, 7, 8\n\n\nCargo area\n2 feet, 3 feet\n\n\nEngine\nGas, electric, hybrid\n\n\nPrice\n$30,000; $35,000; $40,000\n\n\n\nRespondents see this a question similar to this fifteen different times, with three options with randomly shuffled levels for each of the features.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption 1\nOption 2\nOption 3\n\n\n\n\nPassengers\n7\n8\n6\n\n\nCargo area\n3 feet\n3 feet\n2 feet\n\n\nEngine\nElectric\nGas\nHybrid\n\n\nPrice\n$40,000\n$40,000\n$30,000\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment has one row for each possible alternative (alt) within each set of 15 questions (ques), thus creating 3 × 15 = 45 rows per respondent (resp.id). There were 200 respondents, with 45 rows each, so there are 200 × 45 = 9,000 rows. Here, Respondent 1 chose a $30,000 gas van with 6 seats and 3 feet of cargo space in the first set of three options, a $35,000 gas van with 7 seats and 3 feet of cargo space in the second set of three options, and so on.\nThere’s also a column here for carpool indicating if the respondent carpools with others when commuting. It’s an individual respondent-level characteristic and is constant throughout all the questions and alternatives, and we’ll use it later.\n\nminivans\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt carpool seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 yes     6     2ft   gas   35         0\n##  2       1     1     2 yes     8     3ft   hyb   30         0\n##  3       1     1     3 yes     6     3ft   gas   30         1\n##  4       1     2     1 yes     6     2ft   gas   30         0\n##  5       1     2     2 yes     7     3ft   gas   35         1\n##  6       1     2     3 yes     6     2ft   elec  35         0\n##  7       1     3     1 yes     8     3ft   gas   35         1\n##  8       1     3     2 yes     7     3ft   elec  30         0\n##  9       1     3     3 yes     8     2ft   elec  40         0\n## 10       1     4     1 yes     7     3ft   elec  40         1\n## # ℹ 8,990 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown three different options and asked to select one. We thus have three possible outcomes: a respondent could have selected option 1, option 2, or option 3. Because everything was randomized, there shouldn’t be any patterns in which options people choose—we don’t want to see that the first column is more common, since that would indicate that respondents are just repeatedly selecting the first column to get through the survey. Since there are three possible outcomes (option 1, 2, and 3), we’ll use multinomial logistic regression.\n\nOriginal model as a baseline\nIn the example in their textbook, Chapman and Feit (2019) use {mlogit} to estimate this model and they find these results. This will be our baseline throughout this example.\n\n\n\nOriginal results from Chapman and Feit (2019) p. 371\n\n\n\n\nmlogit model\nThis data is a little more complex now, since there are alternatives nested inside questions inside respondents. To account for this panel structure when using {mlogit}, we need to define two index columns: one for the unique set of alternatives offered to the respondent and one for the respondent ID. We still do this with dfidx(), but need to create a new column with an ID number for each unique combination of respondent ID and question number:\n\nminivans_idx &lt;- minivans %&gt;% \n  # mlogit() needs a column with unique question id numbers\n  group_by(resp.id, ques) %&gt;% \n  mutate(choice.id = cur_group_id()) %&gt;% \n  ungroup() %&gt;% \n  # Make indexed data frame for mlogit\n  dfidx(\n    idx = list(c(\"choice.id\", \"resp.id\"), \"alt\"),\n    choice = \"choice\",\n    shape = \"long\"\n  )\n\nNow we can fit the model. Note the 0 ~ seat syntax here. That suppresses the intercept for the model, which behaves weirdly with multinomial models. Since there are three categories for the outcome (options 1, 2, and 3), there are two intercepts, representing cutpoints-from-ordered-logit-esque shifts in the probability of selecting option 1 vs. option 2 and option 2 vs. option 3. We don’t want to deal with those, so we’ll suppress them.\n\nmodel_minivans_mlogit &lt;- mlogit(\n  choice ~ 0 + seat + cargo + eng + price | 0 | 0, \n  data = minivans_idx\n)\nmodel_parameters(model_minivans_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |         95% CI |        z |           p\n## -------------------------------------------------------------------------\n## seat [7]    |  -0.5353 | 0.0624 | [-0.66, -0.41] |  -8.5837 | 9.1863e-18 \n## seat [8]    |  -0.3058 | 0.0611 | [-0.43, -0.19] |  -5.0032 | 5.6376e-07 \n## cargo [3ft] |   0.4774 | 0.0509 | [ 0.38,  0.58] |   9.3824 | 6.4514e-21 \n## eng [hyb]   |  -0.8113 | 0.0601 | [-0.93, -0.69] | -13.4921 | 1.7408e-41 \n## eng [elec]  |  -1.5308 | 0.0675 | [-1.66, -1.40] | -22.6926 | 5.3004e-114\n## price [35]  |  -0.9137 | 0.0606 | [-1.03, -0.79] | -15.0765 | 2.3123e-51 \n## price [40]  |  -1.7259 | 0.0696 | [-1.86, -1.59] | -24.7856 | 1.2829e-135\n\nThese are the same results from p. 371 in Chapman and Feit (2019), so it worked. Again, the marketing world doesn’t typically do much with these coefficients beyond looking at their direction and magnitude. For instance, in Chapman and Feit (2019) they say that the estimate for seat [7] here is negative, which means that a 7-seat option is less preferred than 6-seat option, and that the estimate for price [40] is more negative than the already-negative estimate for price [35], which means that (1) respondents don’t like the $35,000 option compared to the baseline $30,000 and that (2) respondents really don’t like the $40,000 option. We could theoretically exponentiate these things—like, seeing 7 seats makes it \\(e^{-0.5353}\\) = 0.5855 = 41% less likely to select the option compared to 6 seats—but again, that’s weird.\n\n\nBayesian model with {brms}\nWe can also fit this multinomial model in a Bayesian way using {brms}. Stan has a categorical family for dealing with mulitnomial/categorical outcomes. But first, we’ll look at the nested structure of this data and incorporate that into the model, since we won’t be using the weird {mlogit}-style indexed data frame. As with the chocolate experiment, the data has a natural hierarchy in it, with three questions nested inside 15 separate question sets, nested inside each of the 200 respondents.\n\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents\n\n\n\n\nCurrently, our main outcome variable choice is binary. If we run the model with choice as the outcome with a categorical family, the model will fit, but it will go slow and {brms} will complain about it and recommend switching to regular logistic regression. The categorical family in Stan requires 2+ outcomes and a reference category. Here we have three possible options (1, 2, and 3), and we can imagine a reference category of 0 for rows that weren’t selected.\nWe can create a new outcome column (choice_alt) that indicates which option each respondent selected: 0 if they didn’t choose the option and 1–3 if they chose the first, second, or third option. Because of how the data is recorded, this only requires multiplying alt and choice:\n\nminivans_choice_alt &lt;- minivans %&gt;% \n  mutate(choice_alt = factor(alt * choice))\n\nminivans_choice_alt %&gt;% \n  select(resp.id, ques, alt, seat, cargo, eng, price, choice, choice_alt)\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt seat  cargo eng   price choice choice_alt\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;     \n##  1       1     1     1 6     2ft   gas   35         0 0         \n##  2       1     1     2 8     3ft   hyb   30         0 0         \n##  3       1     1     3 6     3ft   gas   30         1 3         \n##  4       1     2     1 6     2ft   gas   30         0 0         \n##  5       1     2     2 7     3ft   gas   35         1 2         \n##  6       1     2     3 6     2ft   elec  35         0 0         \n##  7       1     3     1 8     3ft   gas   35         1 1         \n##  8       1     3     2 7     3ft   elec  30         0 0         \n##  9       1     3     3 8     2ft   elec  40         0 0         \n## 10       1     4     1 7     3ft   elec  40         1 1         \n## # ℹ 8,990 more rows\n\nWe can now use the new four-category choice_alt column as our outcome with the categorical() family.\nIf we realllly wanted, we could add random effects for question sets nested inside respondents, like (1 | resp.id / ques). We’d want to do that if there were set-specific things that could influences choices. Like maybe we want to account for the possibility that everyone’s just choosing the first option, so it behaves differently? Or maybe the 5th set of questions is set to an extra difficult level on a quiz or something? Or maybe we have so many sets that we think the later ones will be less accurate because of respondent fatigue? idk. In this case, question set-specific effects don’t matter at all. Each question set is equally randomized and no different from the others, so we won’t bother modeling that layer of the hierarchy.\nWe want to model the choice of option 1, 2, or 3 (choice_alt) based on minivan characteristics (seat, cargo, eng, price). With the categorical model, we actually get a set of parameters to estimate the probability of selecting each of the options, which Stan calls \\(\\mu\\), so we have a set of three probabilities: \\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\). We’ll use the subscript \\(i\\) to refer to individual minivan choices and \\(j\\) to refer to respondents. Here’s the fun formal model:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Seat[7]}_{i_j} + \\beta_2 \\text{Seat[8]}_{i_j} + \\beta_3 \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_4 \\text{Engine[hyb]}_{i_j} + \\beta_5 \\text{Engine[elec]}_{i_j} + \\beta_6 \\text{Price[35k]}_{i_j} + \\beta_7 \\text{Price[40k]}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\quad\\quad \\text{Respondent-specific offsets from global probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ \\ \\text{Prior for choice-level coefficients} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\quad \\text{Prior for between-respondent variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model. Notice the much-more-verbose prior section—because the categorical family in Stan estimates separate parameters for each of the categories (\\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\)), we have a mean and standard deviation for the probability of selecting each of those options. We need to specify each of these separately too instead of just doing something like prior(normal(0, 3), class = b). Also notice the refcat argument in categorical()—this makes it so that all the estimates are relative to not choosing an option (or when choice_alt is 0). And also notice the slightly different syntax for the random respondent intercepts: (1 | ID | resp.id). That new middle ID is special {brms} formula syntax that we can use when working with categorical or ordinal families, and it makes it so that the group-level effects for the different outcomes (here options 0, 1, 2, and 3) are correlated (see p. 4 of this {brms} vignette for more about this special syntax).\n\nmodel_minivans_categorical_brms &lt;- brm(\n  bf(choice_alt ~ 0 + seat + cargo + eng + price + (1 | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_minivans_categorical_brms\"\n)\n\nThis model gives us a ton of parameters! We get three estimates per feature level (i.e. mu1_cargo3ft, mu2_cargo3ft, and mu3_cargo3ft for the cargo3ft effect), since we’re actually estimating the effect of each covariate on the probability of selecting each of the three options.\n\nmodel_parameters(model_minivans_categorical_brms)\n## Parameter    | Median |         95% CI |     pd |  Rhat |     ESS\n## -----------------------------------------------------------------\n## mu1_seat6    |  -0.34 | [-0.52, -0.16] | 99.95% | 1.001 | 2673.00\n## mu1_seat7    |  -0.86 | [-1.04, -0.67] |   100% | 1.000 | 3167.00\n## mu1_seat8    |  -0.59 | [-0.77, -0.41] |   100% | 1.000 | 3373.00\n## mu1_cargo3ft |   0.46 | [ 0.32,  0.60] |   100% | 1.000 | 6314.00\n## mu1_enghyb   |  -0.76 | [-0.92, -0.60] |   100% | 1.001 | 4628.00\n## mu1_engelec  |  -1.51 | [-1.69, -1.33] |   100% | 0.999 | 4913.00\n## mu1_price35  |  -0.82 | [-0.99, -0.67] |   100% | 0.999 | 4574.00\n## mu1_price40  |  -1.74 | [-1.94, -1.56] |   100% | 1.000 | 4637.00\n## mu2_seat6    |  -0.39 | [-0.57, -0.20] |   100% | 1.000 | 2387.00\n## mu2_seat7    |  -0.95 | [-1.15, -0.77] |   100% | 1.001 | 2470.00\n## mu2_seat8    |  -0.67 | [-0.85, -0.49] |   100% | 1.001 | 2489.00\n## mu2_cargo3ft |   0.49 | [ 0.35,  0.63] |   100% | 1.000 | 4836.00\n## mu2_enghyb   |  -0.79 | [-0.95, -0.63] |   100% | 1.000 | 4421.00\n## mu2_engelec  |  -1.40 | [-1.57, -1.22] |   100% | 1.000 | 4261.00\n## mu2_price35  |  -0.79 | [-0.95, -0.63] |   100% | 1.001 | 3699.00\n## mu2_price40  |  -1.47 | [-1.65, -1.29] |   100% | 0.999 | 3978.00\n## mu3_seat6    |  -0.28 | [-0.46, -0.11] | 99.85% | 1.000 | 2077.00\n## mu3_seat7    |  -0.78 | [-0.96, -0.60] |   100% | 1.000 | 3025.00\n## mu3_seat8    |  -0.63 | [-0.81, -0.46] |   100% | 1.000 | 2483.00\n## mu3_cargo3ft |   0.36 | [ 0.23,  0.50] |   100% | 0.999 | 5327.00\n## mu3_enghyb   |  -0.73 | [-0.88, -0.58] |   100% | 1.000 | 4039.00\n## mu3_engelec  |  -1.41 | [-1.59, -1.23] |   100% | 1.001 | 3818.00\n## mu3_price35  |  -0.85 | [-1.01, -0.69] |   100% | 1.000 | 4315.00\n## mu3_price40  |  -1.56 | [-1.75, -1.39] |   100% | 0.999 | 4774.00\n\nImportantly, the estimates here are all roughly equivalent to what we get from {mlogit}: the {mlogit} estimate for cargo3ft was 0.4775, while the three median posterior {brms} estimates are 0.46 (95% credible interval: 0.32–0.60), 0.49 (0.35–0.63), and 0.36 (0.23–0.50)\nSince all the features are randomly shuffled between the three options each time, and each option is selected 1/3rd of the time, it’s probably maybe legal to pool these posterior estimates together (maaaaybeee???) so that we don’t have to work with three separate estimates for each parameter? To do this we’ll take the average of each of the three \\(\\mu\\) estimates within each draw, which is also called “marginalizing” across the three options.\nHere’s how we’d do that with {tidybayes}. The medians are all roughly the same now!\n\nminivans_cat_marginalized &lt;- model_minivans_categorical_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_cat_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi()\n## # A tibble: 8 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo3ft   0.439  0.340  0.532   0.95 median qi       \n## 2 engelec   -1.44  -1.56  -1.32    0.95 median qi       \n## 3 enghyb    -0.762 -0.871 -0.651   0.95 median qi       \n## 4 price35   -0.823 -0.935 -0.713   0.95 median qi       \n## 5 price40   -1.59  -1.72  -1.47    0.95 median qi       \n## 6 seat6     -0.337 -0.464 -0.208   0.95 median qi       \n## 7 seat7     -0.862 -0.994 -0.734   0.95 median qi       \n## 8 seat8     -0.629 -0.753 -0.503   0.95 median qi\n\nAnd for fun, here’s what the posterior for new combined/collapsed/marginalized cargo3ft looks like. Great.\n\nminivans_cat_marginalized %&gt;% \n  filter(.variable == \"cargo3ft\") %&gt;% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye(fill = clrs[4]) +\n  labs(x = \"Posterior distribution of β (logit-scale)\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nAs we saw in the first example with chocolates, the marketing world typically uses predictions from these kinds of models to estimate the predicted market share for products with different constellations of features. That was a pretty straightforward task with the chocolate model since respondents were shown all 8 options simultaneously. It’s a lot trickier with the minivan example where respondents were shown 15 sets of 3 options. Dealing with multinomial predictions is a bear of a task because these models are a lot more complex.\n\nFrequentist predictions\nWith the chocolate model, we could just use avg_predictions(model_chocolate_mlogit) and automatically get predictions for all 8 options. That’s not the case here:\n\navg_predictions(model_minivans_mlogit)\n## \n##  Group Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n##      1    0.333   0.000349  955   &lt;0.001 0.332  0.334\n##      2    0.333   0.000145 2291   &lt;0.001 0.333  0.334\n##      3    0.334   0.000376  889   &lt;0.001 0.333  0.335\n## \n## Columns: group, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe get three predictions, and they’re all 33ish%. That’s because respondents were presented with three randomly shuffled options and chose one of them. All these predictions tell us is that across all 15 iterations of the questions, 1/3 of respondents selected the first option, 1/3 the second, and 1/3 the third. That’s a good sign in this case—there’s no evidence that people were just repeatedly choosing the first option. But in the end, these predictions aren’t super useful.\nWe instead want to be able to get predicted market shares (or predicted probabilities) for any given mix of products. For instance, here are six hypothetical products with different combinations of seats, cargo space, engines, and prices:\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\nexample_product_mix\n## # A tibble: 6 × 4\n##   seat  cargo eng   price\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1 7     2ft   hyb   30   \n## 2 6     2ft   gas   30   \n## 3 8     2ft   gas   30   \n## 4 7     3ft   gas   40   \n## 5 6     2ft   elec  40   \n## 6 7     2ft   hyb   35\n\nIf we were working with any other type of model, we could plug this data into the newdata argument of predictions() and get predicted values. That doesn’t work here though. There were 200 respondents in the original data, and {mlogit}-predictions need to happen on a dataset with a multiple of that many rows. We can’t just feed it 6 values.\n\npredictions(model_minivans_mlogit, newdata = example_product_mix)\n## Error: The `newdata` argument for `mlogit` models must be a data frame with a number of rows equal to a multiple of the number of choices: 200.\n\nInstead, following Chapman and Feit (2019) (and this Stan forum post), we can manually multiply the covariates in example_product_mix with the model coefficients to calculate “utility” (or predicted vales on the logit scale), which we can then exponentiate and divide to calculate market shares.\n\n\n\n\n\n\nLimits of {marginaleffects} and {mlogit}\n\n\n\nFrom what I can tell, this is not possible with {marginaleffects} because that package can’t work with the coefficients from {mlogit} models and can only really work with predictions. {mlogit} predictions are forced to be on the response/probability scale since they’re, like, predictions, so there’s no way to get them on the log/logit link scale to calculate utilities and shares.\n\n\n\n# Create a matrix of 0s and 1s for the values in `example_product_mix`, omitting\n# the first column (seat6)\nexample_product_dummy_encoded &lt;- model.matrix(\n  update(model_minivans_mlogit$formula, 0 ~ .),\n  data = example_product_mix\n)[, -1]\nexample_product_dummy_encoded\n##   seat7 seat8 cargo3ft enghyb engelec price35 price40\n## 1     1     0        0      1       0       0       0\n## 2     0     0        0      0       0       0       0\n## 3     0     1        0      0       0       0       0\n## 4     1     0        1      0       0       0       1\n## 5     0     0        0      0       1       0       1\n## 6     1     0        0      1       0       1       0\n\n# Matrix multiply the matrix of 0s and 1s with the model coefficients to get\n# logit-scale predictions, or utility\nutility &lt;- example_product_dummy_encoded %*% coef(model_minivans_mlogit)\n\n# Divide each exponentiated utility by the sum of the exponentiated utilities to\n# get the market share\nshare &lt;- exp(utility) / sum(exp(utility))\n\n# Stick all of these in one final dataset\nbind_cols(share = share, logits = utility, example_product_mix)\n## # A tibble: 6 × 6\n##   share[,1] logits[,1] seat  cargo eng   price\n##       &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1    0.113      -1.35  7     2ft   hyb   30   \n## 2    0.433       0     6     2ft   gas   30   \n## 3    0.319      -0.306 8     2ft   gas   30   \n## 4    0.0728     -1.78  7     3ft   gas   40   \n## 5    0.0167     -3.26  6     2ft   elec  40   \n## 6    0.0452     -2.26  7     2ft   hyb   35\n\n\n\n\n\n\n\nFunction version of this kind of prediction\n\n\n\n\n\nOn p. 375 of Chapman and Feit (2019) (and at this Stan forum post), there’s a function called predict.mnl() that does this utility and share calculation automatically. Because this post is more didactic and because I’m more interested in the Bayesian approach, I didn’t use it earlier, but it works well.\n\npredict.mnl &lt;- function(model, data) {\n  # Function for predicting shares from a multinomial logit model \n  # model: mlogit object returned by mlogit()\n  # data: a data frame containing the set of designs for which you want to \n  #       predict shares. Same format at the data used to estimate model. \n  data.model &lt;- model.matrix(update(model$formula, 0 ~ .), data = data)[ , -1]\n  utility &lt;- data.model %*% model$coef\n  share &lt;- exp(utility) / sum(exp(utility))\n  cbind(share, data)\n}\n\npredict.mnl(model_minivans_mlogit, example_product_mix)\n##     share seat cargo  eng price\n## 1 0.11273    7   2ft  hyb    30\n## 2 0.43337    6   2ft  gas    30\n## 3 0.31918    8   2ft  gas    30\n## 4 0.07281    7   3ft  gas    40\n## 5 0.01669    6   2ft elec    40\n## 6 0.04521    7   2ft  hyb    35\n\n\n\n\nThis new predicted share column sums to one, and it shows us the predicted market share assuming these are the only six products available. The $30,000 six-seater 2ft gas van and the $30,000 eight-seater 2ft gas van would comprise more than 75% (0.43337 + 0.31918) of a market consisting of these six products. Because we did this calculation by hand, we lose all of {marginaleffects}’s extra features like standard errors and hypothesis tests. Alas.\n\n\nBayesian predictions\nIf we use the categorical multinomial {brms} model we run into the same issue of getting weird predictions. Here it shows that 2/3rds of predictions are 0, which makes sense—if a respondent is offered 10 iterations of 3 possible choices, that would be 30 total choices, but they can only choose one option per iteration, so 20 choices (or 20/30 or 2/3) wouldn’t be selected. The other three groups are each 11%, since that’s the remaining 33% divided evenly across three options. Neat, I guess, but still not super helpful.\n\navg_predictions(model_minivans_categorical_brms)\n## \n##  Group Estimate 2.5 % 97.5 %\n##      0    0.667 0.657  0.676\n##      1    0.109 0.103  0.115\n##      2    0.112 0.105  0.118\n##      3    0.113 0.106  0.119\n## \n## Columns: group, estimate, conf.low, conf.high\n\nInstead of going through the manual process of matrix-multiplying a dataset of some mix of products with a single set of coefficients, we can use predictions(..., type = \"link\") to get predicted values on the log-odds scale, or that utility value that we found before.\n\n\n\n\n\n\nmarginaleffects::predictions() vs. {tidybayes} functions\n\n\n\nWe can actually use either marginaleffects::predictions() or {tidybayes}’s *_draw() functions for these posterior predictions. They do the same thing, with slightly different syntax:\n\n# Logit-scale predictions with marginaleffects::predictions()\nmodel_minivans_categorical_brms %&gt;% \n  predictions(newdata = example_product_mix, re_formula = NA, type = \"link\") %&gt;% \n  posterior_draws()\n\n# Logit-scale predictions with tidybayes::add_linpred_draws()\nmodel_minivans_categorical_brms %&gt;% \n  add_linpred_draws(newdata = example_product_mix, re_formula = NA)\n\nEarlier in the chocolate example, I used marginaleffects::predictions() with the Bayesian {brms} model. Here I’m going to switch to the {tidybayes} prediction functions instead, in part because these multinomial models with the categorical() family are a lot more complex (though {marginaleffects} can handle them nicely), but mostly because in the actual paper I’m working on with real conjoint data, our MCMC results were generated with raw Stan code through rstan, and {marginaleffects} doesn’t support raw Stan models.\nCheck out this guide for the differences between {tidybayes}’s three general prediction functions: predicted_draws(), epred_draws(), and linpred_draws().\n\n\nAdditionally, we now actually have 4,000 draws in 3 categories (option 1, option 2, and option 3), so we actually have 12,000 sets of coefficients (!). To take advantage of the full posterior distribution of these coefficients, we can calculate shares within each set of draws within each of the three categories, resulting in a distribution of shares rather than single values.\n\ndraws_df &lt;- example_product_mix %&gt;% \n  add_linpred_draws(model_minivans_categorical_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes\n  group_by(.draw, .category) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nWe can summarize this huge dataset of posterior shares to get medians and credible intervals, but we need to do one extra step first. Right now, we have three predictions for each mix type, one for each of the categories (i.e. option 1, option 2, and option 3.\n\nshares_df %&gt;% \n  group_by(mix_type, .category) %&gt;% \n  median_qi(share)\n## # A tibble: 18 × 8\n##    mix_type      .category  share .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 1         0.0161 0.0123 0.0208   0.95 median qi       \n##  2 6 2ft elec 40 2         0.0238 0.0183 0.0302   0.95 median qi       \n##  3 6 2ft elec 40 3         0.0216 0.0168 0.0275   0.95 median qi       \n##  4 7 2ft hyb 35  1         0.0513 0.0400 0.0647   0.95 median qi       \n##  5 7 2ft hyb 35  2         0.0485 0.0379 0.0605   0.95 median qi       \n##  6 7 2ft hyb 35  3         0.0532 0.0424 0.0660   0.95 median qi       \n##  7 7 3ft gas 40  1         0.0693 0.0543 0.0876   0.95 median qi       \n##  8 7 3ft gas 40  2         0.0891 0.0705 0.111    0.95 median qi       \n##  9 7 3ft gas 40  3         0.0775 0.0615 0.0967   0.95 median qi       \n## 10 7 2ft hyb 30  1         0.117  0.0976 0.139    0.95 median qi       \n## 11 7 2ft hyb 30  2         0.107  0.0891 0.128    0.95 median qi       \n## 12 7 2ft hyb 30  3         0.124  0.104  0.146    0.95 median qi       \n## 13 8 2ft gas 30  1         0.326  0.292  0.363    0.95 median qi       \n## 14 8 2ft gas 30  2         0.314  0.282  0.348    0.95 median qi       \n## 15 8 2ft gas 30  3         0.299  0.267  0.331    0.95 median qi       \n## 16 6 2ft gas 30  1         0.418  0.380  0.457    0.95 median qi       \n## 17 6 2ft gas 30  2         0.416  0.379  0.454    0.95 median qi       \n## 18 6 2ft gas 30  3         0.423  0.387  0.462    0.95 median qi\n\nSince those options were all randomized, we can lump them all together as a single choice. To do this we’ll take the average share across the three categories (this is also called “marginalizing”) within each posterior draw.\n\nshares_marginalized &lt;- shares_df %&gt;% \n  # Marginalize across categories within each draw\n  group_by(mix_type, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ungroup()\n\nshares_marginalized %&gt;% \n  group_by(mix_type) %&gt;% \n  median_qi(share)\n## # A tibble: 6 × 7\n##   mix_type       share .lower .upper .width .point .interval\n##   &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6 2ft elec 40 0.0206 0.0173 0.0242   0.95 median qi       \n## 2 7 2ft hyb 35  0.0512 0.0435 0.0600   0.95 median qi       \n## 3 7 3ft gas 40  0.0788 0.0673 0.0915   0.95 median qi       \n## 4 7 2ft hyb 30  0.116  0.103  0.131    0.95 median qi       \n## 5 8 2ft gas 30  0.313  0.291  0.337    0.95 median qi       \n## 6 6 2ft gas 30  0.419  0.394  0.446    0.95 median qi\n\nAnd we can plot them:\n\nshares_marginalized %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(fill = clrs[10], normalize = \"xy\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\n\nThis is great because (1) it includes the uncertainty in the estimated shares, and (2) it lets us do neat Bayesian inference and say things like “there’s a 93% chance that in this market of 6 options, a $30,000 6-passenger gas minivan with 2 feet of storage would reach at least 40% market share”:\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  summarize(prop_greater_40 = sum(share &gt;= 0.4) / n())\n## # A tibble: 1 × 1\n##   prop_greater_40\n##             &lt;dbl&gt;\n## 1           0.931\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt;= 0.4)), fill = clrs[10]) +\n  geom_vline(xintercept = 0.4) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[10], 0.4), guide = \"none\") +\n  labs(x = \"Predicted market share\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nAs explained in the AMCEs section for the chocolate data, in the social sciences we’re less concerned about predicted market shares and more concerned about causal effects. Holding all other features constant, what is the effect of a $5,000 increase in price or moving from 2 feet → 3 feet of storage space on the probability (or favorability) of selecting a minivan?\nIn the chocolate example, we were able to use marginaleffects::avg_comparisons() with the Bayesian model and get categorical contrasts automatically. This was because we cheated and used a Poisson model, since those can secretly behave like multinomial models. For the frequentist {mlogit}-based model, we had to use marginaleffects::predictions() instead and specify a special by argument to collapse the predictions into the different contrasts we were interested in.\nIn this case, since both the frequentist and Bayesian models for minivans are true multinomial models, we have to return to {marginaleffects}’s special syntax that lets us pass a data frame as the by argument.\n\nFrequentist comparisons/contrasts\nTo help with the intuition behind this, since it’s more complex this time, we’ll first create a data frame with all 54 combinations of all the feature levels (3 seats × 2 cargos × 3 engines × 3 prices) and create a by column label that concatenates all the labels together so there’s a single unique label for each row:\n\nby_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price) %&gt;% \n  mutate(by = paste(seat, cargo, eng, price, sep = \"_\"))\nby_all_combos\n## # A tibble: 54 × 5\n##    seat  cargo eng   price by           \n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;        \n##  1 6     2ft   gas   30    6_2ft_gas_30 \n##  2 6     2ft   gas   35    6_2ft_gas_35 \n##  3 6     2ft   gas   40    6_2ft_gas_40 \n##  4 6     2ft   hyb   30    6_2ft_hyb_30 \n##  5 6     2ft   hyb   35    6_2ft_hyb_35 \n##  6 6     2ft   hyb   40    6_2ft_hyb_40 \n##  7 6     2ft   elec  30    6_2ft_elec_30\n##  8 6     2ft   elec  35    6_2ft_elec_35\n##  9 6     2ft   elec  40    6_2ft_elec_40\n## 10 6     3ft   gas   30    6_3ft_gas_30 \n## # ℹ 44 more rows\n\nWe can then feed this by_all_combos data frame into predictions(), which will generate predictions for all these levels collapsed by all three of the possible groups (i.e. option 1, option 2, and option 3). We can then split the by column back into separate columns for each of the feature levels so that we have those original columns back.\n\nall_preds_mlogit &lt;- predictions(\n  model_minivans_mlogit,\n  # predictions.mlogit() weirdly gets mad when working with tibbles :shrug:\n  by = as.data.frame(by_all_combos)\n) %&gt;%\n  # Split the `by` column up into separate columns\n  separate(by, into = c(\"seat\", \"cargo\", \"eng\", \"price\"))\nas_tibble(all_preds_mlogit)\n## # A tibble: 54 × 10\n##    estimate std.error statistic   p.value conf.low conf.high seat  cargo eng   price\n##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n##  1   0.348    0.0135       25.8 1.89e-146   0.321     0.374  6     2ft   elec  30   \n##  2   0.173    0.00907      19.0 7.82e- 81   0.155     0.190  6     2ft   elec  35   \n##  3   0.0872   0.00565      15.4 1.04e- 53   0.0761    0.0983 6     2ft   elec  40   \n##  4   0.695    0.0122       57.0 0           0.671     0.719  6     2ft   gas   30   \n##  5   0.491    0.0147       33.3 1.08e-243   0.462     0.520  6     2ft   gas   35   \n##  6   0.311    0.0130       23.8 1.40e-125   0.285     0.336  6     2ft   gas   40   \n##  7   0.499    0.0138       36.3 3.76e-288   0.472     0.526  6     2ft   hyb   30   \n##  8   0.300    0.0126       23.8 8.97e-125   0.275     0.324  6     2ft   hyb   35   \n##  9   0.161    0.00952      16.9 3.90e- 64   0.142     0.180  6     2ft   hyb   40   \n## 10   0.430    0.0147       29.2 2.13e-187   0.402     0.459  6     3ft   elec  30   \n## # ℹ 44 more rows\n\nThere are a lot of predicted probabilities here, so we need to collapse and average these by groups to make any sense of them. For instance, suppose we’re interested in the AMCE of cargo space. We can first find the average predicted probability of selection with some grouping and summarizing:\n\nmanual_cargo_example &lt;- all_preds_mlogit %&gt;% \n  group_by(cargo) %&gt;% \n  summarize(avg_pred = mean(estimate))\nmanual_cargo_example\n## # A tibble: 2 × 2\n##   cargo avg_pred\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 2ft      0.292\n## 2 3ft      0.375\n\ndiff(manual_cargo_example$avg_pred)\n## [1] 0.08326\n\nHolding all other features constant, the average probability (or average favorability, or average market share, or whatever we want to call it) of selecting a minivan with 2 feet of storage space is 0.292 (this is the average of the 27 predictions from all_preds_mlogit where cargo = 2ft); the average probability for a minivan with 3 feet of storage space is 0.375 (again, this is the average of the 27 predictions from all_preds_mlogit where cargo = 3ft). There’s an 8.3 percentage point difference between these groups. This is the causal effect or AMCE: switching from 2 feet to 3 feet increases minivan favorability by 8 percentage points on average.\nThis manual calculation works, but it’s tedious and doesn’t include anything about standard errors. So instead, we can do it automatically with predictions(..., by = data.frame(...)).\n\npreds_minivan_cargo_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    cargo = levels(minivans$cargo),\n    by = levels(minivans$cargo)\n  )\n)\npreds_minivan_cargo_mlogit\n## \n##  Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %  By\n##     0.291    0.00440 66.2   &lt;0.001 0.283  0.300 2ft\n##     0.375    0.00441 85.2   &lt;0.001 0.367  0.384 3ft\n## \n## Columns: estimate, std.error, statistic, p.value, conf.low, conf.high, by\n\nAnd if we use the hypothesis argument (or the standalone hypotheses() function), we can get the difference between these average predictions, or the AMCE we care about—moving from 2 feet → 3 feet causes an 8 percentage point increase in favorability.\n\nhypotheses(preds_minivan_cargo_mlogit, hypothesis = \"revpairwise\")\n## \n##       Term Estimate Std. Error   z Pr(&gt;|z|)  2.5 % 97.5 %\n##  3ft - 2ft   0.0837    0.00881 9.5   &lt;0.001 0.0664  0.101\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nThe hypothesis functionality works with more than 2 levels too. If calculate average predictions for the 3 seat configurations and specify pariwise or revpairwise, {marginaleffects} will give three differences: row 2 − row 1; row 3 − row 1; and row 3 − row 2. If we specify reference or revreference, it’ll only give two, all based on the first row.\n\npreds_minivan_seat_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    seat = levels(minivans$seat),\n    by = levels(minivans$seat)\n  )\n)\n\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"revpairwise\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6  -0.0996     0.0107 -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6  -0.0557     0.0109 -5.11   &lt;0.001 -0.0771 -0.0343\n##  8 - 7   0.0439     0.0106  4.13   &lt;0.001  0.0230  0.0647\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"reference\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6  -0.0996     0.0107 -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6  -0.0557     0.0109 -5.11   &lt;0.001 -0.0771 -0.0343\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can specify any other comparisons with bN, where N stands for the row number from predictions(). For instance, if we just want the difference between 6 (row 1) and 8 (row 2), we can do this:\n\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"b2 = b1\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 %  97.5 %\n##  b2=b1  -0.0996     0.0107 -9.29   &lt;0.001 -0.121 -0.0786\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can make a big data frame with all the AMCEs we’re interested in. I’ve hidden the code here because it’s really repetitive.\n\n\nMake separate datasets of predictions and combine them in one data frame\npreds_minivan_seat_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    seat = levels(minivans$seat),\n    by = levels(minivans$seat)\n  )\n)\n\npreds_minivan_cargo_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    cargo = levels(minivans$cargo),\n    by = levels(minivans$cargo)\n  )\n)\n\npreds_minivan_eng_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    eng = levels(minivans$eng),\n    by = levels(minivans$eng)\n  )\n)\n\npreds_minivan_price_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    price = levels(minivans$price),\n    by = levels(minivans$price)\n  )\n)\n\namces_minivan_mlogit &lt;- bind_rows(\n  seat = hypotheses(preds_minivan_seat_mlogit, hypothesis = \"reference\"),\n  cargo = hypotheses(preds_minivan_cargo_mlogit, hypothesis = \"reference\"),\n  eng = hypotheses(preds_minivan_eng_mlogit, hypothesis = \"reference\"),\n  # Need to specify these two tests manually because `hypotheses()` reeeeallly\n  # wants to use 35 as the reference level because it's the first value that\n  # appears in the original data. See\n  # https://github.com/vincentarelbundock/marginaleffects/issues/861\n  price = bind_rows(\n    hypotheses(preds_minivan_price_mlogit, hypothesis = \"b1 = b2\") %&gt;% mutate(term = \"35 - 30\"),\n    hypotheses(preds_minivan_price_mlogit, hypothesis = \"b3 = b2\") %&gt;% mutate(term = \"40 - 30\")\n  ),\n  .id = \"variable\"\n)\n\n\n\namces_minivan_mlogit\n## \n##        Term Estimate Std. Error      z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6       -0.0996    0.01072  -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6       -0.0557    0.01091  -5.11   &lt;0.001 -0.0771 -0.0343\n##  3ft - 2ft    0.0837    0.00881   9.50   &lt;0.001  0.0664  0.1010\n##  gas - elec   0.2785    0.01021  27.28   &lt;0.001  0.2585  0.2985\n##  hyb - elec   0.1156    0.01032  11.20   &lt;0.001  0.0954  0.1358\n##  35 - 30      0.1767    0.01117  15.82   &lt;0.001  0.1548  0.1986\n##  40 - 30     -0.1333    0.01014 -13.14   &lt;0.001 -0.1532 -0.1134\n## \n## Columns: variable, term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n\n\nBayesian comparisons/contrasts\nUnlike the chocolate example, where the outcome variable was binary, we have to do similar by-like shenanigans with the Bayesian minivan model here. We could theoretically work with things like marginaleffects::comparisons() or marginaleffects::slopes() to extract the AMCEs from the model, but as I’ll show below, there are some weird mathy things we have to deal with because of the multinomial outcome, and I think it’s beyond what {marginaleffects} is designed to easily do.\nSo instead we can use epred_draws() from {tidybayes} and calculate posterior predictions ourselves (see this guide for an overview of all of {tidybayes}’s different prediction functions).\nTo illustrate why predicting things with this multinomial model is so weird, we’ll first predict the probability that someone chooses a $30,000 6-seater electric van with 2 feet of storage space. For this combination of minivan characteristics, there’s a 66% chance that someone does not select it, shown as category 0. That means there’s a 33% chance that someone does select it. Because options 1, 2 and 3 were randomized, that 33% is split evenly across categories 1, 2, and 3 in the predictions here.\n\none_prediction &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = data.frame(\n    seat = \"6\", cargo = \"2ft\", eng = \"elec\", price = \"30\", resp.id = 1)\n  )\n\none_prediction %&gt;% \n  group_by(.category) %&gt;% \n  median_qi(.epred)\n## # A tibble: 4 × 7\n##   .category .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0          0.661 0.629   0.694   0.95 median qi       \n## 2 1          0.104 0.0843  0.128   0.95 median qi       \n## 3 2          0.112 0.0894  0.138   0.95 median qi       \n## 4 3          0.121 0.0990  0.147   0.95 median qi\n\nWe could add the predictions for categories 1, 2, and 3 together, but that would take a bit of extra data manipulation work. Instead, we can rely on the the fact that the prediction for category 0 is actually the inverse of the sum of categories 1+2+3, so we can instead just use 1 - .epred and only look at category 0. Even though the category column here says 0, it’s really the combined probability of choosing options 1, 2, or 3:\n\none_prediction %&gt;% \n  mutate(.epred = 1 - .epred) %&gt;%\n  filter(.category == 0) %&gt;% \n  median_qi(.epred)\n## # A tibble: 1 × 13\n##   seat  cargo eng   price resp.id  .row .category .epred .lower .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     2ft   elec  30          1     1 0          0.339  0.306  0.371   0.95 median qi\n\nWith {mlogit}, we found AMCEs by essentially calculating marginal means for specific contrasts of predicted probabilities. We created a data frame of all 54 combinations of feature levels and then grouped and summarized that data frame as needed (e.g., the average of the 27 predictions for 2 feet of cargo space and the average of the 27 predictions for 3 feed of cargo space).\nWe can do the same thing with the {brms} model, but selecting only the 0 category and reversing the predicted value:\n\nnewdata_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price) %&gt;% \n  mutate(resp.id = 1)\n\nall_preds_brms &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = newdata_all_combos) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nTo make sure it worked, here are the posterior medians for all the different levels. It’s roughly the same as what we found with in all_preds_mlogit:\n\nall_preds_brms %&gt;% \n  group_by(seat, cargo, eng, price) %&gt;% \n  median_qi(.epred)\n## # A tibble: 54 × 10\n##    seat  cargo eng   price .epred .lower .upper .width .point .interval\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6     2ft   gas   30    0.682  0.651   0.713   0.95 median qi       \n##  2 6     2ft   gas   35    0.485  0.451   0.520   0.95 median qi       \n##  3 6     2ft   gas   40    0.305  0.275   0.338   0.95 median qi       \n##  4 6     2ft   hyb   30    0.502  0.466   0.537   0.95 median qi       \n##  5 6     2ft   hyb   35    0.306  0.276   0.338   0.95 median qi       \n##  6 6     2ft   hyb   40    0.171  0.150   0.194   0.95 median qi       \n##  7 6     2ft   elec  30    0.339  0.306   0.371   0.95 median qi       \n##  8 6     2ft   elec  35    0.183  0.161   0.207   0.95 median qi       \n##  9 6     2ft   elec  40    0.0952 0.0819  0.110   0.95 median qi       \n## 10 6     3ft   gas   30    0.769  0.743   0.795   0.95 median qi       \n## # ℹ 44 more rows\n\nTo pull out specific group-level averages, we can group and summarize. For example, here are the posterior median predictions for the two levels of cargo space:\n\nall_preds_brms %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi(.epred)\n## # A tibble: 2 × 7\n##   cargo .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft    0.256 0.0606  0.676   0.95 median qi       \n## 2 3ft    0.348 0.0905  0.763   0.95 median qi\n\nThe medians here are correct and basically what we found with {mlogit}, but the credible intervals are wildly off (5% to 75% favorability?!). If we plot this we can see why:\n\nall_preds_brms %&gt;% \n  ggplot(aes(x = .epred, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\n\nhahahaha check out those mountain ranges. All those peaks come from combining the 27 different 2ft- and 3ft- posterior distributions for all the different combinations of other feature levels.\nTo get the actual marginal mean for cargo space, we need to marginalize out (or average out) all those other covariates. To do this, we need to group by the cargo column and the .draw column so that we find the average within each set of MCMC draws. To help with the intuition, look how many rows are in each of these groups of cargo and .draw—there are 27 different estimates for each of the 4,000 draws for 2 feet and 27 different estimates for each of the 4,000 draws for 3 feet. We want to collapse (or marginalize) those 27 rows into just one average.\n\nall_preds_brms %&gt;% \n  group_by(cargo, .draw) %&gt;% \n  summarize(nrows = n())\n## # A tibble: 8,000 × 3\n## # Groups:   cargo [2]\n##    cargo .draw nrows\n##    &lt;fct&gt; &lt;int&gt; &lt;int&gt;\n##  1 2ft       1    27\n##  2 2ft       2    27\n##  3 2ft       3    27\n##  4 2ft       4    27\n##  5 2ft       5    27\n##  6 2ft       6    27\n##  7 2ft       7    27\n##  8 2ft       8    27\n##  9 2ft       9    27\n## 10 2ft      10    27\n## # ℹ 7,990 more rows\n\nTo do that, we can find the average predicted value in those groups, then work with that as our main estimand. Check out these marginalized-out posteriors now—the medians are the same as before, but the credible intervals make a lot more sense:\n\npreds_cargo_marginalized &lt;- all_preds_brms %&gt;% \n  # Marginalize out the other covariates\n  group_by(cargo, .draw) %&gt;%\n  summarize(avg = mean(.epred))\n\npreds_cargo_marginalized %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi()\n## # A tibble: 2 × 7\n##   cargo   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft   0.292  0.275  0.309   0.95 median qi       \n## 2 3ft   0.374  0.356  0.393   0.95 median qi\n\nWe can confirm that marginalizing out the other covariates worked by plotting it:\n\npreds_cargo_marginalized %&gt;% \n  ggplot(aes(x = avg, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\n\nheck. yes.\nFinally, we’re actually most interested in the AMCE, or the difference between these two cargo sizes. The compare_levels() function from {tidybayes} can calculate this automatically:\n\npreds_cargo_marginalized %&gt;%\n  compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n  median_qi(avg)\n## # A tibble: 1 × 7\n##   cargo        avg .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 3ft - 2ft 0.0830 0.0643  0.100   0.95 median qi\n\nThat’s it! The causal effect of moving from 2 feet → 3 feet of storage space, holding all other features constant, is 8 percentage points (with a 95% credible interval of 6.5 to 10 percentage points).\nWe can combine all these AMCEs into a huge data frame. The marginalization process + compare_levels() has to happen with one feature at a time, so we need to create several separate data frames:\n\n# I could probably do this with purrr::map() to reduce all this repetition, but\n# whatever, it works\namces_minivan_brms &lt;- bind_rows(\n  seat = all_preds_brms %&gt;% \n    group_by(seat, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms %&gt;% \n    group_by(cargo, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms %&gt;% \n    group_by(eng, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms %&gt;% \n    group_by(price, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_brms %&gt;% \n  group_by(term, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 7 × 8\n##   term  contrast       avg  .lower  .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo 3ft - 2ft   0.0830  0.0643  0.100    0.95 median qi       \n## 2 eng   elec - gas -0.279  -0.300  -0.256    0.95 median qi       \n## 3 eng   hyb - gas  -0.161  -0.184  -0.138    0.95 median qi       \n## 4 price 35 - 30    -0.178  -0.201  -0.155    0.95 median qi       \n## 5 price 40 - 30    -0.309  -0.330  -0.287    0.95 median qi       \n## 6 seat  7 - 6      -0.0995 -0.121  -0.0785   0.95 median qi       \n## 7 seat  8 - 6      -0.0570 -0.0788 -0.0355   0.95 median qi\n\n\n\nPlots\nAgain, plotting these AMCEs so that there’s a reference category at 0 requires some extra data work, so I’ve hidden all that code for the sake of space.\n\n\nExtract variable labels\nminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plot\namces_minivan_mlogit_split &lt;- amces_minivan_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- minivan_var_levels %&gt;%\n  left_join(\n    amces_minivan_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make {brms} plot\nposterior_mfx_minivan_nested &lt;- amces_minivan_brms %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_bayes &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_mfx_minivan_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_minivan_bayes, aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 + p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nWe’ve cheated a little and have already used multilevel structures in the Bayesian models for the chocolate experiment and the minivan experiment. This was because these datasets had a natural panel grouping structure inside them. {mlogit} can work with panel-indexed data frames (that’s the point of that strange dfidx() function). By creating respondent-specific intercepts like we did with the {brms} models, we helped account for some of the variation caused by respondent differences.\nBut we can do better than that and get far richer and more complex models and estimates and predictions. In addition to using respondent-specific intercepts, we can (1) include respondent-level characteristics as covariates, and (2) include respondent-specific slopes for the minivan characteristic.\nIn the minivan data, we have data on feature levels (seat, cargo, eng, price) and on individual characteristics (carpool). The carpool variable indicates if the respondent uses their vehicle for carpooling. This is measured at the respondent level and not the choice level (i.e. someone won’t stop being a carpooler during one set of choices and then resume being a carpooler for another set). We can visualize where these different columns are measured by returning to the hierarchical model diagram:\n\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents, w ith variables measured at different levels\n\n\n\n\nWe can use hierarchical models (or multilevel models, or mixed effects models, or whatever you want to call them) to account for choice-level and respondent-level covariates and incorporate respondent-level heterogeneity and covariance into the model estimates.\n\n\n\nImage by Chelsea Parlett-Pelleriti"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Important sidenote on notation",
    "text": "Important sidenote on notation\nBut before looking at how to incorporate that carpool column into the model, we need to take a quick little detour into the world of notation. There’s no consistent way of writing out multilevel models,1 and accordingly, I thought it was impossible to run fully specified marketing-style hierarchical Bayesian models with {brms}—all because of notation!1 These are not the only approaches—section 12.5 in Gelman and Hill (2007) is called “Five ways to write the same model,” and they don’t include the offset notation as one of their five!\nThere are a couple general ways I’ve seen group-level random effects written out in formal model notation: one with complete random β terms and one with random offsets from a global β term.\n\n\n\n\n\n\n{brms} / {lme4} syntax\n\n\n\nFor the best overview of how to use {brms} and {lme4} with different random group-level intercept and slope specifications, check out this summary table by Ben Bolker.\n\n\n\nRandom intercepts\nIf you want group-specific intercept terms, you can use a formula like this:\nbf(y ~ x + (1 | group))\nIn formal mathy terms, we can write this group-specific intercept as a complete coefficient: \\(\\beta_{0_j}\\). Each group \\(j\\) gets its own intercept coefficient. Nice and straightforward.\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Random group-specific intercepts}\n\\end{aligned}\n\\]\nHowever, I actually like to think of these random effects in a slightly different way, where each group intercept is actually a combination of a global average (\\(\\beta_0\\)) and a group-specific offset from that average (\\(b_{0_j}\\)), like this:\n\\[\n\\beta_{0_j} = \\beta_0 + b_{0_j}\n\\]\nThat offset is assumed to be normally distributed with a mean of 0 (\\(\\mathcal{N}(0, \\sigma_0)\\)):\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random group-specific offsets from global intercept}\n\\end{aligned}\n\\]\nI prefer this offset notation because it aligns with the output of {brms}, which reports population-level coefficients (i.e. the global average \\(\\beta_0\\)) along with group-specific offsets from that average (i.e. \\(b_{0_j}\\)), which you can access with ranef(model_name).\n\n\nRandom slopes\nIf you want group-specific intercepts and slopes, you can use a formula like this:\nbf(y ~ x + (1 + x | group))\nThe same dual syntax applies when using random slopes too. We can either use whole group-specific \\(\\beta_{n_j}\\) terms, or use offsets (\\(b_{n_j}\\)) from a global average slope (\\(\\beta_n\\)). When working with random slopes, the math notation gets a little fancier because the random intercept and slope terms are actually correlated and move together across groups. The \\(\\beta\\) terms come from a multivariate (or joint) normal distribution with shared covariance.\nWith the complete β approach, we’re estimating the joint distribution of \\(\\begin{pmatrix} \\beta_{0_j} \\\\ \\beta_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  \\beta_{0_j} \\\\\n  \\beta_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific slopes and intercepts}\n\\end{aligned}\n\\]\n\nWith the offset approach, we’re estimating the joint distribution of the offsets from the global intercept and slope, or \\(\\begin{pmatrix} b_{0_j} \\\\ b_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  b_{0_j} \\\\\n  b_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    0 \\\\\n    0 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific offsets from global intercept and slope}\n\\end{aligned}\n\\]\n\n\n\nSummary table\nAnd here’s a helpful little table summarizing these two types of notation (mostly for future me).\n\n\n\n\n\n\n\n\n\n\n\nFormula syntax\nFull \\(\\beta\\) notation\nOffset notation\n\n\n\n\nRandom intercept\ny ~ x + (1 | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                       \\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                      \\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0)                                                                                                                                                                                                                                              \n                                                                                                                                       \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            b_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0)                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]\n\n\nRandom intercept + slope\ny ~ x + (1 + x | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                       \\begin{array}{c}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                                                        \\beta_{0_j} \\\\                                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                         \\beta_{1_j}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                           &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                            \n                                                                                                                                                                                                                                                                                                                           \\beta_0 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\beta_1 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                          \\end{array}                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\right)                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                         , \\, \\Sigma                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\right) \\\\                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                        \\Sigma &\\sim                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\begin{array}{cc}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                       \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                   \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\end{aligned}                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} \\\\                                                                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           b_{0_j} \\\\                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             b_{1_j}                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\left(                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\begin{array}{c}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\end{array}                                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\right)                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           , \\, \\Sigma                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\right) \\\\                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\Sigma &\\sim                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{cc}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\end{aligned}                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Translating from marketing-style Stan notation to {brms} syntax",
    "text": "Translating from marketing-style Stan notation to {brms} syntax\nIn Chapman and Feit (2019) and in all the marketing papers I’ve seen that use hierarchical Bayesian models—and even one I coauthored! (Chaudhry, Dotson, and Heiss 2021) (see the appendix)—they define their models using notation like this:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nFor the longest time this threw me off because it’s slightly different from the two different notations we just reviewed (full βs vs. offsets from global β), and I figured that specifying a model like this with {brms} was impossible. The main reason for my confusion is that there are two different datasets involved in this model, and {brms} can only really work with one dataset.\nIn raw Stan (like in this tutorial on conjoint hierarchical Bayes models, or in this example of a different conjoint hierarchical model), you’d typically work with two different datasets or matrices: one \\(X\\) with feature levels and one \\(Z\\) with respondent-level characteristics. (This is actually the recommended way to write hierarchical models in raw Stan!).\nHere’s what separate \\(X\\) and \\(Z\\) matrices would look like with the minivan data—X contains the full data without respondent-level covariates like carpool and it has 9,000 rows; Z contains only respondent-level characteristics like carpool and it only has 200 rows (one per respondent).\n\nX &lt;- minivans %&gt;% select(-carpool)\nX\n## # A tibble: 9,000 × 8\n##    resp.id  ques   alt seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 6     2ft   gas   35         0\n##  2       1     1     2 8     3ft   hyb   30         0\n##  3       1     1     3 6     3ft   gas   30         1\n##  4       1     2     1 6     2ft   gas   30         0\n##  5       1     2     2 7     3ft   gas   35         1\n##  6       1     2     3 6     2ft   elec  35         0\n##  7       1     3     1 8     3ft   gas   35         1\n##  8       1     3     2 7     3ft   elec  30         0\n##  9       1     3     3 8     2ft   elec  40         0\n## 10       1     4     1 7     3ft   elec  40         1\n## # ℹ 8,990 more rows\n\nZ &lt;- minivans %&gt;% \n  # Only keep the first row of each respondent\n  group_by(resp.id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% \n  # Only keep the respondent-level columns\n  select(resp.id, carpool)\nZ\n## # A tibble: 200 × 2\n##    resp.id carpool\n##      &lt;dbl&gt; &lt;fct&gt;  \n##  1       1 yes    \n##  2       2 no     \n##  3       3 no     \n##  4       4 no     \n##  5       5 yes    \n##  6       6 no     \n##  7       7 no     \n##  8       8 yes    \n##  9       9 no     \n## 10      10 no     \n## # ℹ 190 more rows\n\nX and Z are then passed to Stan as separate matrices and used at different places in the model fitting process. Here’s what that looks like in pseudo-Stan code. The matrix of individual characteristics Z is matrix-multiplied with a bunch of estimated \\(\\gamma\\) coefficients (Gamma here) to generate individual-specific \\(\\beta\\) coefficients (Beta here). The matrix of choices X is then matrix-multiplied with the individual-specific \\(\\beta\\) coefficients to generate predicted outcomes (Y here).\nfor (r in 1:n_respondents) {\n  // All the individual-specific slopes and intercepts\n  Beta[,r] ~ multi_normal(Gamma * Z[,r], ...);\n\n  // All the question-level outcomes, using individual-specific slopes and intercepts\n  for (s in 1:n_questions) {\n     Y[r,s] ~ categorical_logit( X [r,s] * Beta[,r]);\n  }\n}\nThat’s a neat way of working with multilevel models, but it’s different from how I’ve always worked with them (and it requires working with raw Stan). As seen throughout this post, I’m a fan of {brms}’s formula-style syntax for specifying multilevel models, but {brms} can only work with one dataset at a time—you can’t pass it both X and Z like you’d do with raw Stan. So I (naively) figured that this went beyond {brms}’s abilities and was only possible with raw Stan.\nHowever, if we use {brms}’s special formula syntax, we can actually specify an identical model with only one dataset (again, see this for a fantastic overview of the syntax).\nFirst, let’s look at the marketing-style syntax again:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nThis is actually just a kind of really compact notation. That second line with the \\(\\beta \\sim \\operatorname{Multivariate}\\, \\mathcal{N}(\\cdot)\\) distribution is a shorthand version of the full-β syntax from earlier. To illustrate this, let’s expand this out to a more complete formal definition of the model. Instead of using \\(X\\) to stand in for all the feature levels and \\(Z\\) for all the individual characteristics, we’ll expand those to include all the covariates we’re using. And instead of calling the distribution “Multinomial logit” we’ll call it “Categorical” so it aligns with Stan. It’ll make for a really massive formula, but it shows what’s really going on.\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right]\n\\end{aligned}\n\\]\n\n \nImportantly, pay attention to where the choice-level and respondent-level variables show up in this expanded version. All the choice-level variables have respondent-specific \\(\\beta\\) coefficients, while the respondent-level variable (carpool) is down in that massive multivariate normal matrix with its own \\(\\gamma\\) coefficients, helping determine the respondent-specific \\(\\beta\\) coefficients. That’s great and exactly what we want, and we can do that with raw Stan, but raw Stan is no fun.\nWe can create this exact same model structure with {brms} like this:\nbf(choice_alt ~\n  # Choice-level predictors that are nested within respondents...\n  (seat + cargo + eng + price) *\n  # ...interacted with all respondent-level predictors...\n  (carpool) +\n  # ... with random respondent-specific slopes for the\n  # nested choice-level predictors\n  (1 + seat + cargo + eng + price | resp.id))\nWe can confirm that this worked by using the miraculous {equatiomatic} package, which automatically converts model objects into LaTeX code. {equatiomatic} doesn’t work with {brms} models, but it does work with frequentist {lme4} models, so we can fit a throwaway frequentist model with this syntax (it won’t actually converge and it’ll give a warning, but that’s fine—we don’t actually care about this model) and then feed it to equatiomatic::extract_eq() to see what it looks like in formal notation.\n(This is actually how I figured out the correct combination of interactions and random slopes—I kept trying different combinations that I thought were right until the math matched the huge full model above, with the \\(\\beta\\) and \\(\\gamma\\) terms in the right places.)\n\nlibrary(lme4)\nlibrary(equatiomatic)\n\nmodel_throwaway &lt;- lmer(\n  choice ~ (seat + cargo + eng + price) * (carpool) +\n    (1 + seat + cargo + eng + price | resp.id),\n  data = minivans\n)\n\nprint(extract_eq(model_throwaway))\n\\[\n\\begin{aligned}\n  \\operatorname{choice}_{i}  &\\sim N \\left(\\mu, \\sigma^2 \\right) \\\\\n    \\mu &=\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{seat}_{\\operatorname{7}}) + \\beta_{2j[i]}(\\operatorname{seat}_{\\operatorname{8}}) + \\beta_{3j[i]}(\\operatorname{cargo}_{\\operatorname{3ft}}) + \\beta_{4j[i]}(\\operatorname{eng}_{\\operatorname{hyb}}) + \\beta_{5j[i]}(\\operatorname{eng}_{\\operatorname{elec}}) + \\beta_{6j[i]}(\\operatorname{price}_{\\operatorname{35}}) + \\beta_{7j[i]}(\\operatorname{price}_{\\operatorname{40}}) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j} \\\\\n      &\\beta_{2j} \\\\\n      &\\beta_{3j} \\\\\n      &\\beta_{4j} \\\\\n      &\\beta_{5j} \\\\\n      &\\beta_{6j} \\\\\n      &\\beta_{7j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}})\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} & \\rho_{\\alpha_{j}\\beta_{2j}} & \\rho_{\\alpha_{j}\\beta_{3j}} & \\rho_{\\alpha_{j}\\beta_{4j}} & \\rho_{\\alpha_{j}\\beta_{5j}} & \\rho_{\\alpha_{j}\\beta_{6j}} & \\rho_{\\alpha_{j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{2j}\\alpha_{j}} & \\rho_{\\beta_{2j}\\beta_{1j}} & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{3j}\\alpha_{j}} & \\rho_{\\beta_{3j}\\beta_{1j}} & \\rho_{\\beta_{3j}\\beta_{2j}} & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{4j}\\alpha_{j}} & \\rho_{\\beta_{4j}\\beta_{1j}} & \\rho_{\\beta_{4j}\\beta_{2j}} & \\rho_{\\beta_{4j}\\beta_{3j}} & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{5j}\\alpha_{j}} & \\rho_{\\beta_{5j}\\beta_{1j}} & \\rho_{\\beta_{5j}\\beta_{2j}} & \\rho_{\\beta_{5j}\\beta_{3j}} & \\rho_{\\beta_{5j}\\beta_{4j}} & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{6j}\\alpha_{j}} & \\rho_{\\beta_{6j}\\beta_{1j}} & \\rho_{\\beta_{6j}\\beta_{2j}} & \\rho_{\\beta_{6j}\\beta_{3j}} & \\rho_{\\beta_{6j}\\beta_{4j}} & \\rho_{\\beta_{6j}\\beta_{5j}} & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{7j}\\alpha_{j}} & \\rho_{\\beta_{7j}\\beta_{1j}} & \\rho_{\\beta_{7j}\\beta_{2j}} & \\rho_{\\beta_{7j}\\beta_{3j}} & \\rho_{\\beta_{7j}\\beta_{4j}} & \\rho_{\\beta_{7j}\\beta_{5j}} & \\rho_{\\beta_{7j}\\beta_{6j}} & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for resp.id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nDifferent variations of group-level interactions\nThis syntax is a special R shortcut for interacting carpool with each of the feature variables:\n# Short way\n(seat + cargo + eng + price) * (carpool)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool\nIf we had other respondent-level columns like age (age) and education (ed), the shortcut syntax is really helpful:\n# Short way\n(seat + cargo + eng + price) * (carpool + age + ed)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool +\nseat*age + cargo*age + eng*age + price*age +\nseat*ed + cargo*ed + eng*ed + price*ed\nWe don’t necessarily need to fully interact everything. For instance, if we have theoretical reasons to think that carpool status is associated with seat count preferences, but not other features, we can only interact seat and carpool:\nbf(choice ~ \n    (seat * carpool) + cargo + eng + price + \n  (1 + seat + cargo + eng + price | resp.id))\n\n\n\n\n\n\nModel running times\n\n\n\nThe more individual-level interactions you add, the longer it will take for the model to run. As we’ll see below, interacting carpool with the four feature levels takes ≈30 minutes to fit. As you add more individual-level interactions, the running time blows up.\nIn the replication code for Jensen et al. (2021), where they model a ton of individual variables, they say it takes several days to run. Our models in Chaudhry, Dotson, and Heiss (2021) take hours and hours to run."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "mlogit model",
    "text": "mlogit model\n{mlogit} can estimate hierarchical models with something like this:\n\n# Define the random parameters\nmodel_mlogit_rpar &lt;- rep(\"n\", length = length(model_minivans_mlogit$coef))\nnames(model_mlogit_rpar) &lt;- names(model_minivans_mlogit$coef)\n\n# This means these random terms are all normally distributed\nmodel_mlogit_rpar\n#&gt; seat7    seat8 cargo3ft   enghyb  engelec  price35  price40 \n#&gt;   \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\" \n\n# Run the model with carpool as an individual-level covariate\nmodel_mlogit_hierarchical &lt;- mlogit(\n  choice ~ 0 + seat + eng + cargo + price | carpool,\n  data = minivans_idx,\n  panel = TRUE, rpar = model_mlogit_rpar, correlation = TRUE\n)\n\n# Show the results\nsummary(model_mlogit_hierarchical)\n\nHowever, I’m not a frequentist and I’m already not a huge fan of extracting the predictions and AMCEs from these {mlogit} models. Running and interpreting and working with the results of that object is left as an exercise for the reader :). (See p. 381 in Chapman and Feit (2019) for a worked example of how to do it.)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Finally, the full {brms} model",
    "text": "Finally, the full {brms} model\nThis is a really complex model with a ton of moving parts, but it’s also incredibly powerful. It lets us account for individual-specific differences across each of the minivan features. For instance, whether an individual carpools probably influences their preferences for the number of seats, and maybe cargo space, but probably doesn’t influence their preferences for engine type. If we had other individual-level characteristics, we could also let those influence feature preferences. Like, the number of kids an individual has probably influences seat count preferences; the individual’s income probably influences their price preferences; and so on.\nLet’s define the model more formally again, this time with priors for the parameters we’ll be estimating:\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right] \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for choice-level coefficients}] \\\\\n\\gamma^{\\beta_{0 \\dots 7}}_0 \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for individual-level coefficients}] \\\\\n\\sigma_{\\beta_{0 \\dots 7}} \\sim&\\ \\operatorname{Exponential}(1) \\qquad [\\text{Prior for between-respondent intercept and slope variability}] \\\\\n\\rho \\sim&\\ \\operatorname{LKJ}(1) \\qquad\\qquad [\\text{Prior for correlation between random slopes and intercepts}]\n\\end{aligned}\n\\]\n\n \nHere it is in code form. There are a couple new things here in the Stan settings. First, we’re going to create 4 MCMC chains with 4,000 draws rather than 2,000—there are so many parameters to be estimated that we need to let the simulation run longer. Second, we’ve modified the adapt_delta setting to 0.99. Conceptually, this adjusts the size of the steps the MCMC algorithm takes as it traverses the posterior space for each parameter—higher numbers make the steps smaller and more granular. This slows down the MCMC simulation, but it also helps avoid divergent transitions (or failed out-of-bounds draws).\nOn my 2021 M1 MacBook Pro, running through cmdstanr with 2 CPU cores per chain, it took about 30 minutes to fit. If you’re following along with this post, start running this and go get some lunch or go for a walk or something.\n\n\n\n\n\n\nPre-run model\n\n\n\nAlternatively, you can download an .rds file of this completed. This brm() code load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\n\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\n\n\n\n\nmodel_minivans_mega_mlm_brms &lt;- brm(\n  bf(choice_alt ~\n    # Choice-level predictors that are nested within respondents...\n    (seat + cargo + eng + price) *\n    # ...interacted with all respondent-level predictors...\n    (carpool) +\n    # ... with random respondent-specific slopes for the\n    # nested choice-level predictors\n    (1 + seat + cargo + eng + price | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3),\n    prior(lkj(1), class = cor)\n  ),\n  chains = 4, cores = 4, warmup = 1000, iter = 5000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), # refresh = 0,\n  control = list(adapt_delta = 0.9),\n  file = \"models/model_minivans_mega_mlm_brms\"\n)\n\nThis model is incredibly rich. We just estimated more than 5,000 parameters (!!!)—we have three sets of coefficients for each of the three options, and those are all interacted with carpool, plus we have individual-specific offsets to each of those coefficients, plus all the \\(\\rho\\) terms in that massive correlation matrix.\n\nlength(get_variables(model_minivans_mega_mlm_brms))\n## [1] 5156\n\nSince we’re dealing with interaction terms, these raw log odds coefficients are even less helpful on their own. It’s nearly impossible to interpret these coefficients in any meaningful way—there’s no point in even trying to combine each of the individual parts of each effect (random parts, interaction parts, etc.). The only way we’ll be able to interpret these things is by making predictions.\n\nminivans_mega_marginalized &lt;- model_minivans_mega_mlm_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_mega_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi(.value)\n## # A tibble: 16 × 7\n##    .variable            .value  .lower  .upper .width .point .interval\n##    &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 Intercept           -0.127  -0.281   0.0321   0.95 median qi       \n##  2 cargo3ft             0.405   0.290   0.521    0.95 median qi       \n##  3 cargo3ft:carpoolyes  0.162  -0.0488  0.380    0.95 median qi       \n##  4 carpoolyes          -0.718  -1.00   -0.437    0.95 median qi       \n##  5 engelec             -1.59   -1.77   -1.42     0.95 median qi       \n##  6 engelec:carpoolyes   0.0852 -0.211   0.375    0.95 median qi       \n##  7 enghyb              -0.774  -0.910  -0.640    0.95 median qi       \n##  8 enghyb:carpoolyes   -0.0543 -0.307   0.196    0.95 median qi       \n##  9 price35             -0.812  -0.947  -0.675    0.95 median qi       \n## 10 price35:carpoolyes  -0.164  -0.414   0.0845   0.95 median qi       \n## 11 price40             -1.58   -1.74   -1.43     0.95 median qi       \n## 12 price40:carpoolyes  -0.248  -0.528   0.0320   0.95 median qi       \n## 13 seat7               -0.879  -1.03   -0.735    0.95 median qi       \n## 14 seat7:carpoolyes     1.14    0.872   1.41     0.95 median qi       \n## 15 seat8               -0.646  -0.794  -0.501    0.95 median qi       \n## 16 seat8:carpoolyes     1.13    0.859   1.40     0.95 median qi"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the non-hierarchical model earlier, we made marketing-style predictions by thinking of a product mix and figuring out the predicted utility and market share of each product in that mix. We can do the same thing here, but now we can incorporate individual-level characteristics too.\nHere’s our example product mix again, but this time we’ll repeat it twice—once with carpool set to “yes” and once with it set to “no”. This will let us see the predicted market share for each mix of products for a market of only carpoolers and only non-carpoolers.\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n)\n\nproduct_mix_carpool &lt;- bind_rows(\n  mutate(example_product_mix, carpool = \"yes\"),\n  mutate(example_product_mix, carpool = \"no\")\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\n\nWe can now go through the same process from earlier where we get logit-scale predictions for this smaller dataset and find the shares inside each draw + category (options 1, 2, and 3) + carpool status group.\n\ndraws_df &lt;- product_mix_carpool %&gt;% \n  add_linpred_draws(model_minivans_mega_mlm_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes across both levels of carpooling\n  group_by(.draw, .category, carpool) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nThis new dataset contains 576,000 (!!) rows: 6 products × 2 carpool types × 3 \\(\\mu\\)-specific sets of coefficients × 16,000 MCMC draws. We can summarize this to get posterior medians and credible intervals, making sure to find the average share across the three outcomes (choice 1, 2, and 3), or marginalizing across the outcome.\n\nshares_df %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  median_qi(share)\n## # A tibble: 12 × 8\n##    mix_type      carpool   share  .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 no      0.0217  0.0173  0.0272   0.95 median qi       \n##  2 6 2ft elec 40 yes     0.00946 0.00647 0.0136   0.95 median qi       \n##  3 7 2ft hyb 35  no      0.0434  0.0350  0.0532   0.95 median qi       \n##  4 7 2ft hyb 35  yes     0.0573  0.0425  0.0761   0.95 median qi       \n##  5 7 3ft gas 40  no      0.0655  0.0533  0.0798   0.95 median qi       \n##  6 7 3ft gas 40  yes     0.0976  0.0716  0.130    0.95 median qi       \n##  7 7 2ft hyb 30  no      0.0972  0.0824  0.114    0.95 median qi       \n##  8 7 2ft hyb 30  yes     0.148   0.118   0.183    0.95 median qi       \n##  9 8 2ft gas 30  no      0.265   0.239   0.293    0.95 median qi       \n## 10 8 2ft gas 30  yes     0.423   0.367   0.479    0.95 median qi       \n## 11 6 2ft gas 30  no      0.506   0.472   0.540    0.95 median qi       \n## 12 6 2ft gas 30  yes     0.261   0.223   0.303    0.95 median qi\n\nAnd we can plot them:\n\nshares_df %&gt;% \n  mutate(carpool = case_match(carpool, \"no\" ~ \"Non-carpoolers\", \"yes\" ~ \"Carpoolers\")) %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ggplot(aes(x = share, y = mix_type, slab_alpha = carpool)) +\n  stat_halfeye(normalize = \"groups\", fill = clrs[10]) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(1, 0.4),\n    guide = \"none\"\n  ) +\n  facet_wrap(vars(carpool)) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\n\nThis is so cool! In general, the market share for these six hypothetical products is roughly the same across carpoolers and non-carpoolers, with one obvious exception—among non-carpoolers, the $30,000 8-passenger gas minivan with 2 feet of space has 26% of the market, while among carpoolers it has 42%. Individuals who carpool apparently really care about the number of passengers their vehicle can carry."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nTo find the average marginal component effects (AMCEs), or the causal effect of moving one of these features to another value, holding all other variables constant, we can go through the same process as before. We’ll calculate the predicted probabilities of choosing option 0, 1, 2, and 3 across a full grid of all the combinations of feature levels and carpool status. We’ll then filter those predictions to only look at option 0 and reverse the predicted probabilities. Again, that feels weird, but it’s a neat little trick—if there’s a 33% chance that someone will select a specific combination of features, that would imply a 66% chance of not selecting it and an 11% chance of selecting it when it appears in option 1, option 2, and option 3. Rather than adding the probabilities within those three options together, we can do 100% − 66% to get the same 33% value, only it’s automatically combined.\nEarlier we had 54 combinations—now we have 108 (54 × 2). We’ll set resp.id to one that’s not in the dataset (201) so that these effects all deal with a generic hypothetical respondent (we could also do some fancy “integrating out” work and find population-level averages; see here for more about that).\n\nnewdata_all_combos_carpool &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price, carpool) %&gt;% \n  mutate(resp.id = 201)\nnewdata_all_combos_carpool\n## # A tibble: 108 × 6\n##    seat  cargo eng   price carpool resp.id\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;\n##  1 6     2ft   gas   30    no          201\n##  2 6     2ft   gas   30    yes         201\n##  3 6     2ft   gas   35    no          201\n##  4 6     2ft   gas   35    yes         201\n##  5 6     2ft   gas   40    no          201\n##  6 6     2ft   gas   40    yes         201\n##  7 6     2ft   hyb   30    no          201\n##  8 6     2ft   hyb   30    yes         201\n##  9 6     2ft   hyb   35    no          201\n## 10 6     2ft   hyb   35    yes         201\n## # ℹ 98 more rows\n\nNext we can plug this grid into the model, filter to only keep option 0, and reverse the predictions:\n\nall_preds_brms_carpool &lt;- model_minivans_mega_mlm_brms %&gt;% \n  epred_draws(\n    newdata = newdata_all_combos_carpool,\n    re_formula = NULL, allow_new_levels = TRUE\n  ) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nThis thing has 1.7 million rows in it, so we need to group and summarize to do anything useful with it. We also need to marginalize across all the other covariates when grouping (i.e. if we want the estimates for passenger seat count across carpool status, we need to average out all the other covariates).\nTo test that this worked, here are the posterior marginal means for seat count:\n\npreds_seat_carpool_marginalized &lt;- all_preds_brms_carpool %&gt;% \n  # Marginalize out the other covariates in each draw\n  group_by(seat, carpool, .draw) %&gt;% \n  summarize(avg = mean(.epred))\n\npreds_seat_carpool_marginalized %&gt;% \n  group_by(seat, carpool) %&gt;% \n  median_qi(avg)\n## # A tibble: 6 × 8\n##   seat  carpool   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     no      0.425  0.377  0.483   0.95 median qi       \n## 2 6     yes     0.287  0.245  0.339   0.95 median qi       \n## 3 7     no      0.262  0.205  0.337   0.95 median qi       \n## 4 7     yes     0.335  0.267  0.419   0.95 median qi       \n## 5 8     no      0.305  0.225  0.409   0.95 median qi       \n## 6 8     yes     0.378  0.288  0.488   0.95 median qi\n\nThose credible intervals all look reasonable (i.e. not ranging from 5% to 80% or whatever), but it’s hard to see any trends from just this table. Let’s plot it:\n\npreds_seat_carpool_marginalized %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) + \n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\n\nNeat! The average posterior predicted probability of choosing six seats is substantially higher for carpoolers than for non-carpoolers, while the probability for seven and eight seats is bigger for carpoolers.\nWe’re most interested in the AMCE though, and not the marginal means, so we’ll use compare_levels() to find the carpool-specific differences between the effect of moving from 6 → 7 and 6 → seats:\n\namces_seat_carpool &lt;- preds_seat_carpool_marginalized %&gt;% \n  group_by(carpool) %&gt;% \n  compare_levels(variable = avg, by = seat, comparison = \"control\") \n\namces_seat_carpool %&gt;% \n  median_qi(avg)\n## # A tibble: 4 × 8\n##   carpool seat      avg  .lower   .upper .width .point .interval\n##   &lt;fct&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 no      7 - 6 -0.162  -0.231  -0.0877    0.95 median qi       \n## 2 no      8 - 6 -0.120  -0.219  -0.00669   0.95 median qi       \n## 3 yes     7 - 6  0.0478 -0.0247  0.129     0.95 median qi       \n## 4 yes     8 - 6  0.0912 -0.0114  0.207     0.95 median qi\n\nAmong carpoolers, the causal effect of moving from 6 → 7 passengers, holding all other features constant, is a 5ish percentage point increase in the probability of selecting the vehicle. The effect is bigger (9ish percentage points) when moving from 6 → 8.\nAmong non-carpoolers, the causal effect is reversed. Moving from 6 → 7 passengers causes a 16 percentage point decrease in the probability of selection, while moving from 6 → 8 causes a 12 percentage point decrease, holding all other features constant.\nThese effects are “significant” and have a 90–97% probability of being greater than zero for carpoolers and 98–99% probability of being less than zero for the non-carpoolers.\n\n# Calculate probability of direction\namces_seat_carpool %&gt;% \n  group_by(seat, carpool) %&gt;% \n  summarize(p_gt_0 = sum(avg &gt; 0) / n()) %&gt;% \n  mutate(p_lt_0 = 1 - p_gt_0)\n## # A tibble: 4 × 4\n## # Groups:   seat [2]\n##   seat  carpool   p_gt_0 p_lt_0\n##   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n## 1 7 - 6 no      0.000625 0.999 \n## 2 7 - 6 yes     0.908    0.0916\n## 3 8 - 6 no      0.0208   0.979 \n## 4 8 - 6 yes     0.961    0.0387\n\n\namces_seat_carpool %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\n\nHere are all the AMCEs across carpool status:\n\namces_minivan_carpool &lt;- bind_rows(\n  seat = all_preds_brms_carpool %&gt;% \n    group_by(seat, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms_carpool %&gt;% \n    group_by(cargo, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms_carpool %&gt;% \n    group_by(eng, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms_carpool %&gt;% \n    group_by(price, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_carpool %&gt;% \n  group_by(term, carpool, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 14 × 9\n##    term  carpool contrast       avg  .lower   .upper .width .point .interval\n##    &lt;chr&gt; &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 cargo no      3ft - 2ft   0.0750  0.0358  0.117     0.95 median qi       \n##  2 cargo yes     3ft - 2ft   0.102   0.0555  0.152     0.95 median qi       \n##  3 eng   no      elec - gas -0.288  -0.400  -0.147     0.95 median qi       \n##  4 eng   no      hyb - gas  -0.160  -0.208  -0.108     0.95 median qi       \n##  5 eng   yes     elec - gas -0.273  -0.389  -0.125     0.95 median qi       \n##  6 eng   yes     hyb - gas  -0.166  -0.223  -0.104     0.95 median qi       \n##  7 price no      35 - 30    -0.167  -0.223  -0.107     0.95 median qi       \n##  8 price no      40 - 30    -0.294  -0.354  -0.230     0.95 median qi       \n##  9 price yes     35 - 30    -0.203  -0.270  -0.131     0.95 median qi       \n## 10 price yes     40 - 30    -0.342  -0.412  -0.272     0.95 median qi       \n## 11 seat  no      7 - 6      -0.162  -0.231  -0.0877    0.95 median qi       \n## 12 seat  no      8 - 6      -0.120  -0.219  -0.00669   0.95 median qi       \n## 13 seat  yes     7 - 6       0.0478 -0.0247  0.129     0.95 median qi       \n## 14 seat  yes     8 - 6       0.0912 -0.0114  0.207     0.95 median qi\n\nAnd finally, here’s a polisci-style plot of all these AMCEs, which is so so neat. An individual’s carpooling behavior interacts with seat count (increasing the seat count causes carpoolers to select the minivan more often), and it also interacts a bit with cargo space (increasing the cargo space makes both types of individuals more likely to select the minivan, but moreso for carpoolers) and also with price (increasing the price makes both types of individuals less likely to select the minivan, but moreso for carpoolers). Switching from gas → hybrid and gas → electric has a negative effect on both types of consumers, and there’s no carpooling-based difference.\n\n\nExtract variable labels\nminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make plot\nposterior_amces_minivan_carpool_nested &lt;- amces_minivan_carpool %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_carpool &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_amces_minivan_carpool_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  # Make the missing carpool values be \"yes\" for the reference category\n  mutate(carpool = replace_na(carpool, \"yes\"))\n\nplot_data_minivan_carpool %&gt;% \n  ggplot(aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(slab_alpha = carpool), normalize = \"groups\") + \n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    title = \"AMCEs across respondent carpool status\",\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )"
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "",
    "text": "In a project I’m working on, I need to generate predictions from a logistic regression model. That’s typically a really straightforward task—we can just use predict() to plug a dataset of values into a model, which will spit out predictions either on the (gross, uninterpretable) log odds scale or on the (nice, interpretable) percentage-point scale.11 And for pro-level predictions, use predictions() from {marginaleffects}.\nHowever, in this project I cannot use predict()—I’m working with a big matrix of posterior coefficient draws from a Bayesian model fit with raw Stan code, so there’s no special predict() function that will work. Instead, I need to use matrix multiplication and manually multiply a matrix of new data with a vector of slopes from the model.\nI haven’t had to matrix multiply coefficients with data since my first PhD stats class back in 2012 and I’ve completely forgotten how.\nI created this little guide as a reference for myself, and I figured it’d probably be useful for others, so up on the blog it goes (see the introduction to this post for more about my philosophy of public work)."
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 1: Logistic regression model with an intercept and slopes",
    "text": "Example 1: Logistic regression model with an intercept and slopes\nHere’s a basic regression model where we predict if a penguin is a Gentoo based on its bill length and body mass.\n\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins |&gt; \n  tidyr::drop_na(sex) |&gt; \n  dplyr::mutate(is_gentoo = species == \"Gentoo\")\n\nmodel &lt;- glm(\n  is_gentoo ~ bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\n\nWe can generate predicted values across different three different values of bill length: 40 mm, 44 mm, and 48 mm, holding body mass constant at the average (4207 g):\n\ndata_to_plug_in &lt;- expand.grid(\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in\n##   bill_length_mm body_mass_g\n## 1             40        4207\n## 2             44        4207\n## 3             48        4207\n\nWe can feed this little dataset into the model using predict(), which can generate predictions as log odds (type = \"link\") or probabilities (type = \"response\"):\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\n\nYay, nice and easy.\nHere’s how to do the same thing with manual matrix multiplication:\n\n# Get all the coefficients\n(coefs &lt;- coef(model))\n##    (Intercept) bill_length_mm    body_mass_g \n##     -32.401514       0.088906       0.006358\n\n# Split intercept and slope coefficients into separate objects\n(intercept &lt;- coefs[1])\n## (Intercept) \n##       -32.4\n(slopes &lt;- coefs[-1])\n## bill_length_mm    body_mass_g \n##       0.088906       0.006358\n\n# Convert the data frame of new data into a matrix\n(data_to_plug_in_mat &lt;- as.matrix(data_to_plug_in))\n##      bill_length_mm body_mass_g\n## [1,]             40        4207\n## [2,]             44        4207\n## [3,]             48        4207\n\n# Matrix multiply the new data with the slope coefficients, then add the intercept\n(log_odds &lt;- as.numeric((data_to_plug_in_mat %*% slopes) + intercept))\n## [1] -2.097 -1.741 -1.385\n\n# Convert to probability scale\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002\n\nThe results are the same as predict():\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\nlog_odds\n## [1] -2.097 -1.741 -1.385\n\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002"
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 2: Logistic regression model with a categorical predictor and no intercept",
    "text": "Example 2: Logistic regression model with a categorical predictor and no intercept\nThis gets a little more complex when working with categorical predictors, especially if you’ve omitted the intercept term. For instance, in the data I’m working with, we have a model that looks something like this, with 0 added as a term to suppress the intercept and give separate coefficients for each of the levels of sex:\n\nmodel_categorical &lt;- glm(\n  is_gentoo ~ 0 + sex + bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\ncoef(model_categorical)\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\nWhen using predict(), we don’t have to do anything special with this intercept-free model. We can plug in a dataset with different variations of predictors:\n\ndata_to_plug_in_cat &lt;- expand.grid(\n  sex = c(\"female\", \"male\"),\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in_cat\n##      sex bill_length_mm body_mass_g\n## 1 female             40        4207\n## 2   male             40        4207\n## 3 female             44        4207\n## 4   male             44        4207\n## 5 female             48        4207\n## 6   male             48        4207\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nIf we want to do this manually, we have to create a matrix version of data_to_plug_in_cat that has separate columns for sexfemale and sexmale. We can’t just use as.matrix(data_to_plug_in_cat), since that only has a single column for sex (and because that column contains text, it forces the rest of the matrix to be text, which makes it so we can’t do math with it anymore):\n\nas.matrix(data_to_plug_in_cat)\n##      sex      bill_length_mm body_mass_g\n## [1,] \"female\" \"40\"           \"4207\"     \n## [2,] \"male\"   \"40\"           \"4207\"     \n## [3,] \"female\" \"44\"           \"4207\"     \n## [4,] \"male\"   \"44\"           \"4207\"     \n## [5,] \"female\" \"48\"           \"4207\"     \n## [6,] \"male\"   \"48\"           \"4207\"\n\nInstead, we can use model.matrix() to create a design matrix—also called a dummy-encoded matrix2 or a one-hot encoded matrix—which makes columns of 0s and 1s for each of the levels of sex2 Though we should probably quit using the word “dummy” because of its ableist connotations—see Google’s developer documentation style guide for alternatives.\n\ndata_to_plug_in_cat_mat &lt;- model.matrix(\n  ~ 0 + ., data = data_to_plug_in_cat\n)\ndata_to_plug_in_cat_mat\n##   sexfemale sexmale bill_length_mm body_mass_g\n## 1         1       0             40        4207\n## 2         0       1             40        4207\n## 3         1       0             44        4207\n## 4         0       1             44        4207\n## 5         1       0             48        4207\n## 6         0       1             48        4207\n## attr(,\"assign\")\n## [1] 1 1 2 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$sex\n## [1] \"contr.treatment\"\n\nWe can now do math with this matrix. Since we don’t have an intercept term, we don’t need to create separate objects for the slopes and intercepts and can matrix multiply the new data matrix with the model coefficients:\n\n# Get all the coefficients\n(coefs_cat &lt;- coef(model_categorical))\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\n# Matrix multiply the newdata with the slope coefficients\n(log_odds_cat &lt;- as.numeric(data_to_plug_in_cat_mat %*% coefs_cat))\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\n# Convert to probability scale\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nThe results are the same!\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\nlog_odds_cat\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I’m an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University. I received a PhD in public policy and political science from Duke University’s Sanford School of Public Policy in 2017.\nI study how international NGOs work in authoritarian countries, and I received the 2016–2018 Emerging Scholar Dissertation Award from the International Society for Third Sector Research (ISTR). I do research in public administration and policy, nonprofit management, international relations, and comparative politics.\nI teach courses on program evaluation and causal inference, statistics and data science, data visualization, economics, and science communication. I’m also a certified RStudio instructor and a Posit Academy mentor and absolutely love teaching how to use R and the tidyverse."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given. Research presentations given at conferences are typically stored in their respective repositories on GitHub; one-off workshops and talks can all be found in a catch-all GitHub repository."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2022",
    "text": "2022\n\n\n    \n                  \n            September 12, 2022\n        \n        \n            APIs and web scraping with R\n            Workshop for SEACEN's September 2022 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n            \n                \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            February 22, 2022\n        \n        \n            Putting everything out there as a researcher\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-1",
    "href": "talks/index.html#section-1",
    "title": "Talks",
    "section": "2021",
    "text": "2021\n\n\n    \n                  \n            November 16, 2021\n        \n        \n            Introduction to the tidyverse + data visualization with ggplot2\n            Two sessions for SEACEN's November 2021 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n            \n                \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            August 17, 2021\n        \n        \n            Making documents, websites, and dashboards with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            July 13, 2021\n        \n        \n            Introduction to Geographic Information Systems (GIS) data with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            June 29, 2021\n        \n        \n            Graphic design for public administrators\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            March 4, 2021\n        \n        \n            Universal documents and reproducibility\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Companion site\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            February 5, 2021\n        \n        \n            What does the internet say about you?\n            Workshop for the Political Science Graduate Student Association, Department of Political Science, Georgia State University | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-2",
    "href": "talks/index.html#section-2",
    "title": "Talks",
    "section": "2020",
    "text": "2020\n\n\n    \n                  \n            October 29, 2020\n        \n        \n            The US and the 2020 Election\n            Guest lecture for KIEN 2263 (English Academic and Professional Skills II), Centre for Language & Communication Studies, University of Turku | Turku, Finland (via Zoom)\n            \n        \n    \n    \n                  \n            October 23, 2020\n        \n        \n            Universal documents and reproducibility\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Companion site\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            October 23, 2020\n        \n        \n            What does the internet say about you?\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            September 17, 2020\n        \n        \n            Truth, beauty, and data: Why data visualization matters in research\n            Methods workshop in the Département de science politique, Université de Montréal | Montréal, Canada (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            August 21, 2020\n        \n        \n            Program evaluation and causal inference with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 20, 2020\n        \n        \n            Data visualization with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 16, 2020\n        \n        \n            What does the internet say about you?\n            Workshop for members of the International Society for Third-Sector Research (ISTR) | Online workshop via Zoom\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            July 9, 2020\n        \n        \n            Welcome to the tidyverse: Introduction to R and tidy data analysis\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 7, 2020\n        \n        \n            Truth, Beauty, and Data\n            Workshop on data visualization for employees of the Church of Jesus Christ of Latter-day Saints | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-3",
    "href": "talks/index.html#section-3",
    "title": "Talks",
    "section": "2019",
    "text": "2019\n\n\n    \n                  \n            November 21, 2019\n        \n        \n            Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy\n            2019 Conference for the Association for Research on Nonprofit Organizations and Voluntary Action (ARNOVA) | San Diego, California\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            November 5, 2019\n        \n        \n            NGOs and authoritarianism\n            Guest lecture for International NGOs (PMAP 8201) | Atlanta, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            October 18, 2019\n        \n        \n            Open source resources for teaching data public service-focused data science courses\n            2019 Conference for the Network of Schools of Public Policy, Affairs, and Administration (NASPAA) | Los Angeles, California\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n    \n                  \n            October 10, 2019\n        \n        \n            Data Science, Open Source, and Radical Transparency\n            Brown bag presentation to PhD students in the Andrew Young School of Policy Studies | Atlanta, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 19, 2019\n        \n        \n            Curiosity, Transparency, and Failure\n            Guest talk to epidemiology PhD students at the University of Georgia | Athens, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            May 14, 2019\n        \n        \n            \"Why won't this run again?!\": Making R analysis more reproducible\n            Utah County R Users Group | Provo, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-4",
    "href": "talks/index.html#section-4",
    "title": "Talks",
    "section": "2018",
    "text": "2018\n\n\n    \n                  \n            September 19, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            Utah Valley University Department of Biology Seminar | Orem, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n    \n                  \n            July 11, 2018\n        \n        \n            Introduction to data visualization with R\n            Utah R Users Group | Salt Lake City, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Resources and code examples\n                    \n                \n                \n                    \n                         Video\n                    \n                \n            \n        \n    \n    \n                  \n            June 12, 2018\n        \n        \n            Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries\n            Interest Groups, International Organizations, and Global Problem-solving Capacity workshop | Stockholm, Sweden\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for paper\n                    \n                \n            \n        \n    \n    \n                  \n            April 27, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            West Coast Nonprofit Data Conference | Salt Lake City, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-5",
    "href": "talks/index.html#section-5",
    "title": "Talks",
    "section": "2016",
    "text": "2016\n\n\n    \n                  \n            March 7, 2016\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy, Duke University | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-6",
    "href": "talks/index.html#section-6",
    "title": "Talks",
    "section": "2015",
    "text": "2015\n\n\n    \n                  \n            October 16, 2015\n        \n        \n            Enhancing basic statistical figures: Make pretty pictures like the New York Times\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            October 2, 2015\n        \n        \n            Graphic Design for Non-Designers\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 15, 2015\n        \n        \n            A Very Brief Overview of Topics in Data Visualization\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            April 17, 2015\n        \n        \n            Telling Stories with Data\n            PhD workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-7",
    "href": "talks/index.html#section-7",
    "title": "Talks",
    "section": "2014",
    "text": "2014\n\n\n    \n                  \n            September 11, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 2, 2014\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            January 22, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-8",
    "href": "talks/index.html#section-8",
    "title": "Talks",
    "section": "2013",
    "text": "2013\n\n\n    \n                  \n            September 12, 2013\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 12, 2013\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/herasimenka-2023-a/index.html",
    "href": "research/articles/herasimenka-2023-a/index.html",
    "title": "The Political Economy of Digital Profiteering: Communication Resource Mobilization by Anti-Vaccination Actors",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/herasimenka-2023-a/index.html#citation",
    "href": "research/articles/herasimenka-2023-a/index.html#citation",
    "title": "The Political Economy of Digital Profiteering: Communication Resource Mobilization by Anti-Vaccination Actors",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/kosonen-2022/index.html",
    "href": "research/articles/kosonen-2022/index.html",
    "title": "Verkkoviha väkivaltana – vihapuhetutkijoiden n̨̈\"ülmia",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  },
  {
    "objectID": "research/articles/kosonen-2022/index.html#citation",
    "href": "research/articles/kosonen-2022/index.html#citation",
    "title": "Verkkoviha väkivaltana – vihapuhetutkijoiden n̨̈\"ülmia",
    "section": "",
    "text": "Add to Zotero \n\n{{&lt; include _content.qmd &gt;}}"
  }
]