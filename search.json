[
  {
    "objectID": "uses/index.html",
    "href": "uses/index.html",
    "title": "What I use",
    "section": "",
    "text": "People often ask me what programs I use for my writing and design. In truth, my workflow tends to look like this or this, but here’s a more detailed list of all the interconnected programs I use.\nI try to keep this updated fairly regularly. As of August 21, 2023 this is what I’m using:"
  },
  {
    "objectID": "uses/index.html#writing",
    "href": "uses/index.html#writing",
    "title": "What I use",
    "section": "Writing",
    "text": "Writing\n\nI permanently ditched Word as a writing environment in 2008 after starting grad school. I do all my writing in pandoc-flavored Markdown (including e-mails and paper-and-pencil writing)—it’s incredibly intuitive, imminently readable, flexible, future proof, and lets me ignore formatting and focus on content.\nI live in Ulysses. At first I chafed at the fact that it stores everything in its own internal folder structure, since I store most of my writing in git repositories, but exporting a compiled Markdown file from a bunch of Ulysses sheets is trivial (and still easily trackable in version control).\nUlysses has decent HTML previewing powers, but when I need more editorial tools, I use Marked.\nI use Typora to edit standalone Markdown files, since Ulysses uses its own syntax when using fancy things like footnotes. Typora is my favorite standalone Markdown editor I’ve found so far because it inherently supports pandoc-flavored Markdown.\nThe key to my writing workflow is the magical pandoc, which converts Markdown files into basically anything else. I use my own variation of Kieran Healy’s Plain Text Social Science workflow to convert Markdown to HTML, PDF (through LaTeX), and Word (through LibreOffice).\nI store all my bibliographic references, books, and articles in Zotero (see here for why).\nI read and annotate all my PDFs with Skim (and iAnnotate on iOS), since both export annotations as clean plain text.\nI store all my notes in Obsidian. Before switching to Obsidian I used Bear, which was great but didn’t support fancier things like math or syntax highlighting. Before that, I used Evernote, but I abandoned it in September 2018 after 9 years of heavy use, given their ongoing privacy controversies and mass layoffs."
  },
  {
    "objectID": "uses/index.html#development",
    "href": "uses/index.html#development",
    "title": "What I use",
    "section": "Development",
    "text": "Development\n\nScience and research\n\nI post almost everything I write or develop on GitHub.\nI use R and RStudio for most of my statistical computing, and I’m a dedicated devotee of the tidyverse. In the interest of full reproducibility and transparency, I make R Markdown websites for each of my projects. I don’t typically make full-blown literate documents (like, I have yet to write a full article or book in R Markdown)—instead, I generate figures and tables with R and reference them in my writing. See a list of these websites.\nI also use Python (3!) pretty regularly, especially for natural language processing (with nltk) and web scraping (with Requests + BeautifulSoup). Every few months I play with pandas and numpy and Jupyter, but I’m far more comfortable with R for scientific computing.\nI use RStudio for editing R files, but I use Visual Studio Code for everything else.\nI adapted the idea for research haikus from Kirby Nielsen.\nI use The Rogue Scholar to create stable DOIs for each of my blog posts.\n\n\n\nWeb\n\nI run my main web server on a DigitalOcean droplet, and I spin up temporary droplets all the time to offload scraping scripts, complicated R models, and to create on-the-fly VPNs.\nI normally access my remote files through SSH in a terminal, but for more complicated things, I’ve found that Mountain Duck is indispensable.\nMy website uses Pelican. My teaching websites all use blogdown + Hugo.\nI use Let’s Encrypt for SSL.\n\n\n\nMiscellaneous\n\nI use a system-wide hotkey (ctrl + `) to open iTerm2 from anywhere.\nI use Homebrew to install Unix-y programs.\nI’m partial to both Fira Code and Consolas for my monospaced fonts."
  },
  {
    "objectID": "uses/index.html#desktop-apps",
    "href": "uses/index.html#desktop-apps",
    "title": "What I use",
    "section": "Desktop apps",
    "text": "Desktop apps\n\nGraphic design\n\nThough I regularly use LaTeX (through pandoc), I adore InDesign CC and use it to make fancier academic and policy documents. I also used it for all the typesetting I did for BYU’s Neal A. Maxwell Institute.\nI use Illustrator CC all the time to enhance graphics I make in R and to make non-data-driven figures and diagrams.\nI use Lightroom and Photoshop too, but less often nowadays.\nDespite my dislike for Word and Excel, I use PowerPoint for all my presentations. It’s not my favorite, but in the apocryphal words of Churchill, “PowerPoint is the worst form of slide editor, except for all the others.”\n\n\n\nProductivity\n\nMy secret for avoiding the siren call of the internet is Freedom. I have two blocklists: (1) antisocial, which blocks Facebook and Twitter, and (2) nuclear, which blocks everything. I have the antisocial blocklist enabled on my laptop and phone from 8:00 AM–6:00 PM and 8:30 PM–11:30 PM. Since I accidentally discovered that it’s relatively easy to circumvent the blocking on the Mac, I also use Focus with the same schedule.\nI use Vitamin-R as a souped-up Pomodoro timer.\nI was an early convert to Todo.txt and used it for years until my tasks and projects got too unwieldy. I switched to Taskpaper for a while, used 2Do for a couple years, and now I’m a convert to OmniFocus.\nFantastical 2’s natural language input is a glorious thing.\nI use Timery as an interface to Toggl to track my time during the day\nI keep a log of what I work on (and occasionally do more traditional diary-like entries) with Day One on both iOS and macOS.\nI use TextExpander to replace and expand a ton of snippets, and I use Keyboard Maestro to run dozens of little scripts that help control my computer with the keyboard.\nI use Übersicht to show weather, iTunes track information, and my todo lists on my desktop.\nI use Dropbox religiously and use Backblaze to back up all the computers in our house to the cloud.\nWith all these little helper apps, I use Bartender to keep my menubar clean."
  },
  {
    "objectID": "uses/index.html#hardware",
    "href": "uses/index.html#hardware",
    "title": "What I use",
    "section": "Hardware",
    "text": "Hardware\n\nI use a 2021 14″ M1 Max MacBook Pro, a 2018 15″ MacBook Pro, a 5th generation iPad, and an iPhone 8."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given. Research presentations given at conferences are typically stored in their respective repositories on GitHub; one-off workshops and talks can all be found in a catch-all GitHub repository."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2022",
    "text": "2022\n\n\n    \n                  \n            September 12, 2022\n        \n        \n            APIs and web scraping with R\n            Workshop for SEACEN's September 2022 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n            \n                \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            February 22, 2022\n        \n        \n            Putting everything out there as a researcher\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-1",
    "href": "talks/index.html#section-1",
    "title": "Talks",
    "section": "2021",
    "text": "2021\n\n\n    \n                  \n            November 16, 2021\n        \n        \n            Introduction to the tidyverse + data visualization with ggplot2\n            Two sessions for SEACEN's November 2021 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n            \n                \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            August 17, 2021\n        \n        \n            Making documents, websites, and dashboards with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            July 13, 2021\n        \n        \n            Introduction to Geographic Information Systems (GIS) data with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            June 29, 2021\n        \n        \n            Graphic design for public administrators\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Companion site and slides\n                    \n                \n            \n        \n    \n    \n                  \n            March 4, 2021\n        \n        \n            Universal documents and reproducibility\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Companion site\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            February 5, 2021\n        \n        \n            What does the internet say about you?\n            Workshop for the Political Science Graduate Student Association, Department of Political Science, Georgia State University | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-2",
    "href": "talks/index.html#section-2",
    "title": "Talks",
    "section": "2020",
    "text": "2020\n\n\n    \n                  \n            October 29, 2020\n        \n        \n            The US and the 2020 Election\n            Guest lecture for KIEN 2263 (English Academic and Professional Skills II), Centre for Language & Communication Studies, University of Turku | Turku, Finland (via Zoom)\n            \n        \n    \n    \n                  \n            October 23, 2020\n        \n        \n            Universal documents and reproducibility\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Companion site\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            October 23, 2020\n        \n        \n            What does the internet say about you?\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            September 17, 2020\n        \n        \n            Truth, beauty, and data: Why data visualization matters in research\n            Methods workshop in the Département de science politique, Université de Montréal | Montréal, Canada (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            August 21, 2020\n        \n        \n            Program evaluation and causal inference with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 20, 2020\n        \n        \n            Data visualization with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 16, 2020\n        \n        \n            What does the internet say about you?\n            Workshop for members of the International Society for Third-Sector Research (ISTR) | Online workshop via Zoom\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n            \n        \n    \n    \n                  \n            July 9, 2020\n        \n        \n            Welcome to the tidyverse: Introduction to R and tidy data analysis\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n            \n                \n                    \n                         Slides and materials\n                    \n                \n            \n        \n    \n    \n                  \n            July 7, 2020\n        \n        \n            Truth, Beauty, and Data\n            Workshop on data visualization for employees of the Church of Jesus Christ of Latter-day Saints | Salt Lake City, Utah (via Zoom)\n            \n                \n                    \n                         Slides\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-3",
    "href": "talks/index.html#section-3",
    "title": "Talks",
    "section": "2019",
    "text": "2019\n\n\n    \n                  \n            November 21, 2019\n        \n        \n            Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy\n            2019 Conference for the Association for Research on Nonprofit Organizations and Voluntary Action (ARNOVA) | San Diego, California\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            November 5, 2019\n        \n        \n            NGOs and authoritarianism\n            Guest lecture for International NGOs (PMAP 8201) | Atlanta, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            October 18, 2019\n        \n        \n            Open source resources for teaching data public service-focused data science courses\n            2019 Conference for the Network of Schools of Public Policy, Affairs, and Administration (NASPAA) | Los Angeles, California\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n    \n                  \n            October 10, 2019\n        \n        \n            Data Science, Open Source, and Radical Transparency\n            Brown bag presentation to PhD students in the Andrew Young School of Policy Studies | Atlanta, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 19, 2019\n        \n        \n            Curiosity, Transparency, and Failure\n            Guest talk to epidemiology PhD students at the University of Georgia | Athens, Georgia\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            May 14, 2019\n        \n        \n            \"Why won't this run again?!\": Making R analysis more reproducible\n            Utah County R Users Group | Provo, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-4",
    "href": "talks/index.html#section-4",
    "title": "Talks",
    "section": "2018",
    "text": "2018\n\n\n    \n                  \n            September 19, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            Utah Valley University Department of Biology Seminar | Orem, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n    \n                  \n            July 11, 2018\n        \n        \n            Introduction to data visualization with R\n            Utah R Users Group | Salt Lake City, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Resources and code examples\n                    \n                \n                \n                    \n                         Video\n                    \n                \n            \n        \n    \n    \n                  \n            June 12, 2018\n        \n        \n            Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries\n            Interest Groups, International Organizations, and Global Problem-solving Capacity workshop | Stockholm, Sweden\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for paper\n                    \n                \n            \n        \n    \n    \n                  \n            April 27, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            West Coast Nonprofit Data Conference | Salt Lake City, Utah\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                    \n                         Handout page\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-5",
    "href": "talks/index.html#section-5",
    "title": "Talks",
    "section": "2016",
    "text": "2016\n\n\n    \n                  \n            March 7, 2016\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy, Duke University | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-6",
    "href": "talks/index.html#section-6",
    "title": "Talks",
    "section": "2015",
    "text": "2015\n\n\n    \n                  \n            October 16, 2015\n        \n        \n            Enhancing basic statistical figures: Make pretty pictures like the New York Times\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            October 2, 2015\n        \n        \n            Graphic Design for Non-Designers\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 15, 2015\n        \n        \n            A Very Brief Overview of Topics in Data Visualization\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            April 17, 2015\n        \n        \n            Telling Stories with Data\n            PhD workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-7",
    "href": "talks/index.html#section-7",
    "title": "Talks",
    "section": "2014",
    "text": "2014\n\n\n    \n                  \n            September 11, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 2, 2014\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            January 22, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-8",
    "href": "talks/index.html#section-8",
    "title": "Talks",
    "section": "2013",
    "text": "2013\n\n\n    \n                  \n            September 12, 2013\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n    \n                  \n            September 12, 2013\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n            \n                \n                    \n                         Slides\n                    \n                \n                \n                    \n                         GitHub repository\n                    \n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html#important-links",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html#important-links",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html#abstract",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html#abstract",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "Abstract",
    "text": "Abstract\nChina’s 2017 Overseas NGO (ONGO) Law is part of a larger global trend of legal restrictions on international NGOs (INGOs). However, the effects of this global crackdown on INGOs have been difficult to measure since there is often divergence between the formal de jure regulations and the de facto implementation of these laws. The enforcement of these laws also depends on the services provided by these organizations. In particular, authoritarian regimes display conflicting attitudes towards claim-making (e.g. advocacy) INGOs versus service-providing (e.g. humanitarian assistance) INGOs. In this paper, we measure the effect of China’s ONGO law on INGO operations since 2017. We test how INGO operational flexibility under the ONGO law is influenced by organizations’ issue areas. We argue that service provision INGOs have greater flexibility and are allowed to work in a larger number of provinces than their claim-making counterparts. China provides an excellent setting for testing our claim—the 2017 ONGO law regulates both claim-making and service-providing NGOs, and ambiguity in the law gives ample room for arbitrary discretion by authorities. We test our argument using multilevel Bayesian models with administrative data from 635 registered representative offices of INGOs registered in China between 2017–2021. Our results speak to the broader literature on closing civic space and provide an empirical illustration of the practical effect of NGO restrictions on global civil society."
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#important-links",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#important-links",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#abstract",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#abstract",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "Abstract",
    "text": "Abstract\nThe past two decades have been characterized by considerable progress in developing approaches to causal inference in situations where true experimental manipulation is either impractical or impossible. With few exceptions, however, commonly employed techniques in political science have developed largely within the frequentist framework (i.e., Blackwell and Glynn 2018; Imai and Kim 2019; Torres 2020). In this article, we argue that common approaches rest fundamentally upon assumptions that are difficult to defend in many areas of political research and highlight the benefits of quantifying uncertainty in the estimation of causal effects (Gill 1999; Gill and Heuberger 2020; Schrodt 2014; Western and Jackman 1994). Extending the approach to causal inference for cross-sectional time series and panel data under selection on observables introduced by Blackwell and Glynn (2018), we develop a two-step Bayesian approach to the estimation of marginal structural models. We demonstrate our proposed procedure in the context of parametric survival analysis and linear mixed effects models via a simulation study and two empirical examples. Finally, we provide flexible open-source software implementing the proposed method."
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#bibtex-citation",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#bibtex-citation",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{HeissNafa:2022,\n    Author = {Andrew Heiss and A. Jordan Nafa},\n    Note = {Working paper},\n    Title = {Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science},\n    Year = {2022}}"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html",
    "href": "research/working-papers/heiss-amicable-contempt/index.html",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html#important-links",
    "href": "research/working-papers/heiss-amicable-contempt/index.html#important-links",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html#abstract",
    "href": "research/working-papers/heiss-amicable-contempt/index.html#abstract",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "Abstract",
    "text": "Abstract\nOver the past decade, international NGOs (INGOs) have become increasingly active in authoritarian regimes as they respond to emergencies, assist with development, or advocate for human rights. Though these services and advocacy can challenge the legitimacy and power of the regime, many autocratic states permit INGO activities and INGOs continue to work in these countries despite the sometimes heavy restrictions on their activities. In my dissertation, I theorize that this relationship between INGOs and autocrats creates a state of amicable contempt, where each party is aware that the other threatens—yet sustains—their existence. Autocrats and INGOs engage in a dance of cost-benefit calculus, each trying to advance their own agenda without upsetting their counterpart. Regimes work to set the optimal level of INGO regulations, maximizing the practical and reputational benefits that INGOs provide and minimizing the potential destabilizing costs of INGO activities. Meanwhile, INGOs work to find the optimal mix of programming within a country that allows them to pursue their principled objectives within the boundaries the regime has set—affecting as much change and providing as many services as possible without risking expulsion from the country. I use evidence from a global survey of international NGOs to define each INGO-related element of the theory of amicable contempt. I find that INGOs are primarily motivated by their core vision and values, but that they have to balance the pursuit of their missions with instrumental concerns such as fundraising, time, staffing, and collaboration. These concerns both limit and enable INGO activities—without substantial instrumental resources and programmatic flexibility, organizations are unable to carry out their mission, while too much emphasis on resource concerns distracts organizations from their core programming and reduces their effectiveness."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#important-links",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#important-links",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#abstract",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#abstract",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "Abstract",
    "text": "Abstract\nIn an effort to combat the COVID-19 pandemic, most governments have imposed restrictions on movement, association, and other civic freedoms in the interest of public health. In many cases, such as New Zealand, these emergency measures have been temporary and respect for human rights has returned to normal. In many other instances, however, governments have used these restrictions to suppress opposition and more permanently restrict civic space. Systematically measuring the consequences of COVID restrictions, however, is a difficult task. We will examine two possible quantitative measures of the relationship between of COVID restrictions and civil society space. First, we will use the Variety of Democracy project’s newly released Pandemic Violations of Democratic Standards Index to explore if and how civil society restrictions predict pandemic backsliding. Second, while many countries sign international human rights treaties that ostensibly bind states to respect rights, several treaties allow for emergency derogations from these obligations. We will tabulate formal human rights treaty derogations due to the pandemic and explore whether these emergency measures led to lasting declines in associational and human rights. We hope that our exploration of these two measures will provide rich data-based descriptions of the relationship between COVID restrictions and civic space that will allow for more causal work in the future."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#important-figure",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#important-figure",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding\n\n\n\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#bibtex-citation",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#bibtex-citation",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Note = {Working paper},\n    Title = {Derogations and Democratic Backsliding: Exploring the Pandemic's Effects on Civic Spaces},\n    Year = {2021}}"
  },
  {
    "objectID": "research/working-papers/chaudhry-comstock-heiss-pandemic-pass/index.html",
    "href": "research/working-papers/chaudhry-comstock-heiss-pandemic-pass/index.html",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Statistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-comstock-heiss-pandemic-pass/index.html#important-links",
    "href": "research/working-papers/chaudhry-comstock-heiss-pandemic-pass/index.html#important-links",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Statistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research spans public policy, nonprofit management, political science, and international relations. I study human rights and international nonprofit management and I focus on authoritarian regulation of civil society and international NGO responses to administrative crackdown. I also research causal inference methods, particularly using panel data."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n\n        \n            \n                Steven L. Peck and Andrew Heiss, “Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?” Oikos 130, no. 9 (September 2021): 1425–1439, doi: 10.1111/oik.07621\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “NGO Repression as a Predictor of Worsening Human Rights Abuses,” Journal of Human Rights 21, no. 2 (2022): 123–140, doi: 10.1080/14754835.2022.2030205\n            \n            \n                    \n                            Human rights\n                        \n                    \n                            INGOs\n                        \n                    \n                            Civil society\n                        \n                    \n                            Bayes\n                        \n                    \n                            NGO regulations\n                        \n            \n            \n                Look at state actions\n                 / —not formal NGO laws—\n                 / to predict abuse.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code and data\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry, Marc Dotson, and Andrew Heiss, “Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy,” Global Policy 12, no. S5 (July 2021): 45–58, doi: 10.1111/1758-5899.12984\n            \n            \n                    \n                            Philanthropy\n                        \n                    \n                            Experiment\n                        \n                    \n                            Conjoint analysis\n                        \n                    \n                            Simulation\n                        \n                    \n                            Bayes\n                        \n                    \n                            INGOs\n                        \n            \n            \n                When seeing crackdown,\n                 / people with low social trust\n                 / are fairweather friends.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Data\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Steven L. Peck and Andrew Heiss, “Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?” Oikos 130, no. 9 (September 2021): 1425–1439, doi: 10.1111/oik.07621\n            \n            \n                    \n                            Data visualization\n                        \n                    \n                            Ecology\n                        \n                    \n                            Simulation\n                        \n            \n            \n                Fitness, turnover,\n                 / stability, evenness—\n                 / all due to constraints.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences,” Nonprofit and Voluntary Sector Quarterly 50, no. 3 (June 2021): 481–505, doi: 10.1177/0899764020971045\n            \n            \n                    \n                            Philanthropy\n                        \n                    \n                            Experiment\n                        \n                    \n                            Bayes\n                        \n                    \n                            INGOs\n                        \n            \n            \n                In trouble abroad?\n                 / Crackdown may be heuristic—\n                 / those who give give more.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Data\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss, “Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries,” Interest Groups and Advocacy 8, no. 3 (September 2019): 356–75, doi: 10.1057/s41309-019-00061-0\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Institutions\n                        \n                    \n                            Mixed methods\n                        \n                    \n                            Survey\n                        \n            \n            \n                Flexibility—\n                 / what lets NGOs reshape\n                 / host environment.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Data\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and Judith G. Kelley, “Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments,” Journal of Politics 79, no. 2 (April 2017): 732–41, doi: 10.1086/691218.\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Civil society\n                        \n            \n            \n                Donors and targets:\n                 / institutional constraints\n                 / limit NGOs.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and Judith G. Kelley, “From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts,” Journal of Human Trafficking 3, no. 3 (2017): 1500–1528, doi: 10.1080/23322705.2016.1199241.\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Survey\n                        \n                    \n                            Human trafficking\n                        \n            \n            \n                Big sector survey—\n                 / NGOs like U.S. work\n                 / fighting trafficking.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Data\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and Tana Johnson, “Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior,” International Studies Review 18, no. 3 (September 2016): 528–41, doi: 10.1093/isr/viv014.\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Institutions\n                        \n            \n            \n                Study NGOs?\n                 / View behavior; policy\n                 / with a new framework.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Eva Witesman and Andrew Heiss, “Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives,” Voluntas: International Journal of Voluntary and Nonprofit Organizations 28, no. 4 (August 2017): 1500–1528, doi: 10.1007/s11266-016-9684-5.\n            \n            \n                    \n                            Nonprofits\n                        \n                    \n                            Simulation\n                        \n                    \n                            Institutions\n                        \n            \n            \n                When money is scarce;\n                 / when objectives are not shared—\n                 / don’t collaborate.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Add to Zotero\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss, “The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution,” Journal of Third World Studies 28, no. 1 (Spring 2012): 155–171, no doi.\n            \n            \n                    \n                            Management\n                        \n                    \n                            Egypt\n                        \n            \n            \n                Management theory\n                 / meets political science:\n                 / Mubarak messed up.\n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Preprint\n                        \n                    \n                    \n                        \n                             Final version\n                        \n                    \n                    \n                        \n                             Final version (PDF)\n                        \n                    \n                \n            \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers\n\n\n\n        \n            \n                Andrew Heiss, “Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space”\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Institutions\n                        \n                    \n                            Mixed methods\n                        \n                    \n                            Survey\n                        \n                    \n                            Theory\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry, Audrey L. Comstock, and Andrew Heiss, “Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19”\n            \n            \n                    \n                            Human rights\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            COVID-19\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Analysis notebook\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry, Marc Dotson, and Andrew Heiss, Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy”\n            \n            \n                    \n                            Philanthropy\n                        \n                    \n                            Experiment\n                        \n                    \n                            Conjoint analysis\n                        \n                    \n                            Bayes\n                        \n                    \n                            INGOs\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs”\n            \n            \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Foreign aid\n                        \n                    \n                            Causal inference\n                        \n                    \n                            Panel data\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             PDF\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                    \n                        \n                             Analysis notebook\n                        \n                    \n                \n            \n        \n        \n            \n                Vincent Arel-Bundock, Noah Greifer, and Andrew Heiss, “The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R”\n            \n            \n                    \n                            Human rights\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            COVID-19\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss, “‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulation”\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Institutions\n                        \n                    \n                            Mixed methods\n                        \n                    \n                            Survey\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and Meng Ye, “The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society”\n            \n            \n                    \n                            Civil society\n                        \n                    \n                            China\n                        \n                    \n                            INGOs\n                        \n                    \n                            Panel data\n                        \n                    \n                            Bayes\n                        \n                    \n                            NGO regulations\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss, “‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs”\n            \n            \n                    \n                            INGOs\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            Institutions\n                        \n                    \n                            Mixed methods\n                        \n                    \n                            Survey\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and A. Jordan Nafa, “Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science”\n            \n            \n                    \n                            Causal inference\n                        \n                    \n                            Bayes\n                        \n                    \n                            Panel data\n                        \n                    \n                            Methods\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Andrew Heiss and Meng Ye, “Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies”\n            \n            \n                    \n                            Causal inference\n                        \n                    \n                            Nonprofits\n                        \n                    \n                            Methods\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces”\n            \n            \n                    \n                            Human rights\n                        \n                    \n                            Civil society\n                        \n                    \n                            NGO regulations\n                        \n                    \n                            COVID-19\n                        \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                             PDF\n                        \n                    \n                    \n                        \n                             Code\n                        \n                    \n                \n            \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#book-chapters",
    "href": "research/index.html#book-chapters",
    "title": "Research",
    "section": "Book chapters",
    "text": "Book chapters\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#reviews",
    "href": "research/index.html#reviews",
    "title": "Research",
    "section": "Reviews",
    "text": "Reviews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#dormant-working-papers",
    "href": "research/index.html#dormant-working-papers",
    "title": "Research",
    "section": "Dormant working papers",
    "text": "Dormant working papers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#selected-seminar-papers",
    "href": "research/index.html#selected-seminar-papers",
    "title": "Research",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#translations",
    "href": "research/index.html#translations",
    "title": "Research",
    "section": "Translations",
    "text": "Translations\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html",
    "href": "research/articles/heiss-kelley-2017a/index.html",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#abstract",
    "href": "research/articles/heiss-kelley-2017a/index.html#abstract",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#important-figures",
    "href": "research/articles/heiss-kelley-2017a/index.html#important-figures",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\n\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions\n\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#citation",
    "href": "research/articles/heiss-kelley-2017a/index.html#citation",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n  Author = {Andrew Heiss and Judith G. Kelley},\n  Doi = {10.1080/23322705.2016.1199241},\n  Journal = {Journal of Human Trafficking},\n  Number = {3},\n  Pages = {231--254},\n  Title = {From the Trenches: A Global Survey of Anti-{TIP} {NGOs} and their Views of {US} Efforts},\n  Volume = {3},\n  Year = {2017}}"
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html",
    "href": "research/articles/heiss-johnson-2016/index.html",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#abstract",
    "href": "research/articles/heiss-johnson-2016/index.html#abstract",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#important-figures",
    "href": "research/articles/heiss-johnson-2016/index.html#important-figures",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Important figures",
    "text": "Important figures\nFigure 1 from the paper, showing a unified framework for understanding the types of influences on INGO behavior and policy outcomes. The triangular shape conveys the specificity of the layers: from narrow “micro” phenomena at the internal layer to much wider “macro” phenomena at the institutional layer.\n\n\n\nFigure 1: A Unified Framework for Analyzing INGO Behavior and Outputs"
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#books-reviewed",
    "href": "research/articles/heiss-johnson-2016/index.html#books-reviewed",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah S. Stroup, Borders among Activists: International NGOs in the United States, Britain, and France (Ithaca, New York: Cornell University Press, 2012), doi: 10.7591/9780801464256.\nJonas Tallberg, Thomas Sommerer, Theresa Squatrito, and Christer Jönsson, The Opening Up of International Organizations: Transnational Access in Global Governance (Cambridge: Cambridge University Press, 2013), doi: 10.1017/cbo9781107325135.\nWendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca: Cornell University Press, 2012), doi: 10.7591/9780801466069."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#citation",
    "href": "research/articles/heiss-johnson-2016/index.html#citation",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissJohnson:2016,\n    Author = {Andrew Heiss and Tana Johnson},\n    Doi = {10.1093/isr/viv014},\n    Journal = {International Studies Review},\n    Month = {9},\n    Number = {3},\n    Pages = {528--41},\n    Title = {Internal, Interactive, and Institutional Factors: Towards a Unified Theory of {INGO} Behavior},\n    Volume = {18},\n    Year = {2016}}"
  },
  {
    "objectID": "research/articles/heiss-2012/index.html",
    "href": "research/articles/heiss-2012/index.html",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#abstract",
    "href": "research/articles/heiss-2012/index.html#abstract",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#figure",
    "href": "research/articles/heiss-2012/index.html#figure",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Figure",
    "text": "Figure\nFigure 1: NDP Leadership Organizational Chart, 2010 (based on original chart by The Arabist)\n\n\n\nFigure 1: NDP Leadership Organizational Chart, 2010"
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#citation",
    "href": "research/articles/heiss-2012/index.html#citation",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Citation",
    "text": "Citation\n@article{Heiss:2012,\n    Author = {Andrew Heiss},\n    Issue = {Spring},\n    Journal = {Journal of Third World Studies},\n    Number = {1},\n    Pages = {155-171},\n    Title = {The Failed Management of a Dying Regime: {Hosni Mubarak}, {Egypt's National Democratic Party}, and the {January} 25 Revolution},\n    Volume = {28},\n    Year = {2012}}"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Media coverage",
    "text": "Media coverage\n\n“Donors grow more generous when they support nonprofits facing hostile environments abroad,” The Conversation, December 7, 2020"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Abstract",
    "text": "Abstract\nState restrictions on non-governmental organizations (NGOs) have become increasingly pervasive across the globe. While this crackdown has been shown to have a negative impact on public funding flows, we know little about how it impacts private philanthropy. How does information about crackdown abroad, as well as organizational attributes of nonprofits affect individual donors’ willingness to donate internationally? Using a survey experiment, we find that learning about repressive NGO environments increases generosity in that already-likely donors are willing to donate substantially more to legally besieged nonprofits. This generosity persists when mediated by two organizational-level heuristics: NGO issue areas and main funding sources. We discuss the implications of our results on how nonprofits can use different framing appeals to increase fundraising at a time when traditional public donor funding to such organizations is decreasing."
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important figures",
    "text": "Important figures\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames\n\n\n\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1177/0899764020971045},\n    Journal = {Nonprofit and Voluntary Sector Quarterly},\n    Month = {6},\n    Number = {3},\n    Pages = {481--505},\n    Title = {Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences},\n    Volume = {50},\n    Year = {2021}}"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "What I’m doing now",
    "section": "",
    "text": "As of August 21, 2023, I’m spending all my time on these things:\n\nStaying at home pretty much 24/7 because of the COVID-19 pandemic\nRaising 6 kids (16, 13.5, 11, 8, 5.5, and 1.5) and trying to stay sane (family blog)\nLiving in Atlanta and working as an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University\nTeaching data visualization, program evaluation, comparative public administration, nonprofit management, and microeconomics at the Andrew Young School of Policy Studies at Georgia State University\nWorking as a part time data science mentor for Posit Academy\nConverting my dissertation into multiple articles and sending them out to journals + continuing my research on authoritarianism and international NGOs\nWorking on several articles on NGO restrictions with Suparna Chaudhry and Marc Dotson\nReading some sort of religiously themed text every day (books)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "",
    "text": "In a project I’m working on, I need to generate predictions from a logistic regression model. That’s typically a really straightforward task—we can just use predict() to plug a dataset of values into a model, which will spit out predictions either on the (gross, uninterpretable) log odds scale or on the (nice, interpretable) percentage-point scale.11 And for pro-level predictions, use predictions() from {marginaleffects}.\nHowever, in this project I cannot use predict()—I’m working with a big matrix of posterior coefficient draws from a Bayesian model fit with raw Stan code, so there’s no special predict() function that will work. Instead, I need to use matrix multiplication and manually multiply a matrix of new data with a vector of slopes from the model.\nI haven’t had to matrix multiply coefficients with data since my first PhD stats class back in 2012 and I’ve completely forgotten how.\nI created this little guide as a reference for myself, and I figured it’d probably be useful for others, so up on the blog it goes (see the introduction to this post for more about my philosophy of public work)."
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 1: Logistic regression model with an intercept and slopes",
    "text": "Example 1: Logistic regression model with an intercept and slopes\nHere’s a basic regression model where we predict if a penguin is a Gentoo based on its bill length and body mass.\n\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins |&gt; \n  tidyr::drop_na(sex) |&gt; \n  dplyr::mutate(is_gentoo = species == \"Gentoo\")\n\nmodel &lt;- glm(\n  is_gentoo ~ bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\n\nWe can generate predicted values across different three different values of bill length: 40 mm, 44 mm, and 48 mm, holding body mass constant at the average (4207 g):\n\ndata_to_plug_in &lt;- expand.grid(\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in\n##   bill_length_mm body_mass_g\n## 1             40        4207\n## 2             44        4207\n## 3             48        4207\n\nWe can feed this little dataset into the model using predict(), which can generate predictions as log odds (type = \"link\") or probabilities (type = \"response\"):\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\n\nYay, nice and easy.\nHere’s how to do the same thing with manual matrix multiplication:\n\n# Get all the coefficients\n(coefs &lt;- coef(model))\n##    (Intercept) bill_length_mm    body_mass_g \n##     -32.401514       0.088906       0.006358\n\n# Split intercept and slope coefficients into separate objects\n(intercept &lt;- coefs[1])\n## (Intercept) \n##       -32.4\n(slopes &lt;- coefs[-1])\n## bill_length_mm    body_mass_g \n##       0.088906       0.006358\n\n# Convert the data frame of new data into a matrix\n(data_to_plug_in_mat &lt;- as.matrix(data_to_plug_in))\n##      bill_length_mm body_mass_g\n## [1,]             40        4207\n## [2,]             44        4207\n## [3,]             48        4207\n\n# Matrix multiply the new data with the slope coefficients, then add the intercept\n(log_odds &lt;- as.numeric((data_to_plug_in_mat %*% slopes) + intercept))\n## [1] -2.097 -1.741 -1.385\n\n# Convert to probability scale\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002\n\nThe results are the same as predict():\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\nlog_odds\n## [1] -2.097 -1.741 -1.385\n\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002"
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 2: Logistic regression model with a categorical predictor and no intercept",
    "text": "Example 2: Logistic regression model with a categorical predictor and no intercept\nThis gets a little more complex when working with categorical predictors, especially if you’ve omitted the intercept term. For instance, in the data I’m working with, we have a model that looks something like this, with 0 added as a term to suppress the intercept and give separate coefficients for each of the levels of sex:\n\nmodel_categorical &lt;- glm(\n  is_gentoo ~ 0 + sex + bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\ncoef(model_categorical)\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\nWhen using predict(), we don’t have to do anything special with this intercept-free model. We can plug in a dataset with different variations of predictors:\n\ndata_to_plug_in_cat &lt;- expand.grid(\n  sex = c(\"female\", \"male\"),\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in_cat\n##      sex bill_length_mm body_mass_g\n## 1 female             40        4207\n## 2   male             40        4207\n## 3 female             44        4207\n## 4   male             44        4207\n## 5 female             48        4207\n## 6   male             48        4207\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nIf we want to do this manually, we have to create a matrix version of data_to_plug_in_cat that has separate columns for sexfemale and sexmale. We can’t just use as.matrix(data_to_plug_in_cat), since that only has a single column for sex (and because that column contains text, it forces the rest of the matrix to be text, which makes it so we can’t do math with it anymore):\n\nas.matrix(data_to_plug_in_cat)\n##      sex      bill_length_mm body_mass_g\n## [1,] \"female\" \"40\"           \"4207\"     \n## [2,] \"male\"   \"40\"           \"4207\"     \n## [3,] \"female\" \"44\"           \"4207\"     \n## [4,] \"male\"   \"44\"           \"4207\"     \n## [5,] \"female\" \"48\"           \"4207\"     \n## [6,] \"male\"   \"48\"           \"4207\"\n\nInstead, we can use model.matrix() to create a design matrix—also called a dummy-encoded matrix2 or a one-hot encoded matrix—which makes columns of 0s and 1s for each of the levels of sex2 Though we should probably quit using the word “dummy” because of its ableist connotations—see Google’s developer documentation style guide for alternatives.\n\ndata_to_plug_in_cat_mat &lt;- model.matrix(\n  ~ 0 + ., data = data_to_plug_in_cat\n)\ndata_to_plug_in_cat_mat\n##   sexfemale sexmale bill_length_mm body_mass_g\n## 1         1       0             40        4207\n## 2         0       1             40        4207\n## 3         1       0             44        4207\n## 4         0       1             44        4207\n## 5         1       0             48        4207\n## 6         0       1             48        4207\n## attr(,\"assign\")\n## [1] 1 1 2 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$sex\n## [1] \"contr.treatment\"\n\nWe can now do math with this matrix. Since we don’t have an intercept term, we don’t need to create separate objects for the slopes and intercepts and can matrix multiply the new data matrix with the model coefficients:\n\n# Get all the coefficients\n(coefs_cat &lt;- coef(model_categorical))\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\n# Matrix multiply the newdata with the slope coefficients\n(log_odds_cat &lt;- as.numeric(data_to_plug_in_cat_mat %*% coefs_cat))\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\n# Convert to probability scale\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nThe results are the same!\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\nlog_odds_cat\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05"
  },
  {
    "objectID": "blog/2023/07/28/gradient-map-fills-r-sf/index.html",
    "href": "blog/2023/07/28/gradient-map-fills-r-sf/index.html",
    "title": "How to fill maps with density gradients with R, {ggplot2}, and {sf}",
    "section": "",
    "text": "The students in my summer data visualization class are finishing up their final projects this week and I’ve been answering a bunch of questions on our class Slack. Often these are relatively standard reminders of how to tinker with specific ggplot layers (chaning the colors of a legend, adding line breaks in labels, etc.), but today one student had a fascinating and tricky question that led me down a realy fun dataviz rabbit hole. She was making a map with hundreds of points representing specific locations of events. This led to overplotting—it’s really hard to stick hundreds of dots on a small map of a city and have it make any sense. To help fix this, she wanted to fill areas of the map by the count of events, making a filled gradient rather than a bunch of points. This is fairly straightforward with regular scatterplots, but working with geographic data adds some extra wrinkles to the process.\nSo let’s all go down this rabbit hole together (mostly so future-me can remember how to do this)!\n\n\n\n\n\n\nWho this post is for\n\n\n\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly {dplyr} and {ggplot2}).\nYou’re somewhat familiar with {sf} for working with geographic data. I have a whole tutorial here and a simplified one here and the {sf} documentation has a ton of helpful vignettes and blog posts, and there are also two free books about it: Spatial Data Science and Geocomputation with R. Also check this fantastic post out to learn more about the anatomy of a geometry column with {sf}.\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(spatstat)\nlibrary(tigris)\nlibrary(rnaturalearth)\nlibrary(patchwork)\n\ntheme_set(\n  theme_void(base_family = \"Roboto Slab\") +\n    theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n)\n\n\nFixing overplotted scatterplots\nOverplotting happens when there are too many data points in one place in a plot. For instance, here’s a scatterplot of carats and prices for 54,000 diamonds, using {ggplot2}’s built-in diamonds dataset:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWoof. It’s just a blob of black points.\nTo fix overplotting, you can either restyle the points somehow so they’re not so crowded, or you can summarize the data and display it a slightly different way. There are lots of possible ways to fix this though—here’s a quick overview of some:\n\nSmaller pointsSemi-transparent points■-binned points⬢-binned pointsDensity countours\n\n\nWe can try shrinking the points down a bunch:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(size = 0.2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe can make the points partially transparent so that clusters of points are darker (with more points stacked on top of each other)\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(alpha = 0.01) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe can draw a grid across the x- and y-axes and count how many points fall inside each box, then fill each box by that count.\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_bin2d() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe can draw a hexagonal grid across the x- and y-axes and count how many points fall inside each hexagon, then fill each hexagon by that count.\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_binhex() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe can also find the combined density of points along both the x- and y-axes and plot the contours of those densities. Here, the brighter the area, the more concentrated the points:\n\nwithr::with_seed(4393, {\n  dsmall &lt;- diamonds[sample(nrow(diamonds), 1000), ]\n})\n\nggplot(dsmall, aes(x = carat, y = price)) +\n  geom_density2d_filled() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitial overplotted map\nGeographic data, however, is a little trickier to work with. Fundamentally, putting points on a map is the same as making a scatterplot, with latitude on the x-axis and longitude on the y-axis. But maps are strange. Scatterplots are nice rectangles; maps have oddly shaped borders. Scatterplots are naturally flat; maps are curved chunks of a globe and have to be flattened and reprojected into two dimensions somehow. Scatterplots come from nice rectangular datasets; maps from from complex shapefiles.\nShrinking and transparentifying points with map points is the same as with regular points: play with the size and alpha arguments in geom_sf(). Making bins and gradients, however, takes a little more work (hence this rabbit hole).\nTo illustrate this, we’ll plot all 264 campgrounds in the state of Georgia. This doesn’t involve severe overplotting (though at the end I’ve included an example of dealing with 10,000 map points), but it’s useful for playing with these different techniques.\nThe data comes from Georgia’s GIS Clearinghouse, which is a miserable ancient website that requires a (free) login. I downloaded the GNIS Cultural Features dataset (last updated in 1996; direct link + documentation). Since it’s government data from the US Department of the Interior and ostensibly public domain, you can download the shapefile here:\n\ncultural.zip\n\n\n# We'll make all the shapefiles use ESRI:102118 (NAD 1927 Georgia Statewide\n# Albers: https://epsg.io/102118)\nga_crs &lt;- st_crs(\"ESRI:102118\")\n\n# Geographic data from Georgia\nga_cultural &lt;- read_sf(\"data/cultural/cultural.shp\") %&gt;% \n  # This shapefile uses EPSG:4326 (WGS 84), but that projection information\n  # isn't included in the shapefile for whatever reason, so we need to set it\n  st_set_crs(st_crs(\"EPSG:4326\"))\n\nga_campgrounds &lt;- ga_cultural %&gt;% \n  filter(DESCRIPTOR == \"CAMP/CAMPGROUND\") %&gt;% \n  st_transform(ga_crs)\n\nWe’ll also grab a state map of Georgia and a map of all Georgia counties from the US Census Bureau using the {tigris} package:\n\n# Geographic data from the US Census\noptions(tigris_use_cache = TRUE)\nSys.setenv(TIGRIS_CACHE_DIR = \"maps\")\n\nga_state &lt;- states(cb = TRUE, resolution = \"500k\", year = 2022) %&gt;% \n  filter(STUSPS == \"GA\") %&gt;% \n  st_transform(ga_crs)\n\nga_counties &lt;- counties(state = \"13\", cb = TRUE, resolution = \"500k\", year = 2022) %&gt;% \n  st_transform(ga_crs)\n\nAnd finally, to help illustrate maps aren’t mere scatterplots, we’ll add all of Georgia’s rivers and lakes to the maps we make. Lots of campgrounds are clustered around lakes, so this will also help us see some patterns in the data. We’ll get river and lake data from the Natural Earth project, which provides all sorts of physical map data like coastlines, reefs, islands, and so on. They provide shapefiles for large rivers and lakes globally (rivers_lake_centerlines and lakes) and smaller rivers and lakes in North America specifically (rivers_north_america and lakes_north_america).\n\n# See rnaturalearth::df_layers_physical for all possible names\n# Create a vector of the four datasets we want\nne_shapes_to_get &lt;- c(\n  \"rivers_lake_centerlines\", \"rivers_north_america\",\n  \"lakes\", \"lakes_north_america\"\n)\n\n# Loop through ne_shapes_to_get and download each shapefile and store it locally\nif (!file.exists(\"maps/ne_10m_lakes.shp\")) {\n  ne_shapes_to_get %&gt;%\n    walk(~ ne_download(\n      scale = 10, type = .x, category = \"physical\",\n      destdir = \"maps\", load = FALSE\n    ))\n}\n\n# Load each pre-downloaded shapefile and store it in a list\nne_data_list &lt;- ne_shapes_to_get %&gt;%\n  map(~ {\n    ne_load(\n      scale = 10, type = .x, category = \"physical\",\n      destdir = \"maps\", returnclass = \"sf\"\n    ) %&gt;%\n      st_transform(ga_crs)\n  }) %&gt;%\n  set_names(ne_shapes_to_get)\n\n# Load all the datasets in the list into the global environment\nlist2env(ne_data_list, envir = .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\n\nThese physical shapefiles from Natural Earth contain thousands of rivers and lakes, but we only want the ones that exist in or cross through Georgia. We can use the Georgia state shapefile we got from the Census (ga_state) as a sort of cookie cutter on each of these larger shapefiles to only keep the parts of rivers and lakes that fall within Georgia’s boundaries:\n\n# ↓ these give a bunch of (harmless?) warnings about spatially constant attributes\nrivers_global_ga &lt;- st_intersection(ga_state, rivers_lake_centerlines)\nrivers_na_ga &lt;- st_intersection(ga_state, rivers_north_america)\nlakes_global_ga &lt;- st_intersection(ga_state, lakes)\nlakes_na_ga &lt;- st_intersection(ga_state, lakes_north_america)\n\nHere’s what our initial map looks like, with fancy rivers and maps added. It looks nice and detailed, but there are a lot of points, and even shrinking them down to 0.5, there are a few overplotted clusters.\n\nplot_initial &lt;- ggplot() +\n  geom_sf(data = ga_state, fill = \"grey20\") +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  geom_sf(data = ga_campgrounds, size = 0.5, color = \"grey50\") +\n  # Technically this isn't necessary since all the layers already use 102118, but\n  # we'll add it just in case I forgot to do that to one of them\n  coord_sf(crs = ga_crs)\nplot_initial\n\n\n\n\n\n\n\n\n\n\nOption 1: Fill each county by the number of campgrounds\nOne way to address this overplotting is to create bins with counts of the campgrounds in each bin. US states have a natural kind of “bin”, since they’re subdivided into counties. Georgia has an inordinate number of counties, so we can count the number of campgrounds per county and fill each county by that count. We’ll join the campground data to the county data with st_join() (which is the geographic equivalent of left_join()) and then use some group_by() %&gt;% summarize() magic to find the number of locations per county.\n\n# st_join() adds extra rows for repeated counties and returns partially blank\n# rows for counties with no campgrounds. It would ordinarily be easy to use\n# `summarize(total = n())`, but this won't be entirely accurate since counties\n# without campgrounds still appear in the combined data and would get\n# incorrectly counted. So instead, we look at one of the columns from\n# ga_campgrounds (DESCRIPTOR). If it's NA, it means that the county it was\n# joined to didn't have any campgrounds, so we can ignore it when counting.\nga_counties_campgrounds &lt;- ga_counties %&gt;% \n  st_join(ga_campgrounds) %&gt;% \n  group_by(NAMELSAD) %&gt;% \n  summarize(total = sum(!is.na(DESCRIPTOR)))\n\nWe can plot this new ga_counties_campgrounds data and fill by total:\n\nplot_county &lt;- ggplot() +\n  geom_sf(data = ga_counties_campgrounds, aes(fill = total), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\", na.value = \"black\") +\n  coord_sf(crs = ga_crs)\nplot_county\n\n\n\n\n\n\n\n\nThis already helps. We can see a cluster of campgrounds in central Georgia around the Piedmont National Wildlife Refuge and the Oconee National Forest, and another cluster in the mountains of northeast Georgia in the Chattahoochee-Oconee National forests.\n\n\nOption 2: Create a grid and fill each grid box by the number of campgrounds\nCounties are oddly shaped, though, and not all states or cities have this many subdivisions to work with. So instead, we can create our own subdivisions. We can use st_make_grid() to divide the state area up into a grid—here we’ll use 400 boxes:\n\n# Spit the state area into a 20x20 grid\nga_grid &lt;- ga_state %&gt;% \n  st_make_grid(n = c(20, 20))\n\nggplot() +\n  geom_sf(data = ga_state) +\n  geom_sf(data = ga_grid, alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\n\n\nWe can then use st_intersection() to cut the Georgia map into pieces that fall in each of those grid boxes:\n\nga_grid_map &lt;- st_intersection(ga_state, ga_grid) %&gt;% \n  st_as_sf() %&gt;% \n  mutate(grid_id = 1:n())\n\nggplot() +\n  geom_sf(data = ga_grid_map) +\n  theme_void()\n\n\n\n\n\n\n\n\nNext we can join the campground data to these boxes just like we did with the counties, and we can use group_by() %&gt;% summarize() to get counts in each grid box:\n\ncampgrounds_per_grid_box &lt;- ga_grid_map %&gt;% \n  st_join(ga_campgrounds) %&gt;% \n  group_by(grid_id) %&gt;% \n  summarize(total = sum(!is.na(DESCRIPTOR)))\n\nFinally we can plot it:\n\nplot_grid &lt;- ggplot() +\n  geom_sf(data = campgrounds_per_grid_box, aes(fill = total), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_grid\n\n\n\n\n\n\n\n\nThat feels more uniform than the counties and still highlights the clusters of campgrounds in central and northeast Georgia.\n\n\nOption 3: Fill with a gradient of the density of the number of campgrounds\nHowever, it is a little misleading. Technically there are more campgrounds in northeast Georgia than in central Georgia, but because of how (1) county boundaries happened to be drawn, and (2) how the gridlines happened to be drawn, the campgrounds in the northeast were spread across multiple counties/tiles while the campgrounds in central Georgia happened to mostly fall in one county/tile, so it looks like there are more down there.\nTo make the shading more accurate, we can turn to turn to calculus and imagine grid boxes that are infinitely small. We can calculate densities instead of binned or clustered subunits.\nDoing this with geographic data is tricky, though, and requires some extra math and an extra package to handle the fancy math: {spatstat}. (See this and this and this and this for some examples of using {spatstat}.)\nTo calculate the density of campground latitudes and longitudes, we need to first convert our geometry column to a spatial point pattern object (or a ppp object) that {spatstat} can work with. Like sf objects, a ppp object is a collection of geographic points, and it can have overall boundaries embedded in it, or what {spatstat} calls a “window”:\n\n# Convert the campground coordinates to a ppp object with a built-in window\nga_campgrounds_ppp &lt;- as.ppp(ga_campgrounds$geometry, W = as.owin(ga_state))\n\n# Check to see if it worked\nplot(ga_campgrounds_ppp)\n\n\n\n\n\n\n\n\nppp objects have a corresponding density() function that can calculate the joint densities of each point’s latitude and longitude coordinates. It also has a dimyx argument that controls the number of pixels of the density—higher numbers will create smoother and higher resolution images; smaller numbers will be chunkier and less granular. The resulting object is a pixel-based bitmap image with extra attributes that describe how to connect it back to latitude and longitude points. If we feed that image to stars::st_as_stars() (from the {stars} package), we’ll force the image to use that geographic data:\n\n# Create a stars object of the density of campground locations\ndensity_campgrounds_stars &lt;- stars::st_as_stars(density(ga_campgrounds_ppp, dimyx = 300))\n\n# Check to see what it looks like\nplot(density_campgrounds_stars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{spatstat} and projections\n\n\n\nIMPORTANTLY density.ppp() doesn’t work with all CRS systems. From my experimenting, it seems to only work with projections that use meters as their units, like Albers and NAD 83. It gave me an error anytime I tried working with decimal degrees (i.e. the −180° to 180° scale). I don’t know why :(. That’s why I’ve forced all the different geographic datasets in this post to use ESRI:102118 (Georgia Statewide Albers)—it uses meters and it works.\n\n\nWe can convert this {stars} object back to {sf} so it’s normal and plottable with geom_sf():\n\nga_campgrounds_density &lt;- st_as_sf(density_campgrounds_stars) %&gt;%\n  st_set_crs(ga_crs)\n\nWe can then plot it with ggplot() and geom_sf() like normal, filling by v, which is the column that stores the calculated density:\n\nplot_density &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\")\nplot_density\n\n\n\n\n\n\n\n\nooooh that’s so pretty already. Let’s add all the rivers and lakes:\n\nplot_density_fancy &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_density_fancy\n\n\n\n\n\n\n\n\nAbsolutely stunning.\nWe can add the actual campground points back in too:\n\nplot_density_fancy_points &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  geom_sf(data = ga_campgrounds, size = 0.3, color = \"grey80\") +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_density_fancy_points\n\n\n\n\n\n\n\n\nThat’s so so cool.\n\n\nComparison\nHere’s what all these options look like together. There’s no one single best option—it depends on what story you’re trying to tell, how the data is distributed, how crowded it is, and so on—but it’s cool that there are so many options!\n\n\nCode\nlayout &lt;- \"\n#####\n#A#BC\n#####\n#D#EF\n#####\nG####\n\"\n\n(plot_initial + labs(title = \"Overplotted\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  (plot_county + labs(title = \"Filled by county\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  plot_spacer() +\n  (plot_grid + labs(title = \"Filled by grid\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) + \n  (plot_density_fancy + labs(title = \"Filled by density\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  plot_spacer() + plot_spacer() +\n  plot_layout(design = layout, widths = c(0.02, 0.47, 0.02, 0.47, 0.02), heights = c(0.02, 0.47, 0.02, 0.47, 0.02)) +\n  plot_annotation(theme = theme(plot.background = element_rect(fill = \"grey95\", color = NA)))\n\n\n\n\n\n\n\n\n\n\n\nExtra bonus fun: 10,000+ churches\nFinally, for some extra fun, let’s plot something that’s actually overplotted—Georgia’s 10,000+ churches!\n\nga_churches &lt;- ga_cultural %&gt;% \n  filter(DESCRIPTOR == \"CHURCH\") %&gt;% \n  st_transform(st_crs(\"ESRI:102118\"))\n\nggplot() +\n  geom_sf(data = ga_state) +\n  geom_sf(data = ga_churches)\n\n\n\n\n\n\n\n\nlol this is basically just a wildly overplotted population map at this point. Let’s calculate the density of these locations and plot a gradient:\n\n# Convert the church coordinates to a ppp object with a built-in window\nga_churches_ppp &lt;- as.ppp(ga_churches$geometry, W = as.owin(ga_state))\n\n# Create a stars object (whatever that is) of the density of church locations\ndensity_churches_stars &lt;- stars::st_as_stars(density(ga_churches_ppp, dimyx = 300))\n\n# Convert the stars object to an sf object so it's normal and plottable again\nga_churches_density &lt;- st_as_sf(density_churches_stars) %&gt;%\n  st_set_crs(ga_crs)\n\n\nggplot() +\n  geom_sf(data = ga_churches_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = ga_churches, size = 0.005, alpha = 0.3) +\n  scale_fill_viridis_c(option = \"rocket\", guide = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\n\nCheck out that hugely bright spot in Atlanta!\n\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2023,\n  author = {Heiss, Andrew},\n  title = {How to Fill Maps with Density Gradients with {R,}\n    \\{Ggplot2\\}, and \\{Sf\\}},\n  date = {2023-07-28},\n  url = {https://knuutila.net/blog/2023/07/28/gradient-map-fills-r-sf},\n  doi = {10.59350/bsctw-0a955},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2023. “How to Fill Maps with Density Gradients with\nR, {Ggplot2}, and {Sf}.” July 28, 2023. https://doi.org/10.59350/bsctw-0a955."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "",
    "text": "As I explained in my previous blog post, in June 2023 I drove my family across the country in a 5,000-mile roundtrip odyssey from Atlanta, Georgia to just-south-of-Salt Lake City, Utah, and back again. Instead of taking a direct path through the middle the United States, we decided to hit as many tourist-y stops along the way, making a big circle along the southern US on the way out and along the northern US on the way back, stopping at a dozen really neat places, including:\nIn an effort to stay On Brand™, I wanted to keep a log of all our stops so I could go back and analyze the trip. How many miles would we drive? How much time would we spend in the car? How long would each gas stop and bathroom break be? Could I make some sort of map showing our journey?\nSo, at the first gas station we stopped at in Alabama, I started a new file in my phone’s Notes app and dutifully wrote down the arrival time, name and address of the gas station, and departure time. I had a nascent data file and was ready to use it through the trip.\nAbout an hour after leaving our second gas station stop just outside Mississippi, I realized that I forgot to write down anything about the second gas station stop. My data collection process died after a single entry.\nI was initially disappointed that I wouldn’t have any sort of data to work with at the end of the trip, but then I remembered that my phone was tracking every part of the whole trip through Google Maps. When we got to the first hotel, I searched Google to see if it was possible to export your location history from Google Maps and saw that it was, so I gave up on trying to keep my own log and outsourced all that to Google instead. I didn’t know what exactly Google was tracking, but I figured it would at least have the date, time, and location of where we went, so that was good enough.\nRight after we got home from our trip last week, I exported my data and was shocked by how much detail Google actually had. The exported JSON files contained 100,000+ location entries going all the back to 2013 (!!!). Parsing and sifting through and working with all this data initially seemed overwhelming (especially since Google doesn’t actually provide any formal documentation for these files!), but once I worked through a few tricky issues, it was surprisingly straightforward to make maps, calculate distances and times, and do all sorts of other fun things with the data.\nSo in this post I do three things:"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#settings",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#settings",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Settings",
    "text": "Settings\n\n\n\n\n\n\nComplete documentation for Settings.json\n\n\n\n\nFull documentation\n\n\n\nSettings.json is a small file that contains a bunch of device-specific settings and details, like the unique ID for your phone, the date and time when you added your phone to your account, and so on. It’s not super important for working with your data—consider it extra metadata."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Records",
    "text": "Records\n\n\n\n\n\n\nComplete documentation for Records.json\n\n\n\n\nBasic overview\nFull documentation\n\n\n\nRecords.json is important. This contains an entry for every time Google Maps recorded your location somewhere. Here’s what a typical entry looks like (each entry is nested under a \"locations\" property):\n{\n  \"locations\": [{\n    \"latitudeE7\": 300489450,\n    \"longitudeE7\": -899629451,\n    \"accuracy\": 9,\n    \"velocity\": 0,\n    \"heading\": 329,\n    \"altitude\": 1,\n    \"verticalAccuracy\": 4,\n    \"deviceTag\": 0,\n    \"platformType\": \"IOS\",\n    \"serverTimestamp\": \"2023-06-04T01:05:58.248Z\",\n    \"deviceTimestamp\": \"2023-06-04T01:05:57.559Z\",\n    \"batteryCharging\": false,\n    \"formFactor\": \"PHONE\",\n    \"timestamp\": \"2023-06-04T00:52:57Z\"\n  }]\n}\n\nLocation details like latitudeE7 and longitudeE7, multiplied by 10,000,000 so there are no decimals\nAccuracy details like accuracy, or radius of the location measurement, in meters. Smaller numbers mean better precision.\nPosition details like heading (degrees east of north, from 0 to 359), velocity (meters per second), altitude (meters), and verticalAccuracy (accuracy of altitude in meters; smaller means better precision)\nDevice details like deviceTag (Google’s internal ID for your device; I changed mine to 0 in this post for privacy reasons), platformType and formFactor (type of device), batteryCharging (whether your device was charging at the time), and some timestamps for the device time, server time, and UTC time.\n\nThis is pretty straightforward to work with, since it’s essentially rectangular data, or data that can be thought of as rows and columns. Each location entry in the JSON file is a row; each property is a column (accuracy is 9, altitude is 1, and so on), like this:\n\nExample dataset from Google Location History JSON\n\n\n\n\n\n\n\n\n\n\nrowid\nlatitudeE7\nlongitudeE7\n…\nformFactor\ntimestamp\n\n\n\n\n54383\n300489450\n-899629451\n…\nPHONE\n2023-06-04T00:52:57Z\n\n\n54384\n…\n…\n…\n…\n…\n\n\n\nThe only data manipulation we need to do is modify the latitude and longitude columns. Both of those have been multiplied by 10,000,000 so that there are no decimal points in the coordinates (probably to avoid the weirdness inherent in floating point math). There’s actually a hint about this in the column names for latitudeE7 and longitudeE7—in R and most other programming languages, 1e7 is computer notation for \\(10^7\\). If we divide both those columns by 10,000,000, we’ll get standard-looking geographic coordinates:\n\n300489450 / 10000000\n## [1] 30.05\n-899629451 / 1e7  # We can write it as 1e7 too\n## [1] -89.96\n\nFor the sake of this post, the most important pieces of data here will be location (latitudeE7 and longitudeE7), time (timestamp), and elevation (altitude). The velocity data might be cool to do something with some day, but I won’t touch it for now."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Semantic location history",
    "text": "Semantic location history\n\n\n\n\n\n\nComplete documentation for semantic location history files\n\n\n\n\nBasic overview\nFull documentation\n\n\n\nThese month-specific JSON files are super neat and full of really rich data. Each entry is one of two types of events: activity segments (activitySegment) and place visits (placeVisit).\nHere’s what the beginning of a typical activity segment entry looks like (with a lot of parts omitted for space—click here for a complete example entry):\n{\n  \"timelineObjects\": [\n    {\n      \"activitySegment\": {\n        \"startLocation\": {\n          \"latitudeE7\": 339703585,\n          \"longitudeE7\": -842416959,\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          }\n        },\n        \"endLocation\": {\n          \"latitudeE7\": 339786509,\n          \"longitudeE7\": -842006268,\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          }\n        },\n        \"duration\": {\n          \"startTimestamp\": \"2023-06-01T00:15:26.999Z\",\n          \"endTimestamp\": \"2023-06-01T00:28:47Z\"\n        },\n        \"distance\": 5483,\n        \"activityType\": \"IN_PASSENGER_VEHICLE\",\n        \"confidence\": \"HIGH\",\n        \"activities\": [\n          {\n            \"activityType\": \"IN_PASSENGER_VEHICLE\",\n            \"probability\": 91.4052665233612\n          },\n          {\n            \"activityType\": \"STILL\",\n            \"probability\": 6.64205327630043\n          },\n          ...\n        ],\n        ...\n      }\n    }\n  ]\n}\nAnd here’s the beginning typical place visit entry (click here for a complete example entry)\n{\n  \"timelineObjects\": [\n    {\n      \"placeVisit\": {\n        \"location\": {\n          \"latitudeE7\": 323281024,\n          \"longitudeE7\": -863373283,\n          \"placeId\": \"ChIJQ0NgzsqAjogRISqopW4DZMc\",\n          \"address\": \"1030 W South Blvd, Montgomery, AL 36105, USA\",\n          \"name\": \"Chevron\",\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          },\n          \"locationConfidence\": 89.41639,\n          \"calibratedProbability\": 80.56358\n        },\n        \"duration\": {\n          \"startTimestamp\": \"2023-06-03T14:27:23.001Z\",\n          \"endTimestamp\": \"2023-06-03T14:50:24Z\"\n        },\n        \"placeConfidence\": \"MEDIUM_CONFIDENCE\",\n        ...\n      }\n    }\n  ]\n}\nWorking with these gnarly JSON files is a lot trickier than working with Records.json because all this data is deeply nested within specific properties of the activity or place visit.\nTo help visualize all this nesting, here’s what a single placeVisit entry looks like (screenshot via this JSON tree visualizer), with the visit details + alternative places + child visits and their alternative locations and confidence levels + lots of other details:\n\n\n\n\nJSON tree for one placeVisit entry\n\n\n\nYikes.\nFor this post, I don’t care about all the alternative possible locations or possible modes of transportation or confidence levels or anything. I cleaned those up in my Google Timeline before exporting the data, so I’m pretty confident everything is relatively correct.\n\n\n\n\n\n\nMore on cleaning up your Google Timeline\n\n\n\n\n\nGoogle actually has a really nice web-based frontend for your whole location history at Google Timeline.\n\n\n\nMy Google Timeline from June 4, 2023\n\n\nBefore downloading my location data from this road trip, I checked my history for each day at Google Timeline to confirm that it was marking the right trips locations. It generally did a good job, but when an entry has lower confidence (the confidence entry in the JSON data), Google shows you its uncertainty. For instance, here it’s not sure about two stops:\n\n\n\nGoogle’s uncertainty about two locations\n\n\nThose are both correct, so I clicked on the check to confirm.\nOther times, it gets things wrong. Like here, it thought I was on a motorcycle for this leg of the trip:\n\n\n\nGoogle’s incorrect guess that I was motorcycling instead of driving\n\n\nThat’s obviously not correct, so I edited it to be “Driving” instead:\n\n\n\nSwitching motorcylcing to driving\n\n\nAnd other times, when cell reception is bad, it misses entire activities. Like here, where we went to Devil’s Tower where there’s no cell reception at all, Google saw that we left the hotel at 8:14 AM and somehow magically arrived at Devil’s Tower at 9:21 AM. It failed to record how we did it, so I clicked on “Add activity” and told Google I was driving.\n\n\n\nMissing activity\n\n\n\n\n\nHere’s all I’ll worry about here:\nFor placeVisits:\n\nLocation coordinates (location → latitudeE7 and location → longitudeE7)\nLocation ID (location → placeId): Google’s internal ID for the place\nLocation name (location → name)\nLocation address (location → address)\nStart and end times (duration → startTimestamp and duration → endTimestamp)\n\nFor activitySegments:\n\nDistance (distance)\nActivity type (activityType)\nStart and end coordinates (startLocation → (latitudeE7 & longitudeE7) and endLocation → (latitudeE7 & longitudeE7))\nStart and end times (duration → startTimestamp and duration → endTimestamp)"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records.json",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records.json",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Records.json",
    "text": "Records.json\n\nLoading and cleaning data\nLoading Records.json is easy. Because it is essentially rectangular data already, we can use simplifyVector = TRUE to have R convert the locations slot of the JSON file to a data frame automatically.\n\nall_locations_raw &lt;- read_json(\"data/Records.json\", simplifyVector = TRUE) %&gt;% \n  # Pull out the \"locations\" slot (this is the same as doing full_data$locations)\n  pluck(\"locations\") %&gt;% \n  # Make this a tibble just so it prints nicer here on the blog\n  as_tibble() \nall_locations_raw\n## # A tibble: 2,288 × 15\n##    latitudeE7 longitudeE7 accuracy velocity heading altitude verticalAccuracy deviceTag platformType serverTimestamp          batteryCharging formFactor timestamp                source deviceTimestamp\n##         &lt;int&gt;       &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;    &lt;int&gt;            &lt;int&gt;     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                    &lt;lgl&gt;           &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;  &lt;chr&gt;          \n##  1  338465853  -843051758        4       30     205      271                3         0 IOS          2023-06-03T12:00:21.140Z TRUE            PHONE      2023-06-03T12:00:21.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  2  338361607  -843273246       25       32     238      272                3         0 IOS          2023-06-03T12:01:41.112Z TRUE            PHONE      2023-06-03T12:01:41Z     &lt;NA&gt;   &lt;NA&gt;           \n##  3  338283154  -843438356        4       29     228      270                3         0 IOS          2023-06-03T12:02:36.183Z TRUE            PHONE      2023-06-03T12:02:36.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  4  338185757  -843621145        4       30     203      259                3         0 IOS          2023-06-03T12:03:46.098Z TRUE            PHONE      2023-06-03T12:03:46Z     &lt;NA&gt;   &lt;NA&gt;           \n##  5  338086421  -843782498        4       30     233      277                3         0 IOS          2023-06-03T12:04:51.107Z TRUE            PHONE      2023-06-03T12:04:51.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  6  337970326  -843935971        4       27     192      269                3         0 IOS          2023-06-03T12:05:56.167Z TRUE            PHONE      2023-06-03T12:05:55.999Z &lt;NA&gt;   &lt;NA&gt;           \n##  7  337805263  -843911732        4       27     180      282                3         0 IOS          2023-06-03T12:07:03.134Z TRUE            PHONE      2023-06-03T12:07:03.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  8  337672676  -843890003        4       23     125      291                3         0 IOS          2023-06-03T12:08:08.143Z TRUE            PHONE      2023-06-03T12:08:08Z     &lt;NA&gt;   &lt;NA&gt;           \n##  9  337553940  -843786019        4       27     181      310                3         0 IOS          2023-06-03T12:09:16.194Z TRUE            PHONE      2023-06-03T12:09:16.001Z &lt;NA&gt;   &lt;NA&gt;           \n## 10  337437364  -843914546        4       26     207      295                3         0 IOS          2023-06-03T12:10:24.118Z TRUE            PHONE      2023-06-03T12:10:24Z     &lt;NA&gt;   &lt;NA&gt;           \n## # ℹ 2,278 more rows\n\nWe need to clean this data up a bit before using it:\n\nRight now the latitude and longitude coordinates are multiplied by 10,000,000, and they’re just numbers—R doesn’t know that they’re geographic coordinates. We need to divide them by \\(10^7\\) and then use them to create an {sf}-enabled geometry column using st_as_sf() from {sf}.\nRight now all the timestamps are in UTC (Greenwich Mean Time), not local time. Ordinarily converting UTC times to local time zones is really easy with with_tz() from {lubridate}:\n\n# One of the times from the trip\ntime_utc &lt;- ymd_hms(\"2023-06-03 14:27:23\")\ntime_utc\n## [1] \"2023-06-03 14:27:23 UTC\"\n\n# Convert it to US Central Daylight Time since we were in Alabama at the time\nwith_tz(time_utc, tzone = \"America/Chicago\")\n## [1] \"2023-06-03 09:27:23 CDT\"\n\nHowever, this was a big road trip and we covered every possible time zone in the continental US (technically not Pacific Daylight Time, but Arizona doesn’t use daylight saving time, so it’s the same time as Pacific time during the summer), so we can’t just convert all the timestamps to just one time zone. We need to figure out which US time zone each latitude/longitude point is in.\nI fretted a lot about the best way to do this! I eventually stumbled across a project that has shapefiles for all the world’s time zones and was about to start using that to look up which time zone each point fell inside, but then I discovered that there’s already an R package that does this (and it uses the same time zone boundary project behind the scenes): {lutz} (look up time zones). We just have to use tz_lookup() and it’ll identify the correct time zone for each point.\nBut there’s one more wrinkle! This is surprisingly difficult! You’d think we could just use something like mutate(local_time = with_tz(timestamp, tzone = \"whatever\") and make a local time column, but NOPE. R stores time zone details as an attribute for the entire column, so there’s no way to have a column with times across multiple time zones (see this issue here). Notice how the local_time column here is the same regardless of time zone:\n\nexample_thing &lt;- tribble(\n  ~timestamp, ~tz,\n  \"2023-06-01 00:00:00\", \"America/New_York\",\n  \"2023-06-01 01:00:00\", \"America/New_York\",\n  \"2023-06-01 00:00:00\", \"America/Phoenix\",\n  \"2023-06-01 01:00:00\", \"America/Phoenix\"\n) %&gt;% \n  mutate(timestamp = ymd_hms(timestamp))\n\n# The local times are all the same (New York) because columns can only have one time zone\nexample_thing %&gt;% \n  # Have to use rowwise() because with_tz() isn't vectorized (with good reason)\n  rowwise() %&gt;% \n  mutate(local_time = with_tz(timestamp, tzone = tz))\n## # A tibble: 4 × 3\n## # Rowwise: \n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 20:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 21:00:00\n\nSo as a workaround, we can group by time zone, which lets us work on a smaller subset of the data for each time zone, thus getting around the “one time zone per column” rule. However, that still doesn’t quite work, because when R is done with the grouping, it puts everything back together in one column, thus breaking the “one time zone per column” rule, and New York time takes over for the whole column again:\n\nexample_thing %&gt;% \n  group_by(tz) %&gt;% \n  mutate(local_time = with_tz(timestamp, tz))\n## # A tibble: 4 × 3\n## # Groups:   tz [2]\n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 20:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 21:00:00\n\nTo get around that, we can use force_tz() to convert each local time back to a fake version of UTC. Each timestamp all have the same time zone attribute (UTC), but the time will be US Eastern, Central, Mountain, or whatever. It’s the local time as if it were UTC. It works!\n\nexample_thing %&gt;% \n  group_by(tz) %&gt;% \n  mutate(local_time = force_tz(with_tz(timestamp, tz), \"UTC\"))\n## # A tibble: 4 × 3\n## # Groups:   tz [2]\n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 17:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 18:00:00\n\nFinally we’ll add some extra columns for the day, month, and year based on the local time. The file I’m working with here only contains a few days from our trip, but if you’re using your own data, you’ll likely have years of entries, so these columns help with filtering (i.e. you can use filter(year == 2017) to see your whole location history for that year.)\n\n\nall_locations &lt;- all_locations_raw %&gt;% \n  # Scale down the location data (divide any column that ends in E7 by 10000000)\n  mutate(across(ends_with(\"E7\"), ~ . / 1e7)) %&gt;% \n  # Create a geometry column with the coordinates\n  st_as_sf(coords = c(\"longitudeE7\", \"latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz = tz_lookup(., method = \"accurate\")) %&gt;% \n  # Convert the timestamp to an actual UTC-based timestamp\n  mutate(timestamp = ymd_hms(timestamp, tz = \"UTC\")) %&gt;% \n  # Create a version of the timestamp in local time, but in UTC\n  group_by(tz) %&gt;% \n  mutate(timestamp_local = force_tz(with_tz(timestamp, tz), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(timestamp_local),\n    month = month(timestamp_local),\n    day = day(timestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(timestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  )\nall_locations\n## Simple feature collection with 2288 features and 19 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 2,288 × 20\n##    accuracy velocity heading altitude verticalAccuracy deviceTag platformType serverTimestamp          batteryCharging formFactor timestamp           source deviceTimestamp       geometry tz               timestamp_local      year month   day day_month\n##  *    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;    &lt;int&gt;            &lt;int&gt;     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                    &lt;lgl&gt;           &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [°]&gt; &lt;chr&gt;            &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;    \n##  1        4       30     205      271                3         0 IOS          2023-06-03T12:00:21.140Z TRUE            PHONE      2023-06-03 12:00:21 &lt;NA&gt;   &lt;NA&gt;            (-84.31 33.85) America/New_York 2023-06-03 08:00:21  2023     6     3 June 3   \n##  2       25       32     238      272                3         0 IOS          2023-06-03T12:01:41.112Z TRUE            PHONE      2023-06-03 12:01:41 &lt;NA&gt;   &lt;NA&gt;            (-84.33 33.84) America/New_York 2023-06-03 08:01:41  2023     6     3 June 3   \n##  3        4       29     228      270                3         0 IOS          2023-06-03T12:02:36.183Z TRUE            PHONE      2023-06-03 12:02:36 &lt;NA&gt;   &lt;NA&gt;            (-84.34 33.83) America/New_York 2023-06-03 08:02:36  2023     6     3 June 3   \n##  4        4       30     203      259                3         0 IOS          2023-06-03T12:03:46.098Z TRUE            PHONE      2023-06-03 12:03:46 &lt;NA&gt;   &lt;NA&gt;            (-84.36 33.82) America/New_York 2023-06-03 08:03:46  2023     6     3 June 3   \n##  5        4       30     233      277                3         0 IOS          2023-06-03T12:04:51.107Z TRUE            PHONE      2023-06-03 12:04:51 &lt;NA&gt;   &lt;NA&gt;            (-84.38 33.81) America/New_York 2023-06-03 08:04:51  2023     6     3 June 3   \n##  6        4       27     192      269                3         0 IOS          2023-06-03T12:05:56.167Z TRUE            PHONE      2023-06-03 12:05:55 &lt;NA&gt;   &lt;NA&gt;             (-84.39 33.8) America/New_York 2023-06-03 08:05:55  2023     6     3 June 3   \n##  7        4       27     180      282                3         0 IOS          2023-06-03T12:07:03.134Z TRUE            PHONE      2023-06-03 12:07:03 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.78) America/New_York 2023-06-03 08:07:03  2023     6     3 June 3   \n##  8        4       23     125      291                3         0 IOS          2023-06-03T12:08:08.143Z TRUE            PHONE      2023-06-03 12:08:08 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.77) America/New_York 2023-06-03 08:08:08  2023     6     3 June 3   \n##  9        4       27     181      310                3         0 IOS          2023-06-03T12:09:16.194Z TRUE            PHONE      2023-06-03 12:09:16 &lt;NA&gt;   &lt;NA&gt;            (-84.38 33.76) America/New_York 2023-06-03 08:09:16  2023     6     3 June 3   \n## 10        4       26     207      295                3         0 IOS          2023-06-03T12:10:24.118Z TRUE            PHONE      2023-06-03 12:10:24 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.74) America/New_York 2023-06-03 08:10:24  2023     6     3 June 3   \n## # ℹ 2,278 more rows\n\nOver the 4 days I’ve included in this JSON file, Google recorded my location 2,288 times(!!).\n\n\nBasic maps\nSince we converted the latitude and longitude coordinates into a geometry column, we can plot it really easily:\n\n# I used https://www.openstreetmap.org/export to make this window\ntrip_window &lt;- st_sfc(\n  st_point(c(-115.137, 24.567)),  # left (west), bottom (south)\n  st_point(c(-79.146, 37.475)),   # right (east), top (north)\n  crs = st_crs(\"EPSG:4326\")  # WGS 84\n) %&gt;% \n  st_coordinates()\ntrip_window\n##            X     Y\n## [1,] -115.14 24.57\n## [2,]  -79.15 37.48\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = all_locations) +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()\n\n\n\n\n\n\n\n\nThat looks like a solid thick black line, but it’s not. It’s actually 2,288 individual dots. We can confirm if we zoom in really close (this is the area around The Alamo in San Antonio, Texas):\n\nall_locations %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addCircles() %&gt;% \n  # I got these coordinates from Google Maps\n  setView(lat = 29.425733271447523, lng = -98.48627553904525, zoom = 18)\n\n\n\n\n\n \nWe can convert all these points into a single path (or a LINESTRING simple geographic feature instead of lots of POINTs) so that they can connect, using a combination of st_combine() to concatenate all the points and st_cast() to switch their format from a bunch of points to an official LINESTRING:\n\nbig_long_combined_route &lt;- all_locations$geometry %&gt;% \n  st_combine() %&gt;% \n  st_cast(\"LINESTRING\")\nbig_long_combined_route\n## Geometry set for 1 feature \n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n\nNow instead of having two thousand+ points, it’s just one single LINESTRING feature. It shows up as a line now:\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = big_long_combined_route) +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()\n\n\n\n\n\n\n\n\nWe can confirm if we zoom in around the Alamo again too:\n\nbig_long_combined_route %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolylines() %&gt;% \n  setView(lat = 29.425733271447523, lng = -98.48627553904525, zoom = 18)\n\n\n\n\n\n \nThe big_long_combined_route object we just made here isn’t a data frame anymore—it’s a list instead. It still works with geom_sf(), but because we collapsed all the points into one line, we lots lots of detail, like timestamps. We can keep some of those details if we use group_by() to separate lines per day (or hour or whatever grouping variable we want). For instance, earlier we created a column called day. Let’s make a separate linestring for each day using group_by() and nest() and map():\n\n# Combine all the points in the day into a connected linestring\ndaily_routes &lt;- all_locations %&gt;% \n  group_by(day_month) %&gt;% \n  nest() %&gt;% \n  mutate(path = map(data, ~st_cast(st_combine(.), \"LINESTRING\"))) %&gt;% \n  unnest(path) %&gt;% \n  st_set_geometry(\"path\")\ndaily_routes\n## Simple feature collection with 4 features and 2 fields\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 4 × 3\n## # Groups:   day_month [4]\n##   day_month data                                                                                                 path\n##   &lt;fct&gt;     &lt;list&gt;                                                                                   &lt;LINESTRING [°]&gt;\n## 1 June 3    &lt;sf [521 × 19]&gt; (-84.31 33.85, -84.33 33.84, -84.34 33.83, -84.36 33.82, -84.38 33.81, -84.39 33.8, -8...\n## 2 June 4    &lt;sf [596 × 19]&gt; (-89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -...\n## 3 June 5    &lt;sf [509 × 19]&gt; (-98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 ...\n## 4 June 6    &lt;sf [662 × 19]&gt; (-104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 ...\n\nNow we have a data frame with a row per day, and a corresponding path per day too:\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = daily_routes, aes(color = day_month),\n    linewidth = 1) +\n  geom_label_repel(\n    data = daily_routes,\n    aes(label = day_month, fill = day_month, geometry = path),\n    stat = \"sf_coordinates\", seed = 12345,\n    color = \"white\", size = 3, segment.color = \"grey30\", \n    min.segment.length = 0, box.padding = 1,\n    show.legend = FALSE, family = \"Overpass ExtraBold\"\n  ) +\n  scale_color_manual(values = clrs[c(2, 4, 7, 9)], name = NULL, guide = \"none\") +\n  scale_fill_manual(values = clrs[c(2, 4, 7, 9)], name = NULL, guide = \"none\") +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history-1",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history-1",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Semantic location history",
    "text": "Semantic location history\nWorking with Records.json is great and fairly straightforward (once we handle all the weird data cleaning issues and time zones!), but we can do even cooler things with the more detailed semantic location history data. As seen earlier, this data is far more complex, with all sorts of nested entries and predicted probabilities of modes of transportation or location stops, so we can’t use the simplifyVector to get a basic data frame. Instead, we need to pull out each of the elements we’re interested in and build our own data frame.\nAdditionally, the location history file includes both placeVisits and activitySegments in the same file, so to make life a little easier, we can filter the data after we read the file to only keep one of the types of events using map()\n\nplaceVisits\n\nplace_visits_raw &lt;- read_json(\n  \"data/Semantic Location History/2023/2023_JUNE_truncated.json\", \n  simplifyVector = FALSE\n) %&gt;% \n  # Extract the timelineObjects JSON element\n  pluck(\"timelineObjects\") %&gt;%\n  # Filter the list to only keep placeVisits\n  # { More verbose function-based approach: map(~ .x[[\"placeVisit\"]]) }\n  # Neat selection-based approach with just the name!\n  map(\"placeVisit\") %&gt;% \n  # Discard all the empty elements (i.e. the activitySegments)\n  compact()\n\nFor this post, I’m interested in looking at all the places we stopped for gas and bathroom breaks along the way. I’m not interested in hotel stops or tourist stops. I want to know how long we typically stopped for breaks. There are 24 entries in this list…\n\nlength(place_visits_raw)\n## [1] 24\n\n…but not all of them are gas or bathroom breaks, so I manually looked through the data and copied the location IDs of the stops that weren’t gas stops.\n\nnot_driving_stops &lt;- c(\n  \"ChIJnwDSTcsDnogRLyt_lqVprLY\",  # Hotel in New Orleans, LA\n  \"ChIJgcNDAhKmIIYRRA4mio_7VgE\",  # Parking in the French Quarter\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\",  # Louis Armstrong Park\n  \"ChIJ59n6fW8dnogRWi-5N6olcyU\",  # Chalmette Battlefield\n  \"ChIJ_7z4c1_2XIYR6b9p0NvEiVE\",  # Hotel in San Antonio, TX\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\",  # The Alamo\n  \"ChIJAdu5Qad544YRhyJT8qzimi4\",  # Hotel in Carlsbad, NM\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\",  # Carlsbad Caverns\n  \"ChIJERiZZMWOLYcRQbo78w80s34\"   # Hotel in Flagstaff, AZ\n)\n\nCurrently, place_visits_raw is a huge nested list with the complete place information from Google. All we care about is a small subset of that data, so we can take the list, extract the parts we need, and build a data frame with map() %&gt;% list_rbind() (the recommended replacement for {purrr}’s now-superseded map_df()). Like we did with Records.json, we’ll also figure out the time zone for each point and make all the timestamps be local.\n\n# Computer friendly timezones like America/New_York work for computers, but I\n# want to sometimes show them as US-standard abbreviations like EDT (Eastern\n# Daylight Time), so here's a little lookup table we can use to join to bigger\n# datasets for better abbreviations\ntz_abbreviations &lt;- tribble(\n  ~tz,                ~tz_abb,\n  \"America/New_York\", \"EDT\",\n  \"America/Chicago\",  \"CDT\",\n  \"America/Denver\",   \"MDT\",\n  \"America/Phoenix\",  \"MST\"\n)\n\nplace_visits &lt;- place_visits_raw %&gt;% \n  # Extract parts of the nested list\n  map(~{\n    tibble(\n      id = .x$location$placeId,\n      latitudeE7 = .x$location$latitudeE7 / 1e7,\n      longitudeE7 = .x$location$longitudeE7 / 1e7,\n      name = .x$location$name,\n      address = .x$location$address,\n      startTimestamp = ymd_hms(.x$duration$startTimestamp, tz = \"UTC\"),\n      endTimestamp = ymd_hms(.x$duration$endTimestamp, tz = \"UTC\")\n    )\n  }) %&gt;% \n  list_rbind() %&gt;% \n  # Calculate the duration of the stop\n  mutate(duration = endTimestamp - startTimestamp) %&gt;% \n  # Make an indicator for if the stop was a gas or bathroom break\n  mutate(driving_stop = !(id %in% not_driving_stops)) %&gt;% \n  # Make a geometry column\n  st_as_sf(coords = c(\"longitudeE7\", \"latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz = tz_lookup(., method = \"accurate\")) %&gt;% \n  # Create a version of the timestamp in local time, but in UTC\n  group_by(tz) %&gt;% \n  mutate(\n    startTimestamp_local = force_tz(with_tz(startTimestamp, tz), \"UTC\"),\n    endTimestamp_local = force_tz(with_tz(endTimestamp, tz), \"UTC\")\n  ) %&gt;% \n  ungroup() %&gt;% \n  # Add a column for direction\n  # In the real data, I have values for \"There\" (the trip from Atlanta to Utah)\n  # and \"Back again\" (the trip from Utah to Atlanta)\n  mutate(direction = \"There\") %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(startTimestamp_local),\n    month = month(startTimestamp_local),\n    day = day(startTimestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(startTimestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  ) %&gt;% \n  # Bring in abbreviated time zones\n  left_join(tz_abbreviations, by = join_by(tz))\nplace_visits\n## Simple feature collection with 24 features and 16 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 24 × 17\n##    id                          name                            address                                            startTimestamp      endTimestamp        duration    driving_stop       geometry tz              startTimestamp_local endTimestamp_local  direction  year month   day day_month tz_abb\n##    &lt;chr&gt;                       &lt;chr&gt;                           &lt;chr&gt;                                              &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;      &lt;lgl&gt;           &lt;POINT [°]&gt; &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; \n##  1 ChIJQ0NgzsqAjogRISqopW4DZMc Chevron                         1030 W South Blvd, Montgomery, AL 36105, USA       2023-06-03 14:27:23 2023-06-03 14:50:24  23.02 mins TRUE         (-86.34 32.33) America/Chicago 2023-06-03 09:27:23  2023-06-03 09:50:24 There      2023     6     3 June 3    CDT   \n##  2 ChIJMU2weFNKmogRNvJ9RoXN_Vk Walmart Supercenter             5245 Rangeline Service Rd, Mobile, AL 36619, USA   2023-06-03 17:12:34 2023-06-03 17:59:38  47.07 mins TRUE         (-88.16 30.59) America/Chicago 2023-06-03 12:12:34  2023-06-03 12:59:38 There      2023     6     3 June 3    CDT   \n##  3 ChIJnwDSTcsDnogRLyt_lqVprLY Comfort Suites New Orleans East 7051 Bullard Ave, New Orleans, LA 70128, USA       2023-06-03 19:45:22 2023-06-03 20:52:17  66.90 mins FALSE        (-89.96 30.05) America/Chicago 2023-06-03 14:45:22  2023-06-03 15:52:17 There      2023     6     3 June 3    CDT   \n##  4 ChIJgcNDAhKmIIYRRA4mio_7VgE P149 - Chartres St Garage       537 Chartres St, New Orleans, LA 70130, USA        2023-06-03 21:08:30 2023-06-03 22:31:08  82.64 mins FALSE        (-90.06 29.96) America/Chicago 2023-06-03 16:08:30  2023-06-03 17:31:08 There      2023     6     3 June 3    CDT   \n##  5 ChIJv30_Xw-mIIYRpt26QbLBh58 Louis Armstrong Park            701 N Rampart St, New Orleans, LA 70116, USA       2023-06-03 22:44:22 2023-06-03 22:59:51  15.48 mins FALSE        (-90.07 29.96) America/Chicago 2023-06-03 17:44:22  2023-06-03 17:59:51 There      2023     6     3 June 3    CDT   \n##  6 ChIJgcNDAhKmIIYRRA4mio_7VgE P149 - Chartres St Garage       537 Chartres St, New Orleans, LA 70130, USA        2023-06-03 23:10:10 2023-06-03 23:19:40   9.50 mins FALSE        (-90.06 29.96) America/Chicago 2023-06-03 18:10:10  2023-06-03 18:19:40 There      2023     6     3 June 3    CDT   \n##  7 ChIJnwDSTcsDnogRLyt_lqVprLY Comfort Suites New Orleans East 7051 Bullard Ave, New Orleans, LA 70128, USA       2023-06-03 23:37:50 2023-06-04 14:01:02 863.20 mins FALSE        (-89.96 30.05) America/Chicago 2023-06-03 18:37:50  2023-06-04 09:01:02 There      2023     6     3 June 3    CDT   \n##  8 ChIJ59n6fW8dnogRWi-5N6olcyU Chalmette Battlefield           1 Battlefield Rd, Chalmette, LA 70043, USA         2023-06-04 14:24:06 2023-06-04 15:54:20  90.25 mins FALSE        (-89.99 29.94) America/Chicago 2023-06-04 09:24:06  2023-06-04 10:54:20 There      2023     6     4 June 4    CDT   \n##  9 ChIJTZPVqVh9JIYRx10_jt-c4LE Exxon                           2939 Grand Point Hwy, Breaux Bridge, LA 70517, USA 2023-06-04 17:54:09 2023-06-04 18:31:35  37.45 mins TRUE         (-91.83 30.32) America/Chicago 2023-06-04 12:54:09  2023-06-04 13:31:35 There      2023     6     4 June 4    CDT   \n## 10 ChIJFWLuzkT3O4YRBX2lqNQDF4w Exxon                           1410 Gum Cove Rd, Vinton, LA 70668, USA            2023-06-04 20:02:26 2023-06-04 20:26:48  24.37 mins TRUE         (-93.57 30.19) America/Chicago 2023-06-04 15:02:26  2023-06-04 15:26:48 There      2023     6     4 June 4    CDT   \n## # ℹ 14 more rows\n\n\n\nactivitySegments\nWe’ll go through the same data loading and cleaning process for the activitySegments. It’s a little more complicated because we have pairs of timestamps and locations (start/end times, start/end locations), and the times and locations can be in different time zones, so we need to do the group_by(tz) trick twice with group_by(tz_start) and group_by(tz_end). We also need to get two geometry columns, which requires a little data trickery, as you’ll see below:\n\nactivity_segments_raw &lt;- read_json(\n  \"data/Semantic Location History/2023/2023_JUNE_truncated.json\", \n  simplifyVector = FALSE\n) %&gt;% \n  # Extract the timelineObjects JSON element\n  pluck(\"timelineObjects\") %&gt;%\n  # Filter the list to only keep activitySegments\n  map(\"activitySegment\") %&gt;%\n  # Discard all the empty elements (i.e. the placeVisits)\n  compact()\n\nactivity_segments_not_clean &lt;- activity_segments_raw %&gt;% \n  # Extract parts of the nested list\n  map(~{\n    tibble(\n      distance_m = .x$distance,\n      activity_type = .x$activityType,\n      start_latitudeE7 = .x$startLocation$latitudeE7 / 1e7,\n      start_longitudeE7 = .x$startLocation$longitudeE7 / 1e7,\n      end_latitudeE7 = .x$endLocation$latitudeE7 / 1e7,\n      end_longitudeE7 = .x$endLocation$longitudeE7 / 1e7,\n      startTimestamp = ymd_hms(.x$duration$startTimestamp, tz = \"UTC\"),\n      endTimestamp = ymd_hms(.x$duration$endTimestamp, tz = \"UTC\")\n    )\n  }) %&gt;% \n  list_rbind()\n\n# ↑ that needs to be a separate data frame so that we can refer to it to make a\n# geometry column for the end latitude/longitude\nactivity_segments &lt;- activity_segments_not_clean %&gt;% \n  # Calculate the duration and distance and speed of the segment\n  mutate(duration = endTimestamp - startTimestamp) %&gt;% \n  mutate(distance_miles = meters_to_miles(distance_m)) %&gt;% \n  mutate(\n    hours = as.numeric(duration) / 60,\n    avg_mph = distance_miles / hours\n  ) %&gt;% \n  # Make two geometry columns\n  st_as_sf(coords = c(\"start_longitudeE7\", \"start_latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  rename(\"geometry_start\" = \"geometry\") %&gt;% \n  mutate(geometry_end = st_geometry(\n    st_as_sf(\n      activity_segments_not_clean, \n      coords = c(\"end_longitudeE7\", \"end_latitudeE7\"), \n      crs = st_crs(\"EPSG:4326\"))\n    )\n  ) %&gt;% \n  select(-end_longitudeE7, -end_latitudeE7) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz_start = tz_lookup(geometry_start, method = \"accurate\")) %&gt;% \n  mutate(tz_end = tz_lookup(geometry_end, method = \"accurate\")) %&gt;% \n  # Create a version of the timestamps in local time, but in UTC\n  group_by(tz_start) %&gt;% \n  mutate(startTimestamp_local = force_tz(with_tz(startTimestamp, tz_start), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  group_by(tz_end) %&gt;% \n  mutate(endTimestamp_local = force_tz(with_tz(endTimestamp, tz_end), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(startTimestamp_local),\n    month = month(startTimestamp_local),\n    day = day(startTimestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(startTimestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  ) %&gt;% \n  # Bring in abbreviated time zones for both the start and end time zones\n  left_join(\n    rename(tz_abbreviations, \"tz_start_abb\" = \"tz_abb\"), \n    by = join_by(tz_start == tz)\n  ) %&gt;% \n  left_join(\n    rename(tz_abbreviations, \"tz_end_abb\" = \"tz_abb\"),\n    by = join_by(tz_end == tz)\n  ) %&gt;% \n  # Create an id column so we can better reference individual activities \n  # Make it a character so it can combine with the place visit id column\n  mutate(id = as.character(1:n()))\nactivity_segments\n## Simple feature collection with 23 features and 19 fields\n## Active geometry column: geometry_start\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -108.7 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 23 × 21\n##    distance_m activity_type        startTimestamp      endTimestamp        duration    distance_miles hours avg_mph geometry_start   geometry_end tz_start        tz_end          startTimestamp_local endTimestamp_local   year month   day day_month tz_start_abb tz_end_abb id   \n##  *      &lt;int&gt; &lt;chr&gt;                &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;               &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;POINT [°]&gt;    &lt;POINT [°]&gt; &lt;chr&gt;           &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;\n##  1     272317 IN_PASSENGER_VEHICLE 2023-06-03 14:50:24 2023-06-03 17:12:34 142.17 mins        169.    2.37    71.4  (-86.34 32.33) (-88.16 30.59) America/Chicago America/Chicago 2023-06-03 09:50:24  2023-06-03 12:12:34  2023     6     3 June 3    CDT          CDT        1    \n##  2     195400 IN_PASSENGER_VEHICLE 2023-06-03 17:59:38 2023-06-03 19:45:22 105.75 mins        121.    1.76    68.9   (-88.16 30.6) (-89.96 30.05) America/Chicago America/Chicago 2023-06-03 12:59:38  2023-06-03 14:45:22  2023     6     3 June 3    CDT          CDT        2    \n##  3      15368 IN_PASSENGER_VEHICLE 2023-06-03 20:52:17 2023-06-03 21:08:30  16.22 mins          9.55  0.270   35.3  (-89.96 30.05) (-90.07 29.96) America/Chicago America/Chicago 2023-06-03 15:52:17  2023-06-03 16:08:30  2023     6     3 June 3    CDT          CDT        3    \n##  4       1026 WALKING              2023-06-03 22:31:08 2023-06-03 22:44:22  13.23 mins          0.638 0.221    2.89 (-90.06 29.96) (-90.07 29.96) America/Chicago America/Chicago 2023-06-03 17:31:08  2023-06-03 17:44:22  2023     6     3 June 3    CDT          CDT        4    \n##  5       1280 WALKING              2023-06-03 22:59:51 2023-06-03 23:10:10  10.32 mins          0.795 0.172    4.63 (-90.07 29.96) (-90.06 29.96) America/Chicago America/Chicago 2023-06-03 17:59:51  2023-06-03 18:10:10  2023     6     3 June 3    CDT          CDT        5    \n##  6      14611 IN_PASSENGER_VEHICLE 2023-06-03 23:19:40 2023-06-03 23:37:50  18.17 mins          9.08  0.303   30.0  (-90.06 29.96) (-89.96 30.05) America/Chicago America/Chicago 2023-06-03 18:19:40  2023-06-03 18:37:50  2023     6     3 June 3    CDT          CDT        6    \n##  7      14258 IN_PASSENGER_VEHICLE 2023-06-04 14:01:02 2023-06-04 14:24:06  23.07 mins          8.86  0.384   23.0  (-89.96 30.05) (-89.99 29.94) America/Chicago America/Chicago 2023-06-04 09:01:02  2023-06-04 09:24:06  2023     6     4 June 4    CDT          CDT        7    \n##  8     199631 IN_PASSENGER_VEHICLE 2023-06-04 15:54:20 2023-06-04 17:54:09 119.80 mins        124.    2.00    62.1  (-89.99 29.94) (-91.83 30.32) America/Chicago America/Chicago 2023-06-04 10:54:20  2023-06-04 12:54:09  2023     6     4 June 4    CDT          CDT        8    \n##  9     169541 IN_PASSENGER_VEHICLE 2023-06-04 18:31:35 2023-06-04 20:02:26  90.83 mins        105.    1.51    69.6  (-91.83 30.32) (-93.57 30.19) America/Chicago America/Chicago 2023-06-04 13:31:35  2023-06-04 15:02:26  2023     6     4 June 4    CDT          CDT        9    \n## 10      71377 IN_PASSENGER_VEHICLE 2023-06-04 20:26:48 2023-06-04 21:06:44  39.93 mins         44.4   0.666   66.6  (-93.57 30.19) (-94.21 29.99) America/Chicago America/Chicago 2023-06-04 15:26:48  2023-06-04 16:06:44  2023     6     4 June 4    CDT          CDT        10   \n## # ℹ 13 more rows\n\n\n\nBoth combined\nAll these placeVisit and activitySegment entries were originally in the same JSON file since they actually fit together nicely—activity segments lead to place visits, which are then followed by more activity segments (i.e. you drive to a place, do stuff at that place, and drive to a different place, and so on). Because the two event types are structured so differently, we had to split them up and load and clean them separately. But it can be helpful to put them back together so there’s a consistent timeline—it’ll help with making plots below, and it creates a neat log of the whole trip.\nWe’ll use bind_rows() to combine the two and then sort. It’s kind of an ugly dataset, with lots of missing data in every other row since each type of entry has slightly different columns in it, but some columns are consistent throughout, like duration and the different timestamps, so it’ll be helpful.\n\nall_stops_activities &lt;- bind_rows(\n  list(visit = place_visits, segment = activity_segments),\n  .id = \"type\"\n) %&gt;% \n  arrange(startTimestamp)\nall_stops_activities\n## Simple feature collection with 47 features and 26 fields\n## Active geometry column: geometry (with 23 geometries empty)\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 47 × 29\n##    type    id             name  address startTimestamp      endTimestamp        duration driving_stop       geometry tz              startTimestamp_local endTimestamp_local  direction  year month   day day_month tz_abb distance_m activity_type  distance_miles  hours avg_mph geometry_start\n##    &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;   &lt;lgl&gt;           &lt;POINT [°]&gt; &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;POINT [°]&gt;\n##  1 visit   ChIJQ0NgzsqAj… Chev… 1030 W… 2023-06-03 14:27:23 2023-06-03 14:50:24  23.02 … TRUE         (-86.34 32.33) America/Chicago 2023-06-03 09:27:23  2023-06-03 09:50:24 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  2 segment 1              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 14:50:24 2023-06-03 17:12:34 142.17 … NA                    EMPTY &lt;NA&gt;            2023-06-03 09:50:24  2023-06-03 12:12:34 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;       272317 IN_PASSENGER_…        169.     2.37    71.4  (-86.34 32.33)\n##  3 visit   ChIJMU2weFNKm… Walm… 5245 R… 2023-06-03 17:12:34 2023-06-03 17:59:38  47.07 … TRUE         (-88.16 30.59) America/Chicago 2023-06-03 12:12:34  2023-06-03 12:59:38 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  4 segment 2              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 17:59:38 2023-06-03 19:45:22 105.75 … NA                    EMPTY &lt;NA&gt;            2023-06-03 12:59:38  2023-06-03 14:45:22 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;       195400 IN_PASSENGER_…        121.     1.76    68.9   (-88.16 30.6)\n##  5 visit   ChIJnwDSTcsDn… Comf… 7051 B… 2023-06-03 19:45:22 2023-06-03 20:52:17  66.90 … FALSE        (-89.96 30.05) America/Chicago 2023-06-03 14:45:22  2023-06-03 15:52:17 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  6 segment 3              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 20:52:17 2023-06-03 21:08:30  16.22 … NA                    EMPTY &lt;NA&gt;            2023-06-03 15:52:17  2023-06-03 16:08:30 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;        15368 IN_PASSENGER_…          9.55   0.270   35.3  (-89.96 30.05)\n##  7 visit   ChIJgcNDAhKmI… P149… 537 Ch… 2023-06-03 21:08:30 2023-06-03 22:31:08  82.64 … FALSE        (-90.06 29.96) America/Chicago 2023-06-03 16:08:30  2023-06-03 17:31:08 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  8 segment 4              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 22:31:08 2023-06-03 22:44:22  13.23 … NA                    EMPTY &lt;NA&gt;            2023-06-03 17:31:08  2023-06-03 17:44:22 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;         1026 WALKING                 0.638  0.221    2.89 (-90.06 29.96)\n##  9 visit   ChIJv30_Xw-mI… Loui… 701 N … 2023-06-03 22:44:22 2023-06-03 22:59:51  15.48 … FALSE        (-90.07 29.96) America/Chicago 2023-06-03 17:44:22  2023-06-03 17:59:51 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n## 10 segment 5              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 22:59:51 2023-06-03 23:10:10  10.32 … NA                    EMPTY &lt;NA&gt;            2023-06-03 17:59:51  2023-06-03 18:10:10 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;         1280 WALKING                 0.795  0.172    4.63 (-90.07 29.96)\n## # ℹ 37 more rows\n## # ℹ 5 more variables: geometry_end &lt;POINT [°]&gt;, tz_start &lt;chr&gt;, tz_end &lt;chr&gt;, tz_start_abb &lt;chr&gt;, tz_end_abb &lt;chr&gt;"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#maps",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#maps",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Maps",
    "text": "Maps\n\n\nCode\n# Find all the states that daily_routes crosses\nstates_crossed_through &lt;- st_intersection(\n  st_transform(lower_48, st_crs(daily_routes)),\n  daily_routes\n)\n## Warning: attribute variables are assumed to be spatially constant throughout all geometries\n\n# Create a column that flags if the state is crossed through\nlower_48_highlighted &lt;- lower_48 %&gt;% \n  mutate(visited = NAME %in% unique(states_crossed_through$NAME))\n\n\nIn total, we crossed through 17 different states over the course of our big circular 13-day drive. We made 76 different stops along the way, all marked with points in Figure 1:\n\n\nCode\n# Make a map!\nggplot() +\n  geom_sf(data = lower_48_highlighted, aes(fill = visited)) +\n  scale_fill_manual(values = c(\"grey98\", \"grey90\"), guide = \"none\") +\n  geom_sf(data = daily_routes, linewidth = 0.75) +\n  geom_sf(data = place_visits, size = 1.5) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  labs(caption = \"Each point is a stop\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip_map()\n\n\n\n\n\nFigure 1: Basic map of our mega road trip\n\n\n\n\nIn Figure 2 we can color each leg of the trip a different color to see how the driving was divided up over time:\n\n\nCode\n# Use the same colors for each direction\ncolors_there &lt;- rcartocolor::carto_pal(7, \"SunsetDark\")[c(1, 3, 2, 4, 6, 5, 7)]\ncolors_back_again &lt;- rcartocolor::carto_pal(6, \"SunsetDark\")[c(1, 3, 2, 4, 6, 5)]\n\n# CARTO's Sunset Dark is neat, but the first yellow color is a little too light\n# to see comfortably, so we replace it with the yellow from the CARTO Prism\n# palette\ncolors_there[1] &lt;- clrs[6]\ncolors_back_again[1] &lt;- clrs[6]\n\n# Make a map!\nggplot() +\n  geom_sf(data = lower_48_highlighted, aes(fill = visited)) +\n  scale_fill_manual(values = c(\"grey98\", \"grey90\"), guide = \"none\") +\n  # Reset fill scale so that the labels can be filled\n  new_scale_fill() +\n  geom_sf(data = daily_routes, aes(color = day_month), linewidth = 0.75) +\n  scale_color_manual(values = c(colors_there, colors_back_again), guide = \"none\") +\n  geom_sf(data = place_visits, aes(color = day_month), size = 1.5) +\n  geom_label_repel(\n    data = daily_routes,\n    aes(label = day_month, fill = day_month, geometry = path),\n    stat = \"sf_coordinates\", seed = 1234,\n    size = 3, segment.color = \"grey30\", min.segment.length = 0,\n    show.legend = FALSE, family = \"Overpass ExtraBold\", color = \"white\"\n  ) +\n  scale_fill_manual(values = c(colors_there, colors_back_again), guide = \"none\") +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  labs(caption = \"Each point is a stop\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip_map()\n\n\n\n\n\nFigure 2: Map of our mega road trip, by day"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#distance-and-time",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#distance-and-time",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Distance and time",
    "text": "Distance and time\nTable 1 answers a bunch of questions about distance and time:\n\nHow many miles did we drive each day? Roughly 500ish miles per day. The way out was longer than the way back (2,832 miles there vs. 2,390 miles back). In total we drove 5,222 miles (!!!). This is shockingly close to the 5,260 miles that OpenStreetMap mapped out for me in this previous blog post, even with some changes to the route (i.e. we didn’t end up going through Minnesota and went through Nebraska instead)\nHow long did we spend driving each day? Roughly 8.5 hours per day. The way out was 5 hours longer than the way back (though that’s a little inflated because June 8 doesn’t really count, as we were just driving around Capitol Reef National Park). In total we drove for 91 hours and 47 minutes,\nWhat was the earliest time we started each day and the latest time we arrived each evening? The earliest departure was 7:30 AM when we left Atlanta on June 3; the latest arrivals were when we got to Sioux Falls, South Dakota at 10:42 PM on June 22 and when we got to Flagstaff, Arizona on June 6 at 10:35 PM.\n\n\n\nCode\ntime_distance_day &lt;- activity_segments %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(activity_type == \"IN_PASSENGER_VEHICLE\") %&gt;% \n  group_by(direction, day_month) %&gt;% \n  summarize(\n    total_distance = sum(distance_miles),\n    total_time = sum(duration),\n    start_time = min(startTimestamp_local),\n    start_tz = tz_start_abb[which.min(startTimestamp_local)],\n    end_time = max(endTimestamp_local),\n    end_tz = tz_end_abb[which.max(endTimestamp_local)]\n  ) %&gt;% \n  mutate(\n    nice_start_time = paste(strftime(start_time, format = \"%I:%M %p\", tz = \"UTC\"), start_tz),\n    nice_end_time = paste(strftime(end_time, format = \"%I:%M %p\", tz = \"UTC\"), end_tz)\n  ) %&gt;% \n  # Remove leading zeros from hour\n  mutate(across(c(nice_start_time, nice_end_time), ~str_replace(.x, \"^0\", \"\")))\n\ntime_distance_day %&gt;% \n  select(direction, day_month, total_distance, total_time, nice_start_time, nice_end_time) %&gt;% \n  group_by(direction) %&gt;%\n  gt() %&gt;% \n  summary_rows(\n    groups = everything(),\n    columns = c(total_distance, total_time),\n    fns = list(Subtotal = ~ sum(.)),\n    fmt = list(\n      ~fmt_number(., columns = total_distance, decimals = 0),\n      ~fmt(., columns = total_time, fns = fmt_difftime)\n    ),\n  ) %&gt;% \n  grand_summary_rows(\n    columns = c(total_distance, total_time),\n    fns = list(Total = ~ sum(.)),\n    fmt = list(\n      ~fmt_number(., columns = total_distance, decimals = 0),\n      ~fmt(., columns = total_time, fns = fmt_difftime)\n    ),\n    missing_text = \"\"\n  ) %&gt;% \n  tab_footnote(\n    footnote = \"This wasn't really a travel day; we spent the day hiking around Capitol Reef National Park and hanging out at my aunt's cabin in Grover, Utah.\",\n    locations = cells_body(\n      columns = day_month,\n      rows = day_month == \"June 8\"\n    )\n  ) %&gt;% \n  cols_label(\n    day_month = \"Day\",\n    total_distance = \"Miles\",\n    total_time = \"Driving time\",\n    nice_start_time = \"Start time\",\n    nice_end_time = \"End time\"\n  ) %&gt;% \n  cols_align(\n    columns = total_time,\n    align = \"left\"\n  ) %&gt;% \n  fmt_number(columns = total_distance, decimals = 0) %&gt;% \n  fmt(columns = total_time, fns = fmt_difftime) %&gt;% \n  tab_style(\n    locations = cells_column_labels(),\n    style = list(\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_row_groups(),\n    style = list(\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = list(cells_summary(), cells_stub_summary()),\n    style = list(\n      cell_fill(color = \"grey95\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = list(\n      cells_grand_summary(), cells_stub_grand_summary(), \n      cells_column_labels(), cells_stubhead()),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey80\")\n    )\n  ) %&gt;% \n  opt_horizontal_padding(scale = 2) %&gt;% \n  opt_table_font(font = \"Jost\") %&gt;% \n  tab_options(\n    row_group.as_column = TRUE,\n    footnotes.marks = \"standard\"\n  )\n\n\n\n\n\n\nTable 1:  Daily distance and time details \n  \n    \n    \n      \n      Day\n      Miles\n      Driving time\n      Start time\n      End time\n    \n  \n  \n    There\nJune 3\n492\n7 hours 26 minutes\n7:34 AM EDT\n6:37 PM CDT\n    June 4\n544\n8 hours 16 minutes\n9:01 AM CDT\n8:30 PM CDT\n    June 5\n442\n6 hours 56 minutes\n9:06 AM CDT\n6:10 PM MDT\n    June 6\n626\n10 hours 5 minutes\n8:24 AM MDT\n10:35 PM MST\n    June 7\n438\n8 hours 31 minutes\n8:40 AM MST\n8:46 PM MDT\n    June 8*\n63\n2 hours 18 minutes\n9:41 AM MDT\n7:42 PM MDT\n    June 9\n228\n4 hours 41 minutes\n9:04 AM MDT\n3:06 PM MDT\n    Subtotal\n\n2,832\n48 hours 14 minutes\n\n\n    Back again\nJune 20\n290\n4 hours 50 minutes\n8:58 AM MDT\n9:17 PM MDT\n    June 21\n408\n11 hours 36 minutes\n8:16 AM MDT\n10:04 PM MDT\n    June 22\n525\n8 hours 27 minutes\n8:14 AM MDT\n10:42 PM CDT\n    June 23\n459\n7 hours 27 minutes\n8:11 AM CDT\n8:22 PM CDT\n    June 24\n446\n7 hours 9 minutes\n9:09 AM CDT\n9:16 PM CDT\n    June 25\n262\n4 hours 1 minutes\n7:49 AM CDT\n12:51 PM EDT\n    Subtotal\n\n2,390\n43 hours 33 minutes\n\n\n    Total\n\n5,222\n91 hours 47 minutes\n\n\n  \n  \n  \n    \n      * This wasn't really a travel day; we spent the day hiking around Capitol Reef National Park and hanging out at my aunt's cabin in Grover, Utah.\n    \n  \n\n\n\n\n\nFigure 3 shows a timeline of each day’s events.1 We typically started each day with some sort of sightseeing adventure (we didn’t do this on the first day, since we left our house that day—instead, we ended that day with a walk through the French Quarter in New Orleans). Our longest touristy adventure was technically Capitol Reef National Park, since we spent the whole day there, but if we don’t count that and instead look at the along-the-way stops, our longest adventures were Yellowstone, Nauvoo, and Carlsbad Caverns.1 All the times are in local time, which causes some weirdness when crossing time zones. Instead of somehow expanding or shrinking these timelines during stretches where we gained or lost time because of time zone changes, I just put a dotted border around those stretches to point them out.\n\n\nCode\n# Little dataset with names for the tourist stops we made\n# Gotta do some goofy things here because I don't want things double labeled though\n#\n# - Some IDs, like ChIJgcNDAhKmIIYRRA4mio_7VgE (a parking lot near the \n#   French Quarter) show up twice as placeVisits, but there should only be one \n#   label, so down below, after merging this into the timeline data, I use \n#   `group_by(visit_label) %&gt;% slice(1)` to only keep the first visit for labeling\n# - Yellowstone, though, looks goofy if the first segment is labeled, so I want \n#   to label the second. That messes with the grouping/slicing approach, so I \n#   add a trailing space to the Yellowstone visits I don't want labeled \n#   (i.e. \"Yellowstone \") and then filter those out after grouping/slicing\n# - Nauvoo happened on two different days, but grouping/slicing only keeps the \n#   first. So I make the second day's visits use a different label with a space \n#   at the beginning and end of the label (i.e. \" Nauvoo \")\nsightseeing &lt;- tribble(\n  ~id,                           ~visit_label,\n  \"ChIJgcNDAhKmIIYRRA4mio_7VgE\", \"The French\\nQuarter\",\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\", \"The French\\nQuarter\",  # Technically Louis Armstrong Park\n  \"ChIJ59n6fW8dnogRWi-5N6olcyU\", \"Chalmette\\nBattlefield\",\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\", \"The Alamo\",\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\", \"Carlsbad\\nCaverns\",\n  \"ChIJrSLbJJIQM4cR4l5HTswDY8k\", \"The Grand\\nCanyon\",\n  \"ChIJU6LnB_8ASocRB_9PSFPsO94\", \"Capitol Reef\\nNational Park\",\n  \"ChIJIw-BhQkZSocRncIWG0YMLJU\", \"Capitol Reef\\nNational Park\",  # Technically the visitor center\n  \"ChIJaSHejn29SYcR-YzTt_DNlTg\", \"Goblin\\nValley\",\n  \"ChIJVQ4oZOP4VFMREjDKbf7bHIE\", \"My sister's\\nhouse\",\n  \"ChIJp4yR8asLVFMRJJExTuHrYEs\", \"Rexburg\",  # Technically Porter Park\n  \"ChIJz6VI3AMLVFMREvSOw9M0VR4\", \"Rexburg\",  # Technically BYU-I\n  \"51\",                          \"Yellowstone\",  # Technically the drive to Old Faithful; Google missed this\n  \"ChIJ3zGqpb65UVMR0rTSaqVZ5kc\", \"Yellowstone \",\n  \"ChIJXy5ZgRvtUVMRoSJoWid8Owg\", \"Yellowstone \",  # Technically Old Faithful\n  \"ChIJOT5U8z8GM1MResed1BOdJKk\", \"Devil's\\nTower\",\n  \"ChIJ39Y-tdg1fYcRQcZcBb499do\", \"Mount\\nRushmore\",\n  \"ChIJg_2MNnKRk4cRQGXbuvgqba4\", \"Winter\\nQuarters\",\n  \"ChIJh53YJHIm54cRmpf8_ZA3CVw\", \"Nauvoo\",  # Technically the visitors center\n  \"ChIJDUPPu3Im54cRKj6BG8UkOko\", \"Nauvoo\",  # Technically the temple\n  \"ChIJg0abVHYm54cR85yQbfLjt2o\", \" Nauvoo \",  # Technically the family living center\n  \"ChIJm7cRetkl54cR-lEKk-eZnXA\", \" Nauvoo \",  # Technically the Smith family cemetery\n  \"ChIJZ6tHUwsm54cRbmWsF639PjY\", \" Nauvoo \"   # Technically Carthage Jail\n)\n\n\ntimeline_data &lt;- all_stops_activities %&gt;% \n  # Get rid of the geometry column since we're not doing anything map-related\n  st_drop_geometry() %&gt;% \n  # Indicate if there was a time zone change during the activity\n  mutate(\n    tz_change = ifelse(is.na(tz_start) | tz_start_abb == tz_end_abb, NA, \"Time zone change\")\n  ) %&gt;% \n  # Find the midpoint between the start and end times\n  mutate(time_mid = startTimestamp_local - ((startTimestamp_local - endTimestamp_local) / 2)) %&gt;% \n  # Add column for the name of the tourist stop\n  left_join(sightseeing, by = join_by(id)) %&gt;% \n  # Add column for more detailed type of stop\n  mutate(\n    stop_type = case_when(\n      type == \"visit\" & !is.na(visit_label) ~ \"Sightseeing\",\n      type == \"visit\" & is.na(visit_label) ~ \"Gas/bathroom/hotel stop\",\n      type == \"segment\" & id == \"51\" ~ \"Sightseeing\",\n      type == \"segment\" ~ \"Traveling\"\n    )\n  ) %&gt;%\n  # After finishing at Capitol Reef we returned to my aunt's cabin in Grover to\n  # relax. I had to run back to the nearest town with 5G internet (Torrey) to\n  # respond to e-mails and Slack messages (since I was teaching two online\n  # classes throughout this whole road trip!), and that doesn't technically\n  # count as part of the road trip, so we need to remove everything after 5 PM\n  # on that day\n  filter(!(day == 8 & hour(endTimestamp_local) &gt; 17)) %&gt;% \n  # After arriving at the hotel in Rexburg, we made a late-night milkshake run\n  # after the kids were in bed, which Google picked up. That doesn't technically\n  # count as part of the road trip, so we need to remove everything after 6 PM\n  # on that day\n  filter(!(day == 20 & hour(endTimestamp_local) &gt; 18)) %&gt;% \n  # Set the timestamps for the final stop of the day (i.e. hotels) to NA and\n  # then remove those rows for plotting\n  group_by(day) %&gt;% \n  mutate(\n    across(c(startTimestamp_local, endTimestamp_local), \n      ~if_else(row_number() == n(), NA, .x))\n  ) %&gt;% \n  drop_na(startTimestamp_local)\n\n# Force all the timestamps to be on the same day for the sake of plotting\nday(timeline_data$startTimestamp_local) &lt;- 1\nday(timeline_data$endTimestamp_local) &lt;- 1\nday(timeline_data$time_mid) &lt;- 1\n\n# Extract just the labels for plotting\nsightseeing_labels &lt;- timeline_data %&gt;% \n  filter(stop_type == \"Sightseeing\") %&gt;% \n  group_by(visit_label) %&gt;% \n  slice(1) %&gt;% \n  filter(visit_label != \"Yellowstone \")\n\n# Timeline plot\nggplot(timeline_data) +\n  geom_rect(\n    aes(\n      xmin = startTimestamp_local, xmax = endTimestamp_local, \n      ymin = 0, ymax = 1, \n      fill = stop_type, color = tz_change, linetype = tz_change, linewidth = tz_change\n    )\n  ) +\n  geom_label(\n    data = sightseeing_labels,\n    aes(x = time_mid, y = 0.5, label = visit_label),\n    fill = colorspace::lighten(clrs[8], 0.1), color = \"white\", lineheight = 1,\n    inherit.aes = FALSE\n  ) +\n  # Phew fancy aesthetic and scale work here\n  scale_x_datetime(date_breaks = \"3 hours\", date_labels = \"%I:%M %p\") +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_fill_manual(values = c(clrs[6], clrs[8], clrs[2]), guide = guide_legend(order = 1)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  scale_linewidth_discrete(range = c(0.5, 0)) +\n  scale_linetype_manual(\n    values = c(\"21\"), breaks = \"Time zone change\",\n    guide = guide_legend(override.aes = list(fill = \"white\", color = \"black\", linewidth = 0.5), order = 10)\n  ) +\n  guides(color = \"none\", linewidth = \"none\") +\n  facet_wrap(vars(day_month), ncol = 2, dir = \"v\") +\n  labs(x = NULL, y = NULL, linetype = NULL, fill = NULL) +\n  theme_roadtrip() +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\nFigure 3: Timeline of our grand road trip"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#driving-and-breaks",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#driving-and-breaks",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Driving and breaks",
    "text": "Driving and breaks\nAs seen in Figure 3, we had a pretty good rhythm of driving and taking gas/bathroom breaks—we broke up long stretches of driving with stops for gas and bathrooms.\nTable 2 shows the top three longest and shortest stretches of driving. The longest stretch was on the final day—we drove 4 hours from Nashville to Atlanta with no bathroom breaks. We apparently just desperately wanted to get home! We did the same push-through-to-the-last-stop approach a few other times too. The third longest stretch was actually our first on the way home, leaving Spanish Fork, Utah to my sister’s house outside of Rexburg, Idaho, which we did in 3.5 hours, while the 2nd and 5th longest stretches involved pushing to our final destinations for the day (a hotel in Nauvoo, Illinois and my aunt’s cabin in Grover, Utah). The only long stretch that broke this patter was the fourth longest one after Carlsbad Caverns—we spent more than 3 hours on a little state road going all the way from southern New Mexico to a gas station at I-20.\nThe shortest stretches involved going from a main sightseeing stop to a gas station (Mount Rushmore to some Holiday gas station), going between sightseeing stops (the Smith Family Cemetery in Nauvoo to Carthage Jail), or going from a hotel to a sightseeing stop (our hotel in Carlsbad to Carlsbad Caverns National Park).\nMy favorite short stop is number 4 here, on June 4. We got gas in Louisiana somewhere and then had to stop at another gas station in Texas 39 minutes later for an emergency break for little kids who suddenly needed to use the bathroom again. After that particular stop, we instituted a “no drinking soda or juice in the car” rule for the kids and suddenly—like magic—their bladders worked a lot more slowly and we stopped making such frequent stops.\n\n\nCode\ndriving_data &lt;- all_stops_activities %&gt;% \n  # Get rid of the geometry column since we're not doing anything map-related\n  st_drop_geometry() %&gt;% \n  # Find the previous and next place visits\n  mutate(\n    origin_name = lag(name),\n    origin_address = lag(address),\n    destination_name = lead(name),\n    destination_address = lead(address)\n  ) %&gt;% \n  # lag() doesn't work for the first entry, so manually add it\n  mutate(\n    origin_name = ifelse(id == 1, \"Home\", origin_name),\n    origin_address = ifelse(id == 1, \"Atlanta, GA, USA\", origin_address)\n  ) %&gt;% \n  # Only look at driving activities\n  filter(activity_type == \"IN_PASSENGER_VEHICLE\") %&gt;% \n  # Get rid of empty, place-related columns\n  select(-c(name, address, driving_stop, tz, geometry_start, geometry_end))\n\n# Longest, shortest stretch of driving\ndriving_data %&gt;% \n  filter(distance_miles &gt; 10) %&gt;%\n  mutate(duration_rank = rank(-duration)) %&gt;% \n  mutate(top_bottom = case_when(\n    duration_rank %in% 1:5 ~ \"Longest stretches\",\n    duration_rank %in% seq(n() - 4, n(), by = 1) ~ \"Shortest stretches\"\n  )) %&gt;% \n  filter(!is.na(top_bottom)) %&gt;% \n  arrange(\n    top_bottom, \n    ifelse(top_bottom == \"Shortest stretches\", desc(duration_rank), duration_rank)\n  ) %&gt;% \n  # Format time stuff nicely\n  mutate(\n    nice_time = fmt_difftime(duration),\n    nice_start_time = paste(strftime(startTimestamp_local, format = \"%I:%M %p\", tz = \"UTC\"), tz_start_abb),\n    nice_end_time = paste(strftime(endTimestamp_local, format = \"%I:%M %p\", tz = \"UTC\"), tz_end_abb)\n  ) %&gt;% \n  # Remove leading zeros from hour\n  mutate(across(c(nice_start_time, nice_end_time), ~str_replace(.x, \"^0\", \"\"))) %&gt;% \n  # Make nice origin and destination columns\n  mutate(\n    origin_nice = glue('{origin_name}&lt;br&gt;&lt;span class=\"smaller-address\"&gt;{origin_address}&lt;/span&gt;'),\n    destination_nice = glue('{destination_name}&lt;br&gt;&lt;span class=\"smaller-address\"&gt;{destination_address}&lt;/span&gt;')\n  ) %&gt;% \n  select(\n    top_bottom, day_month, nice_time, nice_start_time, nice_end_time, \n    distance_miles, origin_nice, destination_nice\n  ) %&gt;% \n  group_by(top_bottom) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    nice_time = \"Driving time\",\n    day_month = \"Day\",\n    nice_start_time = \"Start time\",\n    nice_end_time = \"End time\",\n    distance_miles = \"Miles\",\n    origin_nice = \"Origin\",\n    destination_nice = \"Destination\"\n  ) %&gt;% \n  tab_style(\n    locations = list(cells_column_labels(), cells_stubhead()),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey80\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_row_groups(),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey95\")\n    )\n  ) %&gt;% \n  cols_align(\n    columns = c(day_month, origin_nice, destination_nice),\n    align = \"left\"\n  ) %&gt;% \n  fmt_markdown(columns = c(origin_nice, destination_nice)) %&gt;% \n  fmt_number(columns = distance_miles, decimals = 0) %&gt;% \n  tab_footnote(\n    footnote = \"Stretches less than 10 miles omitted (like if we had lunch somewhere and drove down the street to get gas).\"\n  ) %&gt;% \n  opt_css(\n    css = \"\n    .smaller-address {\n      font-size: 0.7em;\n    }\"\n  ) %&gt;% \n  opt_table_font(font = \"Jost\")\n\n\n\n\n\n\nTable 2:  Top five longest and shortest stretches of driving during the\ntrip \n  \n    \n    \n      Day\n      Driving time\n      Start time\n      End time\n      Miles\n      Origin\n      Destination\n    \n  \n  \n    \n      Longest stretches\n    \n    June 25\n4 hours 1 minutes\n7:49 AM CDT\n12:51 PM EDT\n262\nQuality Inn2741 York Rd, Pleasant View, TN 37146, USA\n\nHomeAtlanta, GA, USA\n\n    June 23\n3 hours 35 minutes\n1:19 PM CDT\n4:54 PM CDT\n198\nMcDonald’s120 SE 7th St, Stuart, IA 50250, USA\n\nInn at Old Nauvoo1875 Mulholland St, Nauvoo, IL 62354, USA\n\n    June 20\n3 hours 31 minutes\n8:58 AM MDT\n12:30 PM MDT\n244\nMy aunt’s houseSpanish Fork, UT, USA\n\nMy sister’s houseShelley, ID, USA\n\n    June 6\n3 hours 9 minutes\n12:18 PM MDT\n3:28 PM MDT\n191\nCarlsbad Caverns National ParkCarlsbad, NM 88220, USA\n\nConoco1307 8th St, Vaughn, NM 88353, USA\n\n    June 7\n3 hours 7 minutes\n5:39 PM MDT\n8:46 PM MDT\n163\nGlazier’s Market264 S 100 E, Kanab, UT 84741, USA\n\nMy aunt’s cabinGrover, UT, USA\n\n    \n      Shortest stretches\n    \n    June 22\n21 minutes\n3:35 PM MDT\n3:56 PM MDT\n15\nMount Rushmore National Memorial13000 SD-244, Keystone, SD 57751, USA\n\nHoliday StationstoresCaregiver Cir, Rapid City, SD 57701, USA\n\n    June 24\n25 minutes\n12:20 PM CDT\n12:45 PM CDT\n18\nSmith Family CemeteryNauvoo, IL 62354, USA\n\nCarthage Jail310 Buchanan St, Carthage, IL 62321, USA\n\n    June 6\n33 minutes\n8:24 AM MDT\n8:57 AM MDT\n22\nStevens Inn1829 S Canal St, Carlsbad, NM 88220, USA\n\nCarlsbad Caverns National ParkCarlsbad, NM 88220, USA\n\n    June 4\n39 minutes\n3:26 PM CDT\n4:06 PM CDT\n44\nExxon1410 Gum Cove Rd, Vinton, LA 70668, USA\n\nLove’s Travel Stop7495 Smith Rd, Beaumont, TX 77713, USA\n\n    June 20\n42 minutes\n2:14 PM MDT\n2:56 PM MDT\n35\nMy sister’s houseShelley, ID, USA\n\nAmericInn by Wyndham Rexburg BYUI1098 Golden Beauty Dr, Rexburg, ID 83440, USA\n\n  \n  \n  \n    \n       Stretches less than 10 miles omitted (like if we had lunch somewhere and drove down the street to get gas).\n    \n  \n\n\n\n\n\nWhat about the stops? How long was a typical gas/bathroom break? Figure 4 shows the distribution of the duration of all the non-touristy stops, and it actually reveals a pretty neat trimodal distribution! We had three general types of stops:\n\nFast (10ish minutes): stops for switching drivers, getting gas right after a sightseeing visit, or letting one kid go to the bathroom\nStandard (25ish minutes): stops where we got gas and had everyone go to the bathroom\nMeal (40ish minutes): stops where we got gas, went to the bathroom, ate food (typically peanut butter sandwiches that we made in the hotel in the morning), and walked around to stretch\n\n\n\nCode\n# Cut duration into three categories\nplaces_hist_data &lt;- place_visits %&gt;% \n  filter(driving_stop == TRUE) %&gt;% \n  mutate(range = cut(as.numeric(duration), breaks = c(0, 15, 35, 50)))\n\nggplot(places_hist_data, aes(x = as.numeric(duration), fill = range)) +\n  geom_histogram(binwidth = 5, boundary = 0, color = \"white\") +\n  annotate(geom = \"text\", x = 10, y = 7.75, label = \"Fast stop\") +\n  annotate(geom = \"segment\", x = 6, xend = 14, y = 7.35, yend = 7.35) +\n  annotate(geom = \"text\", x = 25, y = 7.75, label = \"Standard gas + bathroom\") +\n  annotate(geom = \"segment\", x = 16, xend = 34, y = 7.35, yend = 7.35) +\n  annotate(geom = \"text\", x = 42.5, y = 7.75, label = \"Gas + bathroom + meal\") +\n  annotate(geom = \"segment\", x = 36, xend = 49, y = 7.35, yend = 7.35) +\n  scale_y_continuous(breaks = seq(0, 6, by = 2)) +\n  scale_fill_manual(values = clrs[c(7, 2, 1)], guide = \"none\") +\n  labs(x = \"Minutes\", y = \"Count\") +\n  theme_roadtrip() +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\nFigure 4: Distribution of non-sightseeing stops for the first four days of the trip"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#elevation-over-time",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#elevation-over-time",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Elevation over time",
    "text": "Elevation over time\nAnd finally, just for fun since Google tracked this data and we might as well do something neat with it, we can look at how much the elevation changed over the course of the trip. Atlanta is 1,000 feet above sea level, and Utah County is 4,500 feed above sea level, but the 3,500-foot increase in elevation was hardly uniform. We dropped down to below sea level in New Orleans, climbed up to above 4,000 feet in Carlsbad (it turns out that Carlsbad Caverns is on top of a big mountain), and then climbed rapidly up to the Grand Canyon and Southern Utah in general at 7,500ish feet until dropping down to 4,500 feet at our final destination in Utah.\nOn the way back, we stayed at around 4,500ish feet until Yellowstone, where we hit the highest elevation of the whole trip at 8,530 feet. We then gradually worked our way down toward sea level as we traveled further east, finally hitting 1,000ish feet at the Mississippi River at Nauvoo, Illinois, where we mostly stayed until getting home (with a little Appalachian spike near the end).\n\n\nCode\nelevation_data &lt;- all_locations %&gt;% \n  mutate(elevation = meters_to_feet(altitude))\n\nelevation_stops &lt;- tribble(\n  ~id,                       ~direction,  ~nudge_direction, ~stop_label,\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\", \"There\", \"up\", \"New Orleans,\\nLouisiana\",\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\", \"There\", \"down\", \"San Antonio,\\nTexas\",\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\", \"There\", \"down\", \"Carlsbad Caverns,\\nNew Mexico\",\n  \"ChIJrSLbJJIQM4cR4l5HTswDY8k\", \"There\", \"up\", \"The Grand Canyon,\\nArizona\",\n  \"ChIJU6LnB_8ASocRB_9PSFPsO94\", \"There\", \"up\", \"Capitol Reef,\\nUtah\",\n  \"ChIJz6VI3AMLVFMREvSOw9M0VR4\", \"Back again\", \"down\", \"Rexburg,\\nIdaho\",\n  \"ChIJSWHsxv8JTlMR82z8b6wF_BM\", \"Back again\", \"up\", \"Yellowstone,\\nWyoming\",\n  \"ChIJ39Y-tdg1fYcRQcZcBb499do\", \"Back again\", \"up\", \"Mount Rushmore,\\nSouth Dakota\",\n  \"ChIJDUPPu3Im54cRKj6BG8UkOko\", \"Back again\", \"down\", \"Nauvoo,\\nIllinois\",\n  \"ChIJtUcJ-n36ZIgRhzY2PM19eWA\", \"Back again\", \"up\", \"Nashville,\\nTennessee\"\n)\n\nstops_to_show &lt;- place_visits %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(id %in% elevation_stops$id) %&gt;% \n  left_join(elevation_stops, by = join_by(id)) %&gt;% \n  # Magical new join_by(closest(...)) syntax for inexact, approximate matching\n  # to get the closest elevation for the stop(!)\n  left_join(elevation_data, by = join_by(closest(startTimestamp_local &gt;= timestamp))) %&gt;% \n  # Get rid of duplicate ids\n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\nggplot(elevation_data, aes(x = timestamp_local, y = elevation)) +\n  geom_line(linewidth = 0.3) +\n  geom_text_repel(\n    data = filter(stops_to_show, nudge_direction == \"up\"),\n    aes(x = startTimestamp_local, label = stop_label),\n    nudge_y = 2500, direction = \"y\", lineheight = 1, family = \"Overpass ExtraBold\",\n    color = clrs[8], segment.color = clrs[7], seed = 1234\n  ) +\n  geom_text_repel(\n    data = filter(stops_to_show, nudge_direction == \"down\"),\n    aes(x = startTimestamp_local, label = stop_label),\n    nudge_y = -1000, lineheight = 1, family = \"Overpass ExtraBold\",\n    color = clrs[8], segment.color = clrs[7], seed = 1234\n  ) +\n  scale_x_datetime(date_breaks = \"1 day\", date_labels = \"%B %e\") +\n  scale_y_continuous(\n    breaks = seq(0, 8000, by = 2000),\n    labels = label_comma(suffix = \" ft.\")\n  ) +\n  labs(x = NULL, y = \"Elevation\") +\n  facet_wrap(vars(direction), scales = \"free_x\") +\n  theme_roadtrip()\n\n\n\n\n\nFigure 5: Elevation over the course of the trip"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "",
    "text": "I’ve been working on converting a couple of my dissertation chapters into standalone articles, so I’ve been revisiting and improving my older R code. As part of my dissertation work, I ran a global survey of international NGOs to see how they adjust their programs and strategies when working under authoritarian legal restrictions in dictatorship. In the survey, I asked a bunch of questions with categorical responses (like “How would you characterize your organization’s relationship with your host-country government?”, with the possible responses “Positive,” “Negative”, and “Neither”).\nAnalyzing this kind of categorical data can be tricky. What I ended up doing (and what people typically do) is looking at the differences in proportions of responses (i.e. is there a substantial difference in the proportion reporting “Positive” or “Negative” in different subgroups?). But doing this requires a little bit of extra work because of how proportions work. We can’t just treat proportions like continuous values. Denominators matter and help determine the level of uncertainty in the proportions. We need to account for these disparate sample sizes underlying the proportions.\nWe need to treat these kinds of survey questions as the categories that they are, which requires a different set of analytical tools than just findings averages and running linear regressions. We instead need to use things like frequencies, proportions, contingency tables and crosstabs, and fancier regression like ordered logistic models.\nI’m a fan of Bayesian statistical inference—I find it way more intuitive and straightforward than frequentist null hypothesis testing. I first “converted” to Bayesianism back when I first analyzed my dissertation data in 2017 and used the newly-invented {rstanarm} package to calculate the difference in proportions of my various survey responses in Bayesian ways, based on a blog post and package for Bayesian proportion tests by Rasmus Bååth. He used JAGS, though, and I prefer to use Stan, hence my use of {rstanarm} back then.\nSince 2017, though, I’ve learned a lot more about Bayesianism. I’ve worked through both Richard McElreath’s Statistical Rethinking and the gloriously accessible Bayes Rules!, and I no longer use {rstanarm}. I instead use {brms} for everything (or raw Stan if I’m feeling extra fancy).\nSo, as a reference for myself while rewriting these chapters, and as a way to consolidate everything I’ve learned about Bayesian-flavored proportion tests, here’s a guide to thinking about differences in proportions in a principled Bayesian way. I explore two different questions (explained in detail below). For the first one I’ll be super pedagogical and long-winded, showing how to find differences in proportions with classical frequentist statistical tests, with different variations of Stan code, and with different variations of {brms} code. For the second one I’ll be less pedagogical and just show the code and results."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Estimand",
    "text": "Estimand\nFor our first question, we want to know if there’s a substantial difference in the proportion of students who read comic books often in the United States and Mexico, or whether the difference between the ■two yellow cells is greater than zero:\n\n\nCode\nfancy_table %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#CD8A39\", 0.7)),\n      cell_text(color = \"#CD8A39\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"Often\", rows = c(1, 3))\n    )\n  )\n\n\n\n\n\n\n  \n    \n    \n      \n      Rarely\n      Sometimes\n      Often\n    \n  \n  \n    \n      Mexico\n    \n    Comic books\n\n26.29%(9,897)\n\n45.53%(17,139)\n\n28.17%(10,605)\n\n    Newspapers\n\n18.02%(6,812)\n\n32.73%(12,369)\n\n49.25%(18,613)\n\n    \n      United States\n    \n    Comic books\n\n62.95%(3,237)\n\n26.86%(1,381)\n\n10.19%(524)\n\n    Newspapers\n\n25.27%(1,307)\n\n37.22%(1,925)\n\n37.51%(1,940)\n\n  \n  \n  \n\n\n\n\nTo be more formal about this, we’ll call this estimand \\(\\theta\\), which is the difference between the two proportions \\(\\pi\\):\n\\[\n\\theta = \\pi_\\text{Mexico, comic books, often} - \\pi_\\text{United States, comic books, often}\n\\]\nWhen visualizing this, we’ll use colors to help communicate the idea of between-group differences. We’ll get fancier with the colors in question 2, where we’ll look at three sets of pairwise differences, but here we’re just looking at a single pairwise difference (the difference between the US and Mexico), so we’ll use a bit of subtle and not-quite-correct color theory. In kindergarten we all learned the RYB color model, where primary pigments can be mixed to create secondary pigments. For instance\n\n■blue + ■yellow = ■green.\n\nIf we do some (wrong color-theory-wise) algebra, we can rearrange the formula so that\n\n■blue − ■green = ■yellow\n\nIf we make the United States ■blue and Mexico ■green, the ostensible color for their difference is ■yellow. This is TOTALLY WRONG and a lot more complicated according to actual color theory, but it’s a cute and subtle visual cue, so we’ll use it.\nThese primary colors are a little too bright for my taste though, so let’s artsty them up a bit. We’re looking at data about the US and Mexico, so we’ll use the Saguaro palette from the {NatParksPalettes} package, since Saguaro National Park is near the US-Mexico border and it has a nice blue, yellow, and green.\nWe’ll use ■yellow for the difference (θ) between the ■United States and ■Mexico.\n\n\n\n“Saguaro Fruit Ripens in June” by the US National Park Service\n\n\nCode\n# US/Mexico question colors\nclrs_saguaro &lt;- NatParksPalettes::natparks.pals(\"Saguaro\")\nclr_usa &lt;- clrs_saguaro[1]\nclr_mex &lt;- clrs_saguaro[6]\nclr_diff &lt;- clrs_saguaro[4]"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Classically",
    "text": "Classically\nIn classical frequentist statistics, there are lots of ways to test for the significance of a difference in proportions of counts, rates, proportions, and other categorical-related things, like chi-squared tests, Fisher’s exact test, or proportion tests. Each of these tests have special corresponding “flavors” that apply to specific conditions within the data being tested or the estimand being calculated (corrections for sample sizes, etc.). In standard stats classes, you memorize big flowcharts of possible statistical operations and select the correct one for the situation.\nSince we want to test the difference between two group proportions, we’ll use R’s prop.test(), which tests the null hypothesis that the proportions in some number of groups are the same:\n\\[\nH_0: \\theta = 0\n\\]\nOur job with null hypothesis significance testing is to calculate a test statistic (\\(\\chi^2\\) in this case) for \\(\\theta\\), determine the probability of seeing that statistic in a world where \\(\\theta\\) is actually 0, and infer whether the value we see could plausibly fit in a world where the null hypothesis is true.\nWe need to feed prop.test() either a matrix with a column of counts of successes (students who read comic books often) and failures (students who do not read comic books often) or two separate vectors: one of counts of successes (students who read comic books often) and one of counts of totals (all students). We’ll do it both ways for fun.\nFirst we’ll make a matrix of the counts of students from Mexico and the United States, with columns for the counts of those who read often and of those who don’t read often.\n\n\nCode\noften_matrix &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\") %&gt;% \n  mutate(n_not_often = total - n) %&gt;% \n  select(n_often = n, n_not_often) %&gt;% \n  as.matrix()\noften_matrix\n##      n_often n_not_often\n## [1,]   10605       27036\n## [2,]     524        4618\n\n\nNow we can feed that to prop.test():\n\n\nCode\nprop.test(often_matrix)\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  often_matrix\n## X-squared = 759, df = 1, p-value &lt;2e-16\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.170 0.189\n## sample estimates:\n## prop 1 prop 2 \n##  0.282  0.102\n\n\nThis gives us some helpful information. The “flavor” of the formal test is a “2-sample test for equality of proportions with continuity correction”, which is fine, I guess.\nWe have proportions that are the same as what we have in the highlighted cells in the contingency table (28.17% / 10.19%) and we have 95% confidence interval for the difference. Oddly, R doesn’t show the actual difference in these results, but we can see that difference if we use model_parameters() from the {parameters} package (which is the apparent successor to broom::tidy()?). Here we can see that the difference in proportions is 18 percentage points:\n\n\nCode\nprop.test(often_matrix) %&gt;% \n  model_parameters()\n## 2-sample test for equality of proportions\n## \n## Proportion      | Difference |       95% CI | Chi2(1) |      p\n## --------------------------------------------------------------\n## 28.17% / 10.19% |     17.98% | [0.17, 0.19] |  759.26 | &lt; .001\n## \n## Alternative hypothesis: two.sided\n\n\nAnd finally we have a test statistic: a \\(\\chi^2\\) value of 759.265, which is huge and definitely statistically significant. In a hypothetical world where there’s no difference in the proportions, the probability of seeing a difference of at least 18 percentage points is super tiny (p &lt; 0.001). We have enough evidence to confidently reject the null hypothesis and declare that the proportions of the groups are not the same. With the confidence interval, we can say that we are 95% confident that the interval 0.17–0.189 captures the true population parameter. We can’t say that there’s a 95% chance that the true value falls in this range—we can only talk about the range.\nWe can also do this without using a matrix by feeding prop.test() two vectors: one with counts of people who read comics often and one with counts of total respondents:\n\n\nCode\noften_values &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n\n(n_often &lt;- as.numeric(c(often_values[1, 4], often_values[2, 4])))\n## [1] 10605   524\n(n_respondents &lt;- as.numeric(c(often_values[1, 5], often_values[2, 5])))\n## [1] 37641  5142\n\nprop.test(n_often, n_respondents)\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  n_often out of n_respondents\n## X-squared = 759, df = 1, p-value &lt;2e-16\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.170 0.189\n## sample estimates:\n## prop 1 prop 2 \n##  0.282  0.102\n\n\nThe results are the same."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-stan",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-stan",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with Stan",
    "text": "Bayesianly with Stan\n\nWhy Bayesian modeling?\nI don’t like null hypotheses and I don’t like flowcharts.\n\n\n\n“Golem” by smokeghost\nRegular classical statistics classes teach about null hypotheses and flowcharts, but there’s a better way. In his magisterial Statistical Rethinking, Richard McElreath describes how in legends people created mythological clay robots called golems that could protect cities from attacks, but that could also spin out of control and wreak all sorts of havoc. McElreaths uses the idea of golems as a metaphor for classical statistical models focused on null hypothesis significance testing, which also consist of powerful quasi-alchemical procedures that have to be followed precisely with specific flowcharts:\n\n\n\n\n\nStatistical models as golems (slide 9 in Statistical Rethinking 2022 Lecture 01)\n\n\n\n\nMcElreath argues that this golem-based approach to statistics is incredibly limiting, since (1) you have to choose the right test, and if it doesn’t exist, you have to wait for some fancy statistician to invent it, and (2) you have to focus on rejecting null hypotheses instead of exploring research hypotheses.\nFor instance, in the null hypothesis framework section above, this was the actual question undergirding the analysis:\n\nIn a hypothetical world where \\(\\theta = 0\\) (or where there’s no difference between the proportions) what’s the probability that this one-time collection of data fits in that world—and if the probability is low, is there enough evidence to confidently reject that hypothetical world of no difference?\n\noof. We set our prop.test() golem to work and got a p-value for the probability of seeing the 18 percentage point difference in a world where there’s actually no difference. That p-value was low, so we confidently declared \\(\\theta\\) to be statistically significant and not zero. But that was it. We rejected the null world. Yay. But that doesn’t say much about our main research hypothesis. Boo.\nOur actual main question is far simpler:\n\nGiven the data here, what’s the probability that there’s no difference between the proportions, or that \\(\\theta \\neq 0\\)?\n\nBayesian-flavored statistics lets us answer this question and avoid null hypotheses and convoluted inference. Instead of calculating the probability of seeing some data given a null hypothesis (\\(P(\\text{Data} \\mid H_0)\\)), we can use Bayesian inference to calculate the probability of a hypothesis given some data (\\(P(\\theta \\neq 0 \\mid \\text{Data})\\)).\n\n\nFormal model\nSo instead of thinking about a specific statistical golem, we can think about modeling the data-generating process that could create the counts and proportions that we see in the PISA data.\nRemember that our data looks like this, with n showing a count of the people who read comic books often in each country and total showing a count of the people who responded to the question.\n\n\nCode\nreading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n## # A tibble: 2 × 6\n##   country       book_type   frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Mexico        Comic books Often     10605 37641 0.282\n## 2 United States Comic books Often       524  5142 0.102\n\n\nThe actual process for generating that n involved asking thousands of individual, independent people if they read comic books often. If someone says yes, it’s counted as a “success” (or marked as “often”); if they say no, it’s not marked as “often”. It’s a binary choice repeated across thousands of independent questions, or “trials.” There’s also an underlying overall probability for reporting “often,” which corresponds to the proportion of people selecting it.\nThe official statistical term for this kind of data-generating processes (a bunch of independent trials with some probability of success) is a binomial distribution, and it’s defined like this, with three parameters:\n\\[\ny \\sim \\operatorname{Binomial}(n, \\pi)\n\\]\n\nNumber of successes (\\(y\\): the number of people responding yes to “often,” or n in our data\nProbability of success (\\(\\pi\\)): the probability someone says yes to “often,” or the thing we want to model for each country\nNumber of trials (\\(n\\)): the total number of people asked the question, or total in our data\n\nThe number of successes and trials are just integers—they’re counts—and we already know those, since they’re in the data. The probability of success \\(\\pi\\) is a percentage and ranges somewhere between 0 and 1. We don’t know this value, but we can estimate it with Bayesian methods by defining a prior and a likelihood, churning through a bunch of Monte Carlo Markov Chain (MCMC) simulations, and finding a posterior distribution of \\(\\pi\\).\nWe can use a Beta distribution to model \\(\\pi\\), since Beta distributions are naturally bound between 0 and 1 and they work well for probability-scale things. Beta distributions are defined by two parameters: (1) \\(\\alpha\\) or \\(a\\) or shape1 in R and (2) \\(\\beta\\) or \\(b\\) or shape2 in R. See this section of my blog post on zero-inflated Beta regression for way more details about how these parameters work and what they mean.\nSuper quickly, we’re interested in the probability of a “success” (or where “often” is yes), which is the number of “often”s divided by the total number of responses:\n\\[\n\\frac{\\text{Number of successes}}{\\text{Number of trials}}\n\\]\nor\n\\[\n\\frac{\\text{Number of often = yes}}{\\text{Number of responses}}\n\\]\nWe can separate that denominator into two parts:\n\\[\n\\frac{\\text{Number of often = yes}}{(\\text{Number of often = yes}) + (\\text{Number of often } \\neq \\text{yes})}\n\\]\nThe \\(\\alpha\\) and \\(\\beta\\) parameters correspond to the counts of successes and failures::\n\\[\n\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nWith these two shape parameters, we can create any percentage or fraction we want. We can also control the uncertainty of the distribution by tinkering with the scale of the parameters. For instance, if we think there’s a 40% chance of something happening, this could be represented with \\(\\alpha = 4\\) and \\(\\beta = 6\\), since \\(\\frac{4}{4 + 6} = 0.4\\). We could also write it as \\(\\alpha = 40\\) and \\(\\beta = 60\\), since \\(\\frac{40}{40 + 60} = 0.4\\) too. Both are centered at 40%, but Beta(40, 60) is a lot narrower and less uncertain.\n\n\nCode\nggplot() +\n  stat_function(fun = ~dbeta(., 4, 6), geom = \"area\",\n                aes(fill = \"Beta(4, 6)\"), alpha = 0.5) +\n  stat_function(fun = ~dbeta(., 40, 60), geom = \"area\",\n                aes(fill = \"Beta(40, 60)\"), alpha = 0.5) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(\"Beta(4, 6)\" = \"#FFDC00\",\n                               \"Beta(40, 60)\" = \"#F012BE\")) +\n  labs(x = \"π\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        legend.position = \"top\")\n\n\n\n\n\nBeta(4, 6) and Beta(40, 60) distributions\n\n\n\n\nWe’ve already seen the data and know that the proportion of students who read comic books often is 10ish% in the United States and 30ish% in Mexico, but we’ll cheat and say that we think that around 25% of students read comic books often, ± some big amount. This implies something like a Beta(2, 6) distribution (since \\(\\frac{2}{2+6} = 0.25\\)), with lots of low possible values, but not a lot of \\(\\pi\\)s are above 50%. We could narrow this down by scaling up the parameters (like Beta(20, 60) or Beta(10, 40), etc.), but leaving the prior distribution of \\(\\pi\\) wide like this allows for more possible responses (maybe 75% of students in one country read comic books often!).\n\n\nCode\nggplot() +\n  stat_function(fun = ~dbeta(., 2, 6), geom = \"area\", fill = \"#AC3414\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Possible values for π\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\nPrior distribution for π using Beta(2, 6)\n\n\n\n\nOkay, so with our Beta(2, 6) prior, we now have all the information we need to specify the official formal model of the data generating process for our estimand \\(\\theta\\) without any flowchart golems or null hypotheses:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Estimand} \\\\\n\\theta =&\\ \\pi_{\\text{comic books, often}_\\text{Mexico}} - \\pi_{\\text{comic books, often}_\\text{US}} \\\\[10pt]\n&\\ \\textbf{Beta-binomial model for Mexico} \\\\\ny_{n \\text{ comic books, often}_\\text{Mexico}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{Mexico}}, \\pi_{\\text{comic books, often}_\\text{Mexico}}) \\\\\n\\pi_{\\text{comic books, often}_\\text{Mexico}} \\sim&\\ \\operatorname{Beta}(\\alpha_\\text{Mexico}, \\beta_\\text{Mexico}) \\\\[10pt]\n&\\ \\textbf{Beta-binomial model for the United States} \\\\\ny_{n \\text{ comic books, often}_\\text{US}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{US}}, \\pi_{\\text{comic books, often}_\\text{US}}) \\\\\n\\pi_{\\text{comic books, often}_\\text{US}} \\sim&\\ \\operatorname{Beta}(\\alpha_\\text{US}, \\beta_\\text{US}) \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\alpha_\\text{Mexico}, \\alpha_\\text{US} =&\\ 2 \\\\\n\\beta_\\text{Mexico}, \\beta_\\text{US} =&\\ 6\n\\end{aligned}\n\\]\n\n\nBasic Stan model\nThe neat thing about Stan is that it translates fairly directly from this mathematical model notation into code. Here we’ll define three different blocks in a Stan program:\n\nData that gets fed into the model, or the counts of respondents (or \\(y\\) and \\(n\\))\nParameters to estimate, or \\(\\pi_\\text{US}\\) and \\(\\pi_\\text{Mexico}\\)\nThe prior and model for estimating those parameters, or \\(\\operatorname{Beta}(2, 6)\\) and \\(y \\sim \\operatorname{Binomial}(n, \\pi)\\)\n\n\n\n\nprops-basic.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; often_us;\n  int&lt;lower=0&gt; total_us;\n  int&lt;lower=0&gt; often_mexico;\n  int&lt;lower=0&gt; total_mexico;\n}\n\n// Things to estimate\nparameters {\n  real&lt;lower=0, upper=1&gt; pi_us;\n  real&lt;lower=0, upper=1&gt; pi_mexico;\n}\n\n// Prior and likelihood\nmodel {\n  pi_us ~ beta(2, 6);\n  pi_mexico ~ beta(2, 6);\n  \n  often_us ~ binomial(total_us, pi_us);\n  often_mexico ~ binomial(total_mexico, pi_mexico);\n}\n\n\nUsing {cmdstanr} as our interface with Stan, we first have to compile the script into an executable file:\n\n\nCode\nmodel_props_basic &lt;- cmdstan_model(\"props-basic.stan\")\n\n\nWe can then feed it a list of data and run a bunch of MCMC chains:\n\n\nCode\noften_values &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n\n(n_often &lt;- as.numeric(c(often_values[1, 4], often_values[2, 4])))\n## [1] 10605   524\n(n_respondents &lt;- as.numeric(c(often_values[1, 5], often_values[2, 5])))\n## [1] 37641  5142\n\nprops_basic_samples &lt;- model_props_basic$sample(\n  data = list(often_us = n_often[2],\n              total_us = n_respondents[2],\n              often_mexico = n_often[1],\n              total_mexico = n_respondents[1]),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\nprops_basic_samples$print(\n  variables = c(\"pi_us\", \"pi_mexico\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##   variable mean median   sd 2.5% 97.5%\n##  pi_us     0.10   0.10 0.00 0.09  0.11\n##  pi_mexico 0.28   0.28 0.00 0.28  0.29\n\n\nWe have the two proportions—10% and 28%—and they match what we found in the original table and in the frequentist prop.test() results (yay!). Let’s visualize these things:\n\n\nCode\nprops_basic_samples %&gt;% \n  gather_draws(pi_us, pi_mexico) %&gt;% \n  ggplot(aes(x = .value, y = .variable, fill = .variable)) +\n  stat_halfeye() +\n  # Multiply axis limits by 1.5% so that the right \"%\" isn't cut off\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\n\n\n\n\nPosterior distributions of the proportion of students who read comic books often in the United States and Mexico\n\n\n\n\nThis plot is neat for a couple reasons. First it shows the difference in variance across these two distributions. The sample size for Mexican respondents is huge, so the average is a lot more precise and ■the distribution is narrower than ■the American one. Second, by just eyeballing the plot we can see that there’s definitely no overlap between the two distributions, which implies that ■the difference (θ) between the two is definitely not zero—Mexican respondents are way more likely than Americans to read comic books often. We can find ■that difference by taking the pairwise difference between the two with compare_levels() from {tidybayes}, which subtracts one group’s posterior from the other:\n\n\nCode\nbasic_diffs &lt;- props_basic_samples %&gt;% \n  gather_draws(pi_us, pi_mexico) %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so by default it\n  # would calculate pi_us - pi_mexico, but we want the opposite, so we have to\n  # make pi_us the first level\n  mutate(.variable = fct_relevel(.variable, \"pi_us\")) %&gt;% \n  compare_levels(.value, by = .variable, comparison = \"pairwise\")\n\nbasic_diffs %&gt;% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\nPosterior distribution of the difference in proportions of students who read comic books often in the United States and Mexico\n\n\n\n\nWe can also do some Bayesian inference and find the probability that ■that difference between the two groups is greater than 0 (or a kind of Bayesian p-value, but way more logical than null hypothesis p-values). We can calculate how many posterior draws are bigger than 0 and divide that by the number of draws to get the official proportion.\n\n\nCode\nbasic_diffs %&gt;% \n  summarize(median = median_qi(.value),\n            p_gt_0 = sum(.value &gt; 0) / n()) %&gt;% \n  unnest(median)\n## # A tibble: 1 × 8\n##   .variable             y  ymin  ymax .width .point .interval p_gt_0\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n## 1 pi_mexico - pi_us 0.180 0.170 0.189   0.95 median qi             1\n\n\nThere’s a 100% chance that ■that difference is not zero, or a 100% chance that Mexican respondents are way more likely than their American counterparts to read comic books often.\n\n\nStan model improvements\nThis basic Stan model is neat, but we can do a couple things to make it better:\n\nRight now we have to feed it 4 separate numbers (counts and totals for the US and Mexico). It would be nice to just feed it a vector of counts and a vector of totals (or even a whole matrix like we did with prop.test()).\nRight now we have to manually calculate the difference between the two groups (0.28 − 0.10). It would be nice to have Stan do that work for us.\n\nWe’ll tackle each of these issues in turn.\nFirst we’ll change how the script handles the data so that it’s more dynamic. Now instead of defining explicit variables and parameters as total_us or pi_mexico or whatever, we’ll use arrays and vectors so that we can use any arbitrary number of groups if we want:\n\n\n\nprops-better.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; n;\n  array[n] int&lt;lower=0&gt; often;\n  array[n] int&lt;lower=0&gt; total;\n}\n\n// Things to estimate\nparameters {\n  vector&lt;lower=0, upper=1&gt;[n] pi;\n}\n\n// Prior and likelihood\nmodel {\n  pi ~ beta(2, 6);\n  \n  // We could specify separate priors like this\n  // pi[1] ~ beta(2, 6);\n  // pi[2] ~ beta(2, 6);\n  \n  often ~ binomial(total, pi);\n}\n\n\nLet’s make sure it works. Note how we now have to feed it an n for the number of countries and vectors of counts for often and total:\n\n\nCode\nmodel_props_better &lt;- cmdstan_model(\"props-better.stan\")\n\n\n\n\nCode\nprops_better_samples &lt;- model_props_better$sample(\n  data = list(n = 2,\n              often = c(n_often[2], n_often[1]),\n              total = c(n_respondents[2], n_respondents[1])),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\nprops_better_samples$print(\n  variables = c(\"pi[1]\", \"pi[2]\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable mean median   sd 2.5% 97.5%\n##     pi[1] 0.10   0.10 0.00 0.09  0.11\n##     pi[2] 0.28   0.28 0.00 0.28  0.29\n\n\nIt worked and the results are the same! The parameter names are now ■“pi[1]” and ■“pi[2]” and we’re responsible for keeping track of which subscripts correspond to which countries, which is annoying, but that’s Stan :shrug:.\nFinally, we can modify the script a little more to automatically calculate ■θ. We’ll add a generated quantities block for that:\n\n\n\nprops-best.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; n;\n  array[n] int&lt;lower=0&gt; often;\n  array[n] int&lt;lower=0&gt; total;\n}\n\n// Things to estimate\nparameters {\n  vector&lt;lower=0, upper=1&gt;[n] pi;\n}\n\n// Prior and likelihood\nmodel {\n  pi ~ beta(2, 6);\n  \n  often ~ binomial(total, pi);\n}\n\n// Stuff Stan will calculate before sending back to R\ngenerated quantities {\n  real theta;\n  theta = pi[2] - pi[1];\n}\n\n\n\n\nCode\nmodel_props_best &lt;- cmdstan_model(\"props-best.stan\")\n\n\n\n\nCode\nprops_best_samples &lt;- model_props_best$sample(\n  data = list(n = 2,\n              often = c(n_often[2], n_often[1]),\n              total = c(n_respondents[2], n_respondents[1])),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\nprops_best_samples$print(\n  variables = c(\"pi[1]\", \"pi[2]\", \"theta\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable mean median   sd 2.5% 97.5%\n##     pi[1] 0.10   0.10 0.00 0.09  0.11\n##     pi[2] 0.28   0.28 0.00 0.28  0.29\n##     theta 0.18   0.18 0.00 0.17  0.19\n\n\nCheck that out! Stan returned our ■18 percentage point difference and we didn’t need to use compare_levels()! We can plot it directly:\n\n\nCode\n# Raw Stan doesn't preserve the original country names or order, so we have to\n# do a bunch of reversing and relabeling on our own here\np1 &lt;- props_best_samples %&gt;% \n  spread_draws(pi[i]) %&gt;% \n  mutate(i = factor(i)) %&gt;% \n  ggplot(aes(x = pi, y = fct_rev(i), fill = fct_rev(i))) +\n  stat_halfeye() +\n  # Multiply axis limits by 1.5% so that the right \"%\" isn't cut off\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_y_discrete(labels = rev(c(\"United States\", \"Mexico\"))) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- props_best_samples %&gt;% \n  spread_draws(theta) %&gt;% \n  ggplot(aes(x = theta)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from raw Stan code"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with {brms}",
    "text": "Bayesianly with {brms}\nWorking with raw Stan code like that is fun and exciting—understanding the inner workings of these models is really neat and important! But in practice, I rarely use raw Stan. It takes too much pre- and post-processing for my taste (the data has to be fed in a list instead of a nice rectangular data frame; the variable names get lost unless you do some extra programming work; etc.).\nInstead, I use {brms} for pretty much all my Bayesian models. It uses R’s familiar formula syntax, it works with regular data frames, it maintains variable names, and it’s just an all-around super-nice-and-polished frontend for working with Stan.\nWith a little formula finagling, we can create the same beta binomial model we built with raw Stan using {brms}\n\nCounts and trials as formula outcomes\nIn R’s standard formula syntax, you put the outcome on the left-hand side of a ~ and the explanatory variables on the right-hand side:\nlm(y ~ x, data = whatever)\nYou typically feed the model function a data frame with columns for each of the variables included. One neat and underappreciated feature of the glm() function is that you can feed function aggregated count data (instead of long data with lots of rows) by specifying the number of successes and the total number of failures as the outcome part of the formula. This runs something called aggregated logistic regression or aggregated binomial regression.\nglm(cbind(n_successes, n_failures) ~ x, data = whatever, family = binomial)\n{brms} uses slightly different syntax for aggregated logistic regression. Instead of the number of failures, it needs the total number of trials, and it doesn’t use cbind(...)—it uses n | trials(total), like this:\nbrm(\n  bf(n | trials(total) ~ x)\n  data = whatever,\n  family = binomial\n)\nOur comic book data is already in this count form, and we have columns for the number of “successes” (number of respondents reading comic books often) and the total number of “trials” (number of respondents reading comic books):\n\n\nCode\noften_comics_only &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\noften_comics_only\n## # A tibble: 2 × 6\n##   country       book_type   frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Mexico        Comic books Often     10605 37641 0.282\n## 2 United States Comic books Often       524  5142 0.102\n\n\n\n\nBinomial model with logistic link\nSince we have columns for n, total, and country, we can run an aggregated binomial logistic regression model like this:\n\n\nCode\noften_comics_model_logit &lt;- brm(\n  bf(n | trials(total) ~ country),\n  data = often_comics_only,\n  family = binomial(link = \"logit\"),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\n\n\n\nCode\noften_comics_model_logit\n##  Family: binomial \n##   Links: mu = logit \n## Formula: n | trials(total) ~ country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              -0.94      0.01    -0.96    -0.91 1.00     4706     3051\n## countryUnitedStates    -1.24      0.05    -1.34    -1.15 1.00      981     1123\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause we used a logit link for the binomial family, these results are on the log odds scale, which, ew. We can’t really interpret them directly unless we do some extra math with plogis() (see here for more about how to do that). The logistic-ness of the results is also apparent in the formal mathy model for this approach, which no longer uses a Beta distribution for estimating \\(\\pi\\):\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\operatorname{logit}(\\pi_{\\text{comic books, often}}) =&\\ \\beta_0 + \\beta_1 \\text{United States} \\\\[5pt]\n\\beta_0, \\beta_1 =& \\text{Whatever brms uses as default priors}\\\\\n\\end{aligned}\n\\]\nWe can still work with percentage point values if we use epred_draws() and a bit of data wrangling, since that automatically back-transforms \\(\\operatorname{logit}(\\pi)\\) from log odds to counts (see here for an explanation of how and why). We can convert these posterior counts to a proportion again by dividing each predicted count by the total for each row.\n\n\nCode\n# brms keeps all the original factor/category names, so there's no need for\n# extra manual work here!\ndraws_logit &lt;- often_comics_model_logit %&gt;% \n  # This gives us counts...\n  epred_draws(newdata = often_comics_only) %&gt;% \n  # ...so divide by the original total to get proportions again\n  mutate(.epred_prop = .epred / total)\n\np1 &lt;- draws_logit %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- draws_logit %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so so we have to\n  # make the United States the first level\n  mutate(country = fct_relevel(country, \"United States\")) %&gt;% \n  compare_levels(.epred_prop, by = \"country\") %&gt;% \n  ggplot(aes(x = .epred_prop)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from logistic regression model in {brms}\n\n\n\n\n\n\nCode\ndraws_logit %&gt;% \n  group_by(country) %&gt;% \n  median_qi(.epred_prop)\n## # A tibble: 2 × 7\n##   country       .epred_prop .lower .upper .width .point .interval\n##   &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Mexico              0.282 0.277   0.286   0.95 median qi       \n## 2 United States       0.102 0.0936  0.110   0.95 median qi\n\n\nCool cool, all the results are the same as using raw Stan.\n\n\nBinomial model with identity link\nWe didn’t set any priors here, and if we want to be good Bayesians, we should. However, given the logit link, we’d need to specify priors on the log odds scale, and I can’t naturally think in logits. I can think about percentages though, which is why I like the Beta distribution for priors for proportions—it just makes sense.\nAlso, the raw Stan models spat out percentage-point scale parameters—it’d be neat if {brms} could too.\nAnd it can! We just have to change the link function for the binomial family from \"logit\" to \"identity\". This isn’t really documented anywhere (I don’t think?), and it feels weird and wrong, but it works. Note how we take the “logit” out of the second line of the model—we’re no longer using a link function:\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\pi_{\\text{comic books, often}} =&\\ \\beta_0 + \\beta_1 \\text{United States} \\\\[5pt]\n\\beta_0, \\beta_1 =&\\ \\text{Whatever brms uses as default priors}\\\\\n\\end{aligned}\n\\]\nDoing this works, but there are some issues. The identity link in a binomial model means that the model parameters won’t be transformed to the logit scale and will instead stay on the proportion scale. We’ll get some errors related to MCMC values because the outcome needs to be constrained between 0 and 1, and the MCMC chains will occasionally wander down into negative numbers and make Stan mad. The model will mostly fit if we specify initial MCMC values at 0.1 or something, but it’ll still complain.\n\n\nCode\noften_comics_model_identity &lt;- brm(\n  bf(n | trials(total) ~ country),\n  data = often_comics_only,\n  family = binomial(link = \"identity\"),\n  init = 0.1,\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0175658, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0175658, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0471304, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0471304, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[2] is -0.03896, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Exception: binomial_lpmf: Probability parameter[2] is -0.03896, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0529492, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0529492, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpQKMbv4/model-d1884317c1cb.stan', line 36, column 4 to column 44)\n## Chain 2 finished in 0.0 seconds.\n## Chain 1 finished in 0.1 seconds.\n## Chain 3 finished in 0.1 seconds.\n## Chain 4 finished in 0.1 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 0.3 seconds.\n\n\n\n\nCode\noften_comics_model_identity\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept               0.28      0.00     0.28     0.29 1.00     3627     2558\n## countryUnitedStates    -0.18      0.00    -0.19    -0.17 1.00     1472     1907\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOne really nice thing about this identity-link model is that the coefficient for countryUnitedStates shows us the percentage-point-scale difference in proportions: −0.18! This is just a regular regression model, so the intercept shows us the average proportion when United States is false (i.e. for Mexico), and the United States coefficient shows the offset from the intercept.\nWorking with the countryUnitedStates coefficient directly is convenient—there’s no need to divide predicted values by totals or use compare_levels() to find the difference between the United States and Mexico. We have ■our estimand immediately.\n\n\nCode\ndraws_diffs_identity &lt;- often_comics_model_identity %&gt;% \n  gather_draws(b_countryUnitedStates) %&gt;% \n  # Reverse the value since our theta is Mexico - US, not US - Mexico\n  mutate(.value = -.value)\n\ndraws_diffs_identity %&gt;% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\nPosterior distribution of the difference in proportions of students who read comic books often in the United States and Mexico; results from binomial model with identity link in {brms}\n\n\n\n\n\n\nCode\ndraws_diffs_identity %&gt;% \n  median_qi(.value)\n## # A tibble: 1 × 7\n##   .variable             .value .lower .upper .width .point .interval\n##   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 b_countryUnitedStates  0.180  0.170  0.189   0.95 median qi\n\n\n\n\nIntercept-free binomial model with identity link\nHowever, I’m still not entirely happy with it. For one thing, I don’t like all the initial MCMC errors. The model still eventually fit, but I’d prefer it to have a less rocky start. I could probably tinker with more options to get it working, but that’s a hassle.\nMore importantly, though, is the issue of priors. We still haven’t set any. Also, we’re no longer using a beta-binomial model—this is just regular old logistic regression, which means we’re working with intercepts and slopes. If we use the n | trials(total) ~ country model with an identity link, we’d need to set priors for the intercept and for the difference, which means we need to think about two types of values: (1) the prior average percentage for Mexico and (2) the prior average difference between Mexico and the United States. In the earlier raw Stan model, we set priors for the average percentages for each country and didn’t worry about thinking about the difference. Conceptually, I think this is easier. In my own work, I can think about the prior distributions for specific survey response categories (30% might agree, 50% might disagree, 20% might be neutral), but thinking about differences is less natural and straightforward (there might be a 20 percentage point difference between agree and disagree? that feels weird).\nTo get percentages for each country and avoid the odd initial value errors and set more natural priors, and ultimately use a beta-binomial model, we can fit an intercept-free model by including a 0 in the right-hand side of the formula. This disables the Mexico reference category and returns estimates for both Mexico and the United States. Now we can finally set a prior too. Here, as I did with the Stan model earlier, I use Beta(2, 6) for both countries, but it could easily be different for each country too. This is one way to force {brms} to essentially use a beta-binomial model, and results in something like this:\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\pi_{\\text{comic books, often}} =&\\ \\beta_\\text{Mexico} + \\beta_\\text{United States} \\\\[10pt]\n\\beta_\\text{Mexico}, \\beta_\\text{United States} =&\\ \\operatorname{Beta}(2, 6)\\\\\n\\end{aligned}\n\\]\n\n\nCode\noften_comics_model &lt;- brm(\n  bf(n | trials(total) ~ 0 + country),\n  data = often_comics_only,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(2, 6), class = b, lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\n\n\n\nCode\noften_comics_model\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ 0 + country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## countryMexico           0.28      0.00     0.28     0.29 1.00     3581     2914\n## countryUnitedStates     0.10      0.00     0.09     0.11 1.00     2824     2470\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients here represent the average proportions for each country. The ■main estimand we care about is still the difference between the two, so we need to do a little bit of data manipulation to calculate that, just like we did with the first logit version of the model:\n\n\nCode\np1 &lt;- often_comics_model %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- often_comics_model %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so so we have to\n  # make the United States the first level\n  mutate(country = fct_relevel(country, \"United States\")) %&gt;% \n  compare_levels(.epred_prop, by = country,\n                 comparison = \"pairwise\") %&gt;% \n  ggplot(aes(x = .epred_prop)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from an intercept-free binomial model with identity link in {brms}\n\n\n\n\n\n\nActual beta-binomial model\nUntil {brms} 2.17, there wasn’t an official beta-binomial distribution for {brms}, but it was used as the example for creating your own custom family. It is now implemented in {brms} and allows you to define both a mean (\\(\\mu\\)) and precision (\\(\\phi\\)) for a Beta distribution, just like {brms}’s other Beta-related models (like zero-inflated, etc.—see here for a lot more about those). This means that we can model both parts of the distribution simultaneously, which is neat, since it allows us to deal with potential overdispersion in outcomes. Paul Bürkner’s original rationale for not including it was that a regular binomial model with a random effect for the observation id also allows you to account for overdispersion, so there’s not really a need for an official beta-binomial family. But in March 2022 the beta_binomial family was added as an official distributional family, which is neat.\nWe can use it here instead of family = binomial(link = \"identity\") with a few adjustments. The family uses a different mean/precision parameterization of the Beta distribution instead of the two shapes \\(\\alpha\\) and \\(\\beta\\), but we can switch between them with some algebra (see this for more details):\n\\[\n\\begin{equation}\n\\begin{aligned}[t]\n\\text{Shape 1:} && \\alpha &= \\mu \\phi \\\\\n\\text{Shape 2:} && \\beta &= (1 - \\mu) \\phi\n\\end{aligned}\n\\qquad\\qquad\\qquad\n\\begin{aligned}[t]\n\\text{Mean:} && \\mu &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Precision:} && \\phi &= \\alpha + \\beta\n\\end{aligned}\n\\end{equation}\n\\]\nBy default, \\(\\mu\\) is modeled on the log odds scale and \\(\\phi\\) is modeled on the log scale, but I find both of those really hard to think about, so we can use an identity link for both parameters like we did before with binomial() to think about counts and proportions instead. This makes it so the \\(\\phi\\) parameter measures the standard deviation of the count on the count scale, so a prior like Exponential(1 / 1000) would imply that the precision (or variance-ish) of the count could vary by mostly low numbers, but maybe up to ±5000ish, which seems reasonable, especially since the Mexico part of the survey has so many respondents:\n\n\nCode\nggplot() +\n  stat_function(fun = ~dexp(., 0.001), geom = \"area\", fill = \"#AC3414\") +\n  scale_x_continuous(labels = label_number(big.mark = \",\"), limits = c(0, 5000)) +\n  labs(x = \"Possible values for φ\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\nExponential(1/1000) distribution\n\n\n\n\n\n\nCode\noften_comics_model_beta_binomial &lt;- brm(\n  bf(n | trials(total) ~ 0 + country),\n  data = often_comics_only,\n  family = beta_binomial(link = \"identity\", link_phi = \"identity\"),\n  prior = c(prior(beta(2, 6), class = \"b\", dpar = \"mu\", lb = 0, ub = 1),\n            prior(exponential(0.001), class = \"phi\", lb = 0)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\n\n\n\n\n\n\n\nSeparate model for \\(\\phi\\)\n\n\n\nIf we wanted to be super fancy, we could define a completely separate model for the \\(\\phi\\) part of the distribution like this, but we don’t need to here:\noften_comics_model_beta_binomial &lt;- brm(\n  bf(n | trials(total) ~ 0 + country,\n     phi ~ 0 + country),\n  data = often_comics_only,\n  family = beta_binomial(link = \"identity\", link_phi = \"identity\"),\n  prior = c(prior(beta(2, 6), class = \"b\", dpar = \"mu\", lb = 0, ub = 1),\n            prior(exponential(0.001), dpar = \"phi\", lb = 0)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n\n\nCheck out the results:\n\n\nCode\noften_comics_model_beta_binomial\n##  Family: beta_binomial \n##   Links: mu = identity; phi = identity \n## Formula: n | trials(total) ~ 0 + country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## countryMexico           0.28      0.03     0.22     0.33 1.00     1786      890\n## countryUnitedStates     0.11      0.02     0.08     0.16 1.00     1626     1085\n## \n## Family Specific Parameters: \n##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## phi  1031.27   1022.22    38.29  3841.42 1.01      751      958\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThose coefficients are the group proportions, as expected, and we have a \\(\\phi\\) parameter representing the overall variation in counts. The proportions here are a little more uncertain than before, though, which is apparent if we plot the distributions. The distributions have a much wider range now (note that the x-axis now goes all the way up to 60%), and the densities are a lot bumpier and jankier. I don’t know why though! This is weird! I’m probably doing something wrong!\n\n\nCode\noften_comics_model_beta_binomial %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\n\n\n\n\nPosterior distributions of the proportion of students who read comic books often in the United States and Mexico"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Final answer to the question",
    "text": "Final answer to the question\nPhew, that was a lot of slow pedagogical exposure. What’s our ■official estimand? What’s our final answer to the question “Do respondents in Mexico read comic books more often than respondents in the United States?”?\nYes. They most definitely do.\nIn an official sort of report or article, I’d write something like this:\n\nStudents in Mexico are far more likely to read comic books often than students in the United States. On average, 28.2% (between 27.7% and 28.6%) of PISA respondents in Mexico read comic books often, compared to 10.2% (between 9.4% and 11.1%) in the United States. There is a 95% posterior probability that the difference between these proportions is between 17.0 and 18.9 percentage points, with a median of 18.0 percentage points. This difference is substantial, and there’s a 100% chance that the difference is not zero."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Estimand",
    "text": "Estimand\nFor this question, we want to know the differences in the the proportions of American newspaper-reading frequencies, or whether the differences between (1) ■rarely and ■sometimes, (2) ■sometimes and ■often, and (3) ■rarely and ■often are greater than zero:\n\n\nCode\nfancy_table %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#E51B24\", 0.8)),\n      cell_text(color = \"#E51B24\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 3, rows = 4)\n    )\n  ) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#FFDE00\", 0.8)),\n      cell_text(color = colorspace::darken(\"#FFDE00\", 0.1), weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 4, rows = 4)\n    )\n  ) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#009DDC\", 0.8)),\n      cell_text(color = \"#009DDC\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 5, rows = 4)\n    )\n  )\n\n\n\n\n\n\n  \n    \n    \n      \n      Rarely\n      Sometimes\n      Often\n    \n  \n  \n    \n      Mexico\n    \n    Comic books\n\n26.29%(9,897)\n\n45.53%(17,139)\n\n28.17%(10,605)\n\n    Newspapers\n\n18.02%(6,812)\n\n32.73%(12,369)\n\n49.25%(18,613)\n\n    \n      United States\n    \n    Comic books\n\n62.95%(3,237)\n\n26.86%(1,381)\n\n10.19%(524)\n\n    Newspapers\n\n25.27%(1,307)\n\n37.22%(1,925)\n\n37.51%(1,940)\n\n  \n  \n  \n\n\n\n\nWe’ll again call this estimand \\(\\theta\\), but have three different versions of it:\n\\[\n\\begin{aligned}\n\\theta_1 &= \\pi_\\text{US, newspapers, often} - \\pi_\\text{US, newspapers, sometimes} \\\\\n\\theta_2 &= \\pi_\\text{US, newspapers, sometimes} - \\pi_\\text{US, newspapers, rarely} \\\\\n\\theta_3 &= \\pi_\\text{US, newspapers, often} - \\pi_\\text{US, newspapers, rarely}\n\\end{aligned}\n\\] We just spent a bunch of time talking about comic books, and now we’re looking at data about newspapers and America. Who represents all three of these things simultaneously? Clark Kent / Superman, obviously, the Daily Planet journalist and superpowered alien dedicated to truth, justice, and the American way a better tomorrow. I found this palette at Adobe Color.\n\n\nCode\n# US newspaper question colors\nclr_often &lt;- \"#009DDC\"\nclr_sometimes &lt;- \"#FFDE00\"\nclr_rarely &lt;- \"#E51B24\"\n\n\n\n\n\n“Superman” by Hannaford"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with {brms}",
    "text": "Bayesianly with {brms}\nLet’s first extract the aggregated data we’ll work with—newspaper frequency in the United States only:\n\n\nCode\nnewspapers_only &lt;- reading_counts %&gt;% \n  filter(book_type == \"Newspapers\", country == \"United States\")\nnewspapers_only\n## # A tibble: 3 × 6\n##   country       book_type  frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;      &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 United States Newspapers Rarely     1307  5172 0.253\n## 2 United States Newspapers Sometimes  1925  5172 0.372\n## 3 United States Newspapers Often      1940  5172 0.375\n\n\nWe’ll define this formal beta-binomial model for each of the group proportions and we’ll use a Beta(2, 6) prior again (so 25% ± a bunch):\n\\[\n\\begin{aligned}\n&\\ \\textbf{Estimands} \\\\\n\\theta_1 =&\\ \\pi_{\\text{newspapers, often}_\\text{US}} - \\pi_{\\text{newspapers, sometimes}_\\text{US}} \\\\\n\\theta_2 =&\\ \\pi_{\\text{newspapers, sometimes}_\\text{US}} - \\pi_{\\text{newspapers, rarely}_\\text{US}} \\\\\n\\theta_3 =&\\ \\pi_{\\text{newspapers, often}_\\text{US}} - \\pi_{\\text{newspapers, rarely}_\\text{US}} \\\\[10pt]\n&\\ \\textbf{Beta-binomial model} \\\\\ny_{n \\text{ newspapers, [frequency]}_\\text{US}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{US}}, \\pi_{\\text{newspapers, [frequency]}_\\text{US}}) \\\\\n\\pi_{\\text{newspapers, [frequency]}_\\text{US}} \\sim&\\ \\operatorname{Beta}(\\alpha, \\beta) \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\alpha =&\\ 2 \\\\\n\\beta =&\\ 6\n\\end{aligned}\n\\]\nWe can estimate this model with {brms} using an intercept-free binomial model with an identity link:\n\n\nCode\nfreq_newspapers_model &lt;- brm(\n  bf(n | trials(total) ~ 0 + frequency),\n  data = newspapers_only,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(2, 6), class = b, lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.1 seconds.\n\n\n\n\nCode\nfreq_newspapers_model\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ 0 + frequency \n##    Data: newspapers_only (Number of observations: 3) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## frequencyRarely        0.25      0.01     0.24     0.26 1.00     4536     2758\n## frequencySometimes     0.37      0.01     0.36     0.38 1.00     4084     2933\n## frequencyOften         0.37      0.01     0.36     0.39 1.00     4318     3217\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt works! These group proportions are the same as what we found in the contingency table:\n\n\nCode\nplot_props_newspaper &lt;- freq_newspapers_model %&gt;% \n  epred_draws(newdata = newspapers_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = frequency, fill = frequency)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(clr_rarely, clr_sometimes, clr_often)) +\n  labs(x = \"Proportion of frequencies of newspaper reading\", y = NULL) +\n  guides(fill = \"none\") +\n  theme_nice()\nplot_props_newspaper\n\n\n\n\n\nPosterior distributions of the proportions of the frequency of reading newspapers among American students\n\n\n\n\nWe’re interested in our three \\(\\theta\\)s, or the posterior differences between each of these proportions. We can again use compare_levels() to find these all at once. If we specify comparison = \"pairwise\", {tidybayes} will calculate the differences between each pair of proportions.\n\n\nCode\nfreq_newspapers_diffs &lt;- freq_newspapers_model %&gt;% \n  epred_draws(newdata = newspapers_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ungroup() %&gt;% \n  compare_levels(.epred_prop, by = frequency) %&gt;% \n  # Put these in the right order\n  mutate(frequency = factor(frequency, levels = c(\"Often - Sometimes\", \n                                                  \"Sometimes - Rarely\",\n                                                  \"Often - Rarely\")))\n\nfreq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, y = fct_rev(frequency), fill = frequency)) +\n  stat_halfeye(fill = clr_diff) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  guides(fill = \"none\") +\n  theme_nice()\n\n\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; all density plots are filled with the same color for now\n\n\n\n\nWe can also calculate the official median differences and the probabilities that the posteriors are greater than 0:\n\n\nCode\nfreq_newspapers_diffs %&gt;% \n  summarize(median = median_qi(.epred_prop, .width = 0.95),\n            p_gt_0 = sum(.epred_prop &gt; 0) / n()) %&gt;% \n  unnest(median)\n## # A tibble: 3 × 8\n##   frequency                y    ymin   ymax .width .point .interval p_gt_0\n##   &lt;fct&gt;                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n## 1 Often - Sometimes  0.00268 -0.0161 0.0214   0.95 median qi         0.615\n## 2 Sometimes - Rarely 0.119    0.101  0.136    0.95 median qi         1    \n## 3 Often - Rarely     0.122    0.104  0.140    0.95 median qi         1\n\n\nThere’s only a 60ish% chance that the difference between ■often and ■sometimes is bigger than zero, so there’s probably not an actual difference between those two categories, but there’s a 100% chance that the differences between ■sometimes and ■rarely and ■often and ■rarely are bigger than zero."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#better-fill-colors",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#better-fill-colors",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Better fill colors",
    "text": "Better fill colors\nBefore writing up the final official answer, we need to tweak the plot of differences. With the comic book question, we used some overly-simplified-and-wrong color theory and created a ■yellow color for the difference (since ■blue − ■green = ■yellow). We could maybe do that here too, but we’ve actually used all primary colors in our Superman palette. I don’t know what ■blue − ■yellow or ■yellow − ■red would be, and even if I calculated it somehow, it wouldn’t be as cutely intuitive as blue minus green.\nSo instead, we’ll do some fancy fill work with the neat {ggpattern} package, which lets us fill ggplot geoms with multiply-colored patterns. We’ll fill each distribution of \\(\\theta\\)s with the combination of the two colors: we’ll fill the difference between ■often and ■sometimes with stripes of those two colors, and so on.\nWe can’t use geom/stat_halfeye() because {tidybayes} does fancier geom work when plotting its density slabs, but we can use geom_density_pattern() to create normal density plots:\n\n\nCode\nfreq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, fill = frequency, pattern_fill = frequency)) +\n  geom_density_pattern(\n    pattern = \"stripe\",  # Stripes\n    pattern_density = 0.5,  # Take up 50% of the pattern (i.e. stripes equally sized)\n    pattern_spacing = 0.2,  # Thicker stripes\n    pattern_size = 0,  # No border on the stripes\n    trim = TRUE,  # Trim the ends of the distributions\n    linewidth = 0  # No border on the distributions\n  ) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  # Set colors for fills and pattern fills\n  scale_fill_manual(values = c(clr_often, clr_sometimes, clr_often)) +\n  scale_pattern_fill_manual(values = c(clr_sometimes, clr_rarely, clr_rarely)) +\n  guides(fill = \"none\", pattern_fill = \"none\") +  # Turn off legends\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  facet_wrap(vars(frequency), ncol = 1) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; all density plots are filled with two stripes corresponding to the difference of their two categories\n\n\n\n\nAhhh that’s so cool!\nThe only thing that’s missing is the pointrange that we get from stat_halfeye() that shows the median and the 50%, 80%, and 95% credible intervals. We can calculate those ourselves and add them with geom_pointinterval():\n\n\nCode\n# Find medians and credible intervals\nfreq_newspapers_intervals &lt;- freq_newspapers_diffs %&gt;% \n  group_by(frequency) %&gt;% \n  median_qi(.epred_prop, .width = c(0.5, 0.8, 0.95))\n\nplot_diffs_nice &lt;- freq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, fill = frequency, pattern_fill = frequency)) +\n  geom_density_pattern(\n    pattern = \"stripe\",  # Stripes\n    pattern_density = 0.5,  # Take up 50% of the pattern (i.e. stripes equally sized)\n    pattern_spacing = 0.2,  # Thicker stripes\n    pattern_size = 0,  # No border on the stripes\n    trim = TRUE,  # Trim the ends of the distributions\n    linewidth = 0  # No border on the distributions\n  ) +\n  # Add 50%, 80%, and 95% intervals + median\n  geom_pointinterval(data = freq_newspapers_intervals, \n                     aes(x = .epred_prop, xmin = .lower, xmax = .upper)) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  # Set colors for fills and pattern fills\n  scale_fill_manual(values = c(clr_often, clr_sometimes, clr_often)) +\n  scale_pattern_fill_manual(values = c(clr_sometimes, clr_rarely, clr_rarely)) +\n  guides(fill = \"none\", pattern_fill = \"none\") +  # Turn off legends\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  facet_wrap(vars(frequency), ncol = 1) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\nplot_diffs_nice\n\n\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; density plots are filled with stripes and a pointrange shows the median and 50%, 80%, and 95% credible intervals\n\n\n\n\nPerfect!"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Final answer to the question",
    "text": "Final answer to the question\nSo, given these results, what’s the answer to the question “Do students in the United States vary in their frequency of reading newspapers?”? What are our three \\(\\theta\\)s? The frequencies vary, but only between ■rarely and the other two categories. There’s no difference between ■sometimes and ■often\n\n\nCode\n(\n  (plot_props_newspaper + labs(x = \"Proportions\") + \n     facet_wrap(vars(\"Response proportions\"))) | \n    plot_spacer() |\n    (plot_diffs_nice + labs(x = \"Percentage point differences\"))\n) +\n  plot_layout(widths = c(0.475, 0.05, 0.475))\n\n\n\n\n\nPosterior distribution of the proportions and difference in proportions of the frequency of reading newspapers among American students\n\n\n\n\nHere’s how I’d write about the results:\n\nStudents in the United States tend to read the newspaper at least sometimes, and are least likely to read it rarely. On average, 25.3% of American PISA respondents report reading the newspaper rarely (with a 95% credible interval of between 24.1% and 26.5%), compared to 37.2% reading sometimes (35.9%–38.5%) and 37.5% reading sometimes (36.2%–38.8%).\nThere is no substantial difference in proportions between those reporting reading newspapers often and sometimes. The posterior median difference is between −1.6 and 2.1 percentage points, with a median of 0.3 percentage points, and there’s only a 61.5% probability that this difference is greater than 0, which implies that the two categories are indistinguishable from each other.\nThere is a clear substantial difference between the proportion reading newspapers rarely and the other two responses, though. The posterior median difference between sometimes and rarely is between 10.1 and 13.6 percentage points (median = 11.9), while the difference between often and rarely is between 10.4 and 14.0 percentage points (median = 12.2). The probability that each of these differences is greater than 0 is 100%.\n\n\nEt voila! Principled, easily interpretable, non-golem-based tests of differences in proportions using Bayesian statistics!"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html",
    "title": "How old was Aragorn in regular human years?",
    "section": "",
    "text": "In The Two Towers, while talking with Eowyn, Aragorn casually mentions that he’s actually 87 years old.\nWhen Aragorn is off running for miles and miles and fighting orcs and trolls and Uruk-hai and doing all his other Lord of the Rings adventures, he hardly behaves like a regular human 87-year-old. How old is he really?\nIt turns out that Tolkien left us a clue in some of his unfinished writings about Númenor, and we can use that information to make some educated guesses about Aragon’s actual human-scale age. In this post I’ll (1) look at Tolkien’s Númenórean years → human years conversion system and (2) extrapolate that system through two types of statistical simulation—(a) just drawing random numbers and (b) Bayesian modeling—to make some predictions about a range of possible ages.\nBut first, some context about why Aragon is so old!"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#super-quick-crash-course-in-the-ages-of-arda-númenor",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#super-quick-crash-course-in-the-ages-of-arda-númenor",
    "title": "How old was Aragorn in regular human years?",
    "section": "Super quick crash course in the Ages of Arda + Númenor",
    "text": "Super quick crash course in the Ages of Arda + Númenor\nThe Lord of the Rings occurs at the end of the Third Age of the world. In Arda (the whole world Tolkien created) there are four recorded ages, plus some pre-game stuff:\n\nCreation: The main god Eru Ilúvatar and a couple dozen sub-gods (Ainur) and bunch of angel-like-sub-sub-gods (Maiar, including Gandalf, Saruman, and Sauron) created the world through music. This is all covered in the first part of The Silmarillion in a sub-book called The Ainulindalë.\nThe First Age: After the creation, the Elves all lived in a place outside of the main world called Valinor until the main bad guy, a fallen Ainu named Morgoth (aka Melkor) destroyed key parts of it and divided the Elves so that a bunch fled to the far west of Middle-earth to a land called Beleriand. This starts the First Age, which is mostly about the Elves of Beleriand and their battles with Morgoth/Melkor. Sauron (the main bad guy of The Lord of the Rings) serves as Morgoth’s lieutenant and they destroy a ton of cities (with armies of balrogs and orcs) and kill a ton of elves. The elves eventually win, but all of Beleriand sinks into the ocean and the elves either go back to Valinor (technically just outside of Valinor, which serves as elvish heaven), or to eastern Middle-earth. This is all covered in the bulk of The Silmarillion.\nThe Second Age: While the First Age is mostly about elves, mortal men eventually show up in Beleriand and they play a key role in the battle against Morgoth. They also intermingle with the immortal elves, sometimes falling in love—including the famous Lúthien and Beren (who are stand-ins for Tolkien and his wife Edith). Beren and Lúthien had kids, and their kids had kids, and so on until two half-elf brothers were born at the end of the First Age: Elrond (the same Elrond from The Hobbit and The Lord of the Rings and The Rings of Power) and Elros.\nAs Beleriand sinks, Elrond and Elros escape to eastern Middle-earth and are then given a choice of how to proceed with their futures. Elrond decides to become immortal like an elf; Elros decides to become mortal like a man, eventually building Rivendell. The gods reward Elros and the men who helped the elves against Morgoth by creating a utopic island in the middle of the ocean between Valinor and Middle-earth named Númenor. The gods also granted them super long life (400+ years, as we’ll explore below), but imposed a strict ban on them—the Númenóreans were forbidden from ever sailing west toward Valinor. Thus begins the two parallel stories of the Second Age—(1) Elrond and other refugees from Valinor doing stuff in Middle-earth, and (2) Elros and his descendants doing stuff on the island of Númenor. This is all covered in a final short part of The Silmarillion (the Akallabêth), and in random appendices in The Lord of the Rings, and in the newer The Fall of Númenor, and in Amazon’s TV series The Rings of Power.\nNúmenor and Middle-earth hum along happily for three thousand years until Sauron (who fled to Middle-earth after Morgoth was destroyed) shows up. He disguises himself as a super affable friendly dude who everyone loves and then sows chaos. He visits the elves and convinces them to make a bunch of rings, and then he heads to Númenor to convince them to violate the ban and sail to Valinor. The Númenóreans do, they get in trouble, the gods sink their island, and Númenórean refugees flee to Middle-earth, led by Elendil and his sons Isildur and Anárion, who set up a Númenórean kingdom in Gondor. Sauron starts using his fancy new One Ring and tries to conquer Middle-earth; there’s a big war against him (see the first few minutes of The Fellowship of the Ring); Sauron kills Elendil; Isildur cuts off Sauron’s finger and makes him lose the ring, which destroys him; Isildur keeps the ring; he gets killed and the kingdom of Gondor falls, thus ending the Second Age.\nThe Third Age: The ring disappears for a few thousand years until Gollum picks it up, then Bilbo gets it in The Hobbit, and then Frodo gets it in The Fellowship of the Ring and destroys it in The Return of the King.\nMeanwhile, Gondor is ruled by a series of stewards who are supposed to take care of the kingdom until a Númenórean king returns to take his place. The magic long life of the Númenóreans starts declining, except for some special refugees from Gondor named the Dúnedain (singular Dúnadan), of which Aragorn is one. These Dúnedain maintain some of the magic Númenórean longevity—they don’t live 400+ years like their Númenórean ancestors, but they live well beyond 150. After the ring is destroyed, Aragorn is installed as king of Gondor, all the remaining elves go back to Valinor (except Arwen, Aragorn’s now-wife), and the Third Age ends.\nThe Fourth Age: Aragorn reigns until he’s 210, then he dies, someone else takes over, and Middle-earth lives happily ever after."
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#packages",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#packages",
    "title": "How old was Aragorn in regular human years?",
    "section": "Packages",
    "text": "Packages\nBefore diving into the data and simulations, we need to load some R libraries and make some helper functions. For the sake of narrative here, the code is automatically collapsed—click on the little triangle arrow to show it.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(glue)\nlibrary(ggtext)\nlibrary(ggimage)\n\nset.seed(1234)\n\nupdate_geom_defaults(\"label\", list(family = \"Diavlo Medium\"))\nupdate_geom_defaults(\"richtext\", list(family = \"Diavlo Medium\"))\n\n# Diavlo: https://www.exljbris.com/diavlo.html\ntheme_numenor &lt;- function() {\n  theme_minimal(base_family = \"Diavlo Medium\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\nclrs &lt;- c(\"#0E2B50\", \"#415B6B\", \"#0B4C7D\", \n          \"#EE9359\", \"#8C6D46\", \"#BF3B0B\", \"#400101\")"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#tolkiens-númenórean-years-normal-human-years-system",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#tolkiens-númenórean-years-normal-human-years-system",
    "title": "How old was Aragorn in regular human years?",
    "section": "Tolkien’s Númenórean years → normal human years system",
    "text": "Tolkien’s Númenórean years → normal human years system\nIn the appendix of the newly published The Fall of Númenor and Other Tales From the Second Age of Middle-earth is a fascinating footnote that explains exactly how to convert Second Age Númenórean years into normal human years. Tolkien writes:\n\n\nDeduct 20: Since at 20 years a Númenórean would be at about the same stage of development as an ordinary person.\n\nAdd to this 20 the remainder divided by 5. Thus a Númenórean man or woman of years [X] would be approximately of the “age” [Y] (Tolkien 2022, “The Life of the Númenóreans,” note 8, p. 262)\n\n\nAnd he provides this helpful table, which we’ll stick in an R data frame so we can play with it:\n\nages &lt;- tibble(\n  numenor_age = c(25, 50, 75, 100, 125, 150, 175, 200, 225, \n                  250, 275, 300, 325, 350, 375, 400, 425),\n  normal_human_age = c(21, 26, 31, 36, 41, 46, 51, 56, 61, \n                       66, 71, 76, 81, 86, 91, 96, 101)\n)\n\n\n\nCode\nages |&gt; \n  rename(\"Númenórean age\" = numenor_age,\n         \"Normal human age\" = normal_human_age) |&gt; \n  t() |&gt; knitr::kable()\n\n\n\n\nTable 1: Tolkien’s original table for converting between Númenórean and human ages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNúmenórean age\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\n275\n300\n325\n350\n375\n400\n425\n\n\nNormal human age\n21\n26\n31\n36\n41\n46\n51\n56\n61\n66\n71\n76\n81\n86\n91\n96\n101\n\n\n\n\n\n\nTolkien’s logic is a little convoluted, but it works. For example, if a Númenórean is 125, subtract 20, then divide the remainder by 5 and add that to 20 to get 41 normal human years:\n\\[\n\\left(\\frac{125-20}{5} + 20\\right) = (21 + 20) = 41\n\\]\nIf we plot all of Tolkien’s example ages, we get a nice linear relationship between Númenórean ages and human ages:\n\n\nCode\nggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_point() +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  theme_numenor()\n\n\n\n\n\nFigure 1: Scatterplot of Tolkien’s original table for converting between Númenórean and human ages\n\n\n\n\nSince it’s linear, we can skip the convoluted subtract-20-add-divided-remainder logic and instead figure out a slope and intercept for the line.\n\n\nCode\nage_model &lt;- lm(normal_human_age ~ numenor_age, data = ages)\nage_model\n## \n## Call:\n## lm(formula = normal_human_age ~ numenor_age, data = ages)\n## \n## Coefficients:\n## (Intercept)  numenor_age  \n##        16.0          0.2\n\n\nThe line starts at the y-axis at 16 normal human years and then increases by 0.2 for every Númenórean year (the 0.2 is that divide-by-5 rule, since 1/5 = 0.2).\n\\[\n\\text{Normal human years} = 16 + (0.2 \\times \\text{Númenórean years})\n\\]\n\n\nCode\nggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[3]) +\n  geom_point() +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  annotate(geom = \"richtext\", x = 300, y = 40, \n           label = \"Normal human years =&lt;br&gt;16 + (0.2 × Númenórean years)\") +\n  theme_numenor()\n\n\n\n\n\nFigure 2: Linear model for converting between Númenórean and human ages"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#estimating-the-dúnedain-normal-human-age-system",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#estimating-the-dúnedain-normal-human-age-system",
    "title": "How old was Aragorn in regular human years?",
    "section": "Estimating the Dúnedain → normal human age system",
    "text": "Estimating the Dúnedain → normal human age system\nAragorn was a Dúnedan, or a descendant of the Númenórean refugees Elendil and Isildur, so he inherited their unnaturally long lifespan. Aragorn was 87 at the end of the Third Age, and he lived until he was 210. If we naively assume he lived as long as a standard Númenórean, he would have gone through the events of The Lord of the Rings at 33 and died surprisingly young at 58:\n\n\nCode\nggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[3]) +\n  geom_point() +\n  geom_vline(xintercept = c(87, 210)) +\n  annotate(geom = \"segment\", x = -Inf, xend = 87, y = 33, yend = 33, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = -Inf, xend = 210, y = 58, yend = 58, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  geom_image(data = tibble(numenor_age = 87, normal_human_age = 80, \n                           image = \"img/aragorn-alive.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 87, y = 62, \n           label = glue(\"&lt;span style='color:{clrs[1]};'&gt;87 Númenórean years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;33 human years&lt;/span&gt;\")) +\n  geom_image(data = tibble(numenor_age = 210, normal_human_age = 80, \n                           image = \"img/aragorn-dead.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 210, y = 98, \n           label = glue(\"&lt;span style='color:{clrs[1]};'&gt;210 Númenórean years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;58 human years&lt;/span&gt;\")) +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[1]),\n        axis.title.x = element_text(color = clrs[1]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\nFigure 3: A naive estimate of Aragorn’s human ages, assuming he’s a full Númenórean\n\n\n\n\nBut the refugee Númenóreans (the Dúnedain) gradually lost their long-living powers after Númenor was destroyed, so this Númenórean formula doesn’t really apply to Aragorn.\nAccording to supplemental Tolkien writings, the 7th steward of Gondor was the last person to live to 150 years, and by the time of the events of The Lord of the Rings, nobody in Gondor had lived past 100 years since Belecthor II, the 21st steward of Gondor. Denethor II—the tomato-massacring father of Boromir and Faramir—was the 26th steward and took the position 112 years after Belecthor II died. So it had been a long time since anyone had lived that long. With the exception of Aragorn and the other Dúnedan Rangers of the North, all the magic Númenórean power had waned.\nAfter the Ring was destroyed, something seems to have changed, though. Faramir—likely distantly related to the Dúnedain—lived until 120, so something new was in the air at the beginning of the post-Sauron Fourth Age. Aragorn—an actual Dúnedan and descendant of Númenor—made it to 210, but he maybe had some special elf help from Arwen.\nSo given that some of the old Númenórean power seems to have returned (and given that Aragorn was unnaturally long-lived), we can try to figure out the Dúnedan → regular human age conversion three different ways.\n\nArbitrary maximum age\nSince the Númenórean → regular human age line is perfectly linear, we’ll assume that the Dúnedan → regular human age line is also perfectly linear, just scaled down. We’ll start the line at 16 again, but now we’ll pretend that 210 years in Fourth Age Dúnedan years is 100 in human years (Aragorn lived a stunningly long time).\nTo figure out the equation for the new Dúnedan line, we need to figure out the slope. We have two points (\\((0, 16)\\) and \\((210, 100)\\)) that we can use to calculate the slope:\nBy hand:\n\\[\n\\begin{aligned}\n\\text{Slope} &= \\frac{y_2 - y_1}{x_2 - x_1} \\\\\n&= \\frac{100 - 16}{210 - 0} \\\\\n&= \\frac{84}{210} \\\\\n&= 0.4\n\\end{aligned}\n\\]\nOr with R:\n\nfind_slope &lt;- function(point1, point2) {\n  (point2[2] - point1[2]) / (point2[1] - point1[1])\n}\n\nslope &lt;- find_slope(c(0, 16), c(210, 100))\nslope\n## [1] 0.4\n\nThat gives us this equation:\n\\[\n\\text{Normal human years} = 16 + (0.4 \\times \\text{Dúnedan years})\n\\]\nTo make it easier to plot things and plug numbers into the formula, we’ll generate a dataset with a range of possible Dúnedan and regular human ages:\n\nages_dunedain &lt;- tibble(dunedain_age = seq(20, 210, by = 1)) |&gt;\n  mutate(normal_human_age = 16 + slope * dunedain_age)\n\nUsing this equation, we can figure out Aragorn’s normal human age during The Lord of the Rings and when he died:\n\nage_model_dunedain &lt;- lm(normal_human_age ~ dunedain_age, data = ages_dunedain)\n\naugment(age_model_dunedain, newdata = tibble(dunedain_age = c(87, 210))) |&gt;\n  rename(normal_human_age = .fitted)\n## # A tibble: 2 × 2\n##   dunedain_age normal_human_age\n##          &lt;dbl&gt;            &lt;dbl&gt;\n## 1           87             50.8\n## 2          210            100\n\nWhen Aragorn tells Eowyn that he’s 87, that’s actually the equivalent of 51ish. This fits with Tolkien’s writings, since in The Fellowship of the Ring, Aragorn was nearing the prime of life (Tolkien 2012, bk. 1, ch. 10, “Strider”).\nHere’s what that looks like across the whole hypothetical Dúnedan lifespan:\n\n\nCode\nggplot(ages_dunedain, aes(x = dunedain_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[7]) +\n  geom_vline(xintercept = c(87, 210)) +\n  annotate(geom = \"segment\", x = -Inf, xend = 87, y = 50.8, yend = 50.8, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = -Inf, xend = 210, y = 100, yend = 100, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  geom_image(data = tibble(dunedain_age = 87, normal_human_age = 15, \n                           image = \"img/aragorn-alive.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 87, y = 41, \n           label = glue(\"&lt;span style='color:{clrs[3]};'&gt;87 Dúnedan years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;50.8 human years&lt;/span&gt;\")) +\n  geom_image(data = tibble(dunedain_age = 210, normal_human_age = 60, \n                           image = \"img/aragorn-dead.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 210, y = 86, \n           label = glue(\"&lt;span style='color:{clrs[3]};'&gt;210 Dúnedan years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;100 human years&lt;/span&gt;\")) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  coord_cartesian(xlim = c(0, 240), ylim = c(0, 110)) +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\nFigure 4: A better estimate of Aragorn’s human ages, scaling down the full Númenórean range of ages to a more plausible range of Dúnedan ages\n\n\n\n\n\n\nSimulating a bunch of slopes\nDeciding that 210 Dúnedan years was 100 human years was a pretty arbitrary choice. Maybe Aragorn lived to be the equivalent of 90? Or 80? Or 120 like Faramir?\nInstead of choosing one single endpoint, we can simulate the uncertainty around the final age at death. We’ll say that 210 years in Fourth Age-era Dúnedan years is the equivalent of somewhere between 80 and 120 regular human years.\n\n\nCode\n# Generate a bunch of maximum human ages, centered around 100, ± 20ish\nlots_of_slopes &lt;- tibble(max_human_age = rnorm(1000, 100, 10)) %&gt;% \n  # Find the slope of each of these new lines\n  mutate(slope = map_dbl(max_human_age, ~find_slope(c(0, 16), c(210, .x)))) %&gt;% \n  # Generate data for each of the new lines\n  mutate(ages = map(slope, ~{\n    tibble(dunedain_age = seq(20, 210, by = 1)) |&gt;\n      mutate(normal_human_age = 16 + .x * dunedain_age)\n  })) %&gt;% \n  mutate(id = 1:n()) %&gt;% \n  unnest(ages)\n\n\nEach of these simulated lines is a plausible age conversion formula.\n\n\nCode\nlots_of_slopes %&gt;% \n  ggplot(aes(x = dunedain_age, y = normal_human_age)) +\n  geom_line(aes(group = id), method = \"lm\", stat = \"smooth\", alpha = 0.05, color = clrs[5]) +\n  geom_smooth(method = \"lm\", color = clrs[7]) +\n  geom_vline(xintercept = c(87, 210)) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\nFigure 5: Plausible Dúnedan-to-human age conversion equations\n\n\n\n\nAt lower values of Dúnedan ages, there’s a lot less of a range of uncertainty, but as Dúnedan age increases, so too does the range of possible human ages. We can look at the distribution of the predicted normal human ages at both 87 and 210 to get a sense for these ranges:\n\n\nCode\nlots_of_slopes |&gt; \n  filter(dunedain_age %in% c(87, 210)) |&gt; \n  mutate(dunedain_age = glue(\"{dunedain_age} Dúnedan years\"),\n         dunedain_age = fct_inorder(dunedain_age)) |&gt; \n  ggplot(aes(x = normal_human_age, fill = dunedain_age)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], clrs[4]), guide = \"none\") +\n  labs(x = \"Normal human age\", y = \"Density\", fill = NULL) +\n  facet_wrap(vars(dunedain_age), scales = \"free_x\") +\n  theme_numenor() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nFigure 6: Distribution of predicted human ages at 87 and 210 Dúnedan years\n\n\n\n\n\n\nBayesian simulation\nAs a final approach for guessing at Aragorn’s age, we’ll use a Bayesian model to generate a posterior distribution of plausible conversion lines and predicted ages. Technically this isn’t a true posterior—there’s no actual data or anything, so we’ll sample just from the prior distributions that we feed the model. But it’s still a helpful exercise in simulation.\nWe’ll define this statistical model:\n\\[\n\\begin{aligned}\n\\text{Normal human age} &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu &= \\beta_0 + \\beta_1\\ \\text{Dúnedan age} \\\\[10pt]\n\\beta_0 &= 16 \\\\\n\\beta_1 &\\sim \\mathcal{N}(0.4, 0.05) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\nWe fix the intercept at 16 as before, and we say that the slope is around 0.4 ± 0.1ish. We’ll use Stan (through brms) to fit a model based on just these priors:\n\n\nCode\n# Stan likes to work with mean-centered variables, so we'll center dunedain_age\n# here, so that 0 represents 115\nages_dunedain_centered &lt;- ages_dunedain |&gt; \n  mutate(dunedain_age = scale(dunedain_age, center = TRUE, scale = FALSE))\n\n# Set some priors\npriors &lt;- c(\n  prior(constant(16), class = Intercept),  # Constant 16 for the intercept\n  prior(normal(0.4, 0.05), class = b, coef = \"dunedain_age\"),  # Slope of 0.4 ± 0.1\n  prior(exponential(1), class = sigma)\n)\n\n# Run some MCMC chains just with the priors, since we don't have any actual data\nage_model_bayes &lt;- brm(\n  bf(normal_human_age ~ dunedain_age),\n  data = ages_dunedain_centered,\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, backend = \"cmdstanr\",\n  seed = 1234, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\nage_model_bayes\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: normal_human_age ~ dunedain_age \n##    Data: ages_dunedain_centered (Number of observations: 191) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept       16.00      0.00    16.00    16.00   NA       NA       NA\n## dunedain_age     0.40      0.05     0.30     0.50 1.00     2258     2191\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.98     0.02     3.51 1.00     1985     1394\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe results from the model aren’t too surprising, given (1) we’ve seen similar results with the other methods, and (2) the 16 intercept and 0.4 slope match the priors, wince we’re only dealing with priors.\nNow that we have a “posterior” (again, it’s not a true posterior since there’s no actual data), we can play with it in a few different ways. First we can look at the whole range of Dúnedan ages and see lots of plausible slopes. As expected, most are around 0.4, resulting in a final age of 100, but some lines are steeper and some are shallower. Since these are posterior distributions, we can find credible intervals too (which we can interpret much more naturally than convoluted confidence intervals):\n\n\nCode\ndraws_prior &lt;- tibble(dunedain_age = seq(25, 210, 1)) |&gt; \n  add_epred_draws(age_model_bayes, ndraws = 500)\n\ndraws_prior |&gt; \n  ggplot(aes(x = dunedain_age, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.05, color = clrs[5]) +\n  geom_vline(xintercept = c(87, 210)) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\nFigure 7: Spaghetti plot of plausible posterior Dúnedan → human conversions\n\n\n\n\nWe can also look at the posterior distribution of predicted human ages at just 87 and 210, along with credible intervals. There’s a 95% chance that at 87, he’s actually between 42 and 59 (with an average of 51ish), and at 210 he’s actually between 79ish and 121.\n\n\nCode\ndraws_aragorn_ages &lt;- tibble(dunedain_age = c(87, 210)) |&gt; \n  add_epred_draws(age_model_bayes, ndraws = 500)\n\ndraws_aragorn_ages |&gt; \n  group_by(dunedain_age) |&gt; \n  median_hdci(.width = 0.95)\n## # A tibble: 2 × 7\n##   dunedain_age .epred .lower .upper .width .point .interval\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1           87   50.7   41.9   59.2   0.95 median hdci     \n## 2          210   99.8   78.4  120.    0.95 median hdci\n\n\n\n\nCode\ndraws_aragorn_ages |&gt; \n  mutate(dunedain_age = glue(\"{dunedain_age} Dúnedan years\"),\n         dunedain_age = fct_inorder(dunedain_age)) |&gt; \n  ggplot(aes(x = .epred, fill = factor(dunedain_age))) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], clrs[4]), guide = \"none\") +\n  labs(x = \"Normal human age\", y = \"Density\", fill = NULL) +\n  facet_wrap(vars(dunedain_age), scales = \"free_x\") +\n  theme_numenor() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\nFigure 8: Posterior distribution of predicted human ages at 87 and 210 Dúnedan years"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#conclusion",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#conclusion",
    "title": "How old was Aragorn in regular human years?",
    "section": "Conclusion",
    "text": "Conclusion\nGiven all the evidence we have about Númenórean ages, and after making some reasonable assumptions about Dúnedan and human lifespans, when Aragorn tells Eowyn that that he’s 87, that’s really the equivalent of 50ish, with a 95% chance that he’s somewhere between 42 and 59.\n\n\n\nAragorn announcing his actual age"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "",
    "text": "When I started my first master’s degree program in 2008, I decided to stop using Word for all my academic writing and instead use plain text Markdown for everything. Markdown itself had been a thing for 4 years, and MultiMarkdown—a pandoc-like extension of Markdown that could handle BibTeX bibliographies—was brand new. I did all my writing for my courses and my thesis in Markdown and converted it all to PDF through LaTeX using MultiMarkdown. I didn’t know about pandoc yet, so I only ever converted to PDF, not HTML or Word.\nI stored all my bibliographic references in a tiny little references.bib BibTeX file that I managed with BibDesk. BibDesk is a wonderful and powerful program with an active developer community and it does all sorts of neat stuff like auto-filing PDFs, importing references from DOIs, searching for references on the internet from inside the program, and just providing a nice overall front end for dealing with BibTeX files.\nI kept using my MultiMarkdown + LaTeX output system throughout my second master’s degree, and my references.bib file and PDF database slowly grew. R Markdown hadn’t been invented yet and I still hadn’t discovered pandoc, so living in a mostly LaTeX-based world was fine.\nWhen I started my PhD in 2012, something revolutionary happened: the {knitr} package was invented. The new R Markdown format let you to mix R code with Markdown text and create multiple outputs (HTML, LaTeX, and docx) through pandoc. I abandoned MultiMarkdown and fully converted to pandoc (thanks also in part to Kieran Healy’s Plain Person’s Gide to Plain Text Social Science). Since 2012, I’ve written exclusively in pandoc-flavored Markdown and always make sure that I can convert everything to PDF, HTML, and Word (see the “Manuscript” entry in the navigation bar here, for instance, where you can download the preprint version of that paper in a ton of different formats). I recently converted a bunch of my output templates to Quarto pandoc too.\nDuring all this time, I didn’t really keep up with other reference managers. I used super early Zotero as an undergrad back in 2006–2008, but it didn’t fit well with my Markdown-based workflow, so I kind of ignored it. I picked it up again briefly at the beginning of my PhD, but I couldn’t get it to play nicely with R Markdown and pandoc, so I kept using trusty old BibDesk. My references.bib file got bigger and bigger as I took more and more doctoral classes and did more research, but BibDesk handled the growing library just fine. As of today, I’ve got 1,400 items in there with nearly 1,000 PDFs, and everything still works great—mostly."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#my-longstanding-workflow-for-writing-citing-and-pdf-management",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#my-longstanding-workflow-for-writing-citing-and-pdf-management",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "",
    "text": "When I started my first master’s degree program in 2008, I decided to stop using Word for all my academic writing and instead use plain text Markdown for everything. Markdown itself had been a thing for 4 years, and MultiMarkdown—a pandoc-like extension of Markdown that could handle BibTeX bibliographies—was brand new. I did all my writing for my courses and my thesis in Markdown and converted it all to PDF through LaTeX using MultiMarkdown. I didn’t know about pandoc yet, so I only ever converted to PDF, not HTML or Word.\nI stored all my bibliographic references in a tiny little references.bib BibTeX file that I managed with BibDesk. BibDesk is a wonderful and powerful program with an active developer community and it does all sorts of neat stuff like auto-filing PDFs, importing references from DOIs, searching for references on the internet from inside the program, and just providing a nice overall front end for dealing with BibTeX files.\nI kept using my MultiMarkdown + LaTeX output system throughout my second master’s degree, and my references.bib file and PDF database slowly grew. R Markdown hadn’t been invented yet and I still hadn’t discovered pandoc, so living in a mostly LaTeX-based world was fine.\nWhen I started my PhD in 2012, something revolutionary happened: the {knitr} package was invented. The new R Markdown format let you to mix R code with Markdown text and create multiple outputs (HTML, LaTeX, and docx) through pandoc. I abandoned MultiMarkdown and fully converted to pandoc (thanks also in part to Kieran Healy’s Plain Person’s Gide to Plain Text Social Science). Since 2012, I’ve written exclusively in pandoc-flavored Markdown and always make sure that I can convert everything to PDF, HTML, and Word (see the “Manuscript” entry in the navigation bar here, for instance, where you can download the preprint version of that paper in a ton of different formats). I recently converted a bunch of my output templates to Quarto pandoc too.\nDuring all this time, I didn’t really keep up with other reference managers. I used super early Zotero as an undergrad back in 2006–2008, but it didn’t fit well with my Markdown-based workflow, so I kind of ignored it. I picked it up again briefly at the beginning of my PhD, but I couldn’t get it to play nicely with R Markdown and pandoc, so I kept using trusty old BibDesk. My references.bib file got bigger and bigger as I took more and more doctoral classes and did more research, but BibDesk handled the growing library just fine. As of today, I’ve got 1,400 items in there with nearly 1,000 PDFs, and everything still works great—mostly."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#why-switch-away-from-bibtex-and-bibdesk",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#why-switch-away-from-bibtex-and-bibdesk",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Why switch away from BibTeX and BibDesk?",
    "text": "Why switch away from BibTeX and BibDesk?\nBibDesk got me through my dissertation and all my research projects up until now, so why consider switching away to some other system? Over the past few years, as I’ve done more reading on my iPad and worked on more coauthored projects, I’ve run into a few pain points in my citation workflow.\n\nProblem 1: Cross-device reading\nI enjoy reading PDFs on my iPad (particularly in the iAnnotate app), but getting PDFs from BibDesk onto the iPad has always required a bizarre dance:\n\nStore references.bib and the BibDesk-managed folder of PDFs in Dropbox\nUse the References iPad app to open the BibTeX file from Dropbox on the iPad\nUse iAnnotate to navigate Dropbox and find the PDF I want to read\nRead and annotate the PDF in iAnnotate\nSend the finished PDF from iAnnotate back to Dropbox and go back to References to ensure that the annotated PDF updates\n\nI’d often get sick of this convoluted process and just find the PDF on my computer and AirDrop it to my iPad directly, completely circumventing Dropbox. I’d then AirDrop it back to my computer and attach the marked up PDF to the reference in BibDesk. It’s inconvenient, but less inconvenient than bouncing around a bunch of different apps and hoping everything works.\n\n\nProblem 2: Collaboration across many projects with many coauthors\nCollaboration with a single huge references.bib file is impossible. I could share my Dropbox folder with coauthors, but then they’d see all my entries and have access to all my annotated PDFs, which seems like overkill. As I started working with coauthors, I decided to make smaller project-specific .bib files that would be shareable and editable.\nThis is great for project modularity—see how this bibliography.bib file only contains things we cited? But it caused major synchronization problems. If me or a coauthor makes any edits to the project-specific files (adding a DOI to an existing entry, adding a new entry, etc.), those changes don’t show up in my big master references.bib file. I have to remember to copy those changes to the main file, and I never remember. With some recent projects, I’ve actually been copying some entries from previous projects’ .bib files rather than from the big references.bib file. Everything’s diverging and it’s a pain.\n\n\nProblem 3: BibTeX was designed for LaTeX—but just LaTeX\nBibTeX works great with LaTeX. That’s why it was invented in the first place! The fact that things like pandoc work with it is partially a historical accident—.bib files were a convenient and widely used plain text bibliography format, so pandoc and MultiMarkdown used BibTeX for citations.\nBut citations are often more complicated than BibTeX can handle. Consider the LaTeX package biblatex-chicago—in order to be fully compliant with all the intricacies of the Chicago Manual of Style, it has to expand the BibTeX (technically BibLaTeX) format to include fields like entrysubtype for distinguishing between magazine/newspaper articles and journal articles, among dozens of other customizations and tweaks. BibTeX has a limited set of entry types, and anything that’s not one of those types gets shoehorned into the misc type.\nInternally, programs like pandoc that can read BibTeX files convert them into a standard Citation Style Language (CSL) format, which it then uses to format references as Chicago, APA, MLA, or whatever. It would be great to store all my citations in a CSL-compliant format in the first place rather than as a LaTeX-only format that has to be constantly converted on-the-fly when converting to any non-LaTeX output.\n\n\nThe solution: Zotero\nZotero conveniently fixes all these issues:\n\nIt has a synchronization service that works across platforms (including iOS). It can work with Dropbox too if you don’t want to be bound by their file size limit or pay for extra storage, though I ended up paying for storage to (1) support open source software and (2) not have to deal with multiple programs. I’ve been doing the BibDesk → iAnnotate → Dropbox → MacBook → AirDrop dance for too many years—I just want Zotero to handle all the syncing for me.\nIt’s super easy to collaborate with Zotero. You can create shared group libraries with different sets of coauthors and not worry about Dropbox synchronization issues or accidental deletion of } characters in the .bib file. For one of my reading-intensive class, I’ve even created a shared Zotero group library that all the students can join and cite from, which is neat.\nIt’s also far easier to maintain a master list of references. You can create a Zotero collection for specific projects, and items can live in multiple collections. Editing an item in one collection updates that item in all other collections. Zotero treats collections like iTunes/Apple Music playlists—just like songs can belong to multiple playlists, bibliographic entries can belong to multiple collections.\nZotero follows the CSL standard that pandoc uses. It was the first program to adopt CSL (way back in 2006!). It supports all kinds of entry types and fields, beyond what BibTeX supports."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#preparing-for-the-migration",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#preparing-for-the-migration",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Preparing for the migration",
    "text": "Preparing for the migration\nMigrating my big .references.bib file to Zotero was a relatively straightforward process, but it required a few minor shenanigans to get everything working right.\n\nMake a backup\nPreparing everything for migration meant I had to make a ton of edits to the original references.bib file, so I made a copy of it first and worked with the copy.\n\n\nInstall extensions\nTo make Zotero work nicely with a pandoc-centric writing workflow, and to make file management and tag management easier, I installed these three extensions:\n\nBetter BibTeX\nZotFile\nZotero Tag\n\n\n\nRatings and read status\nBibDesk allows you to add a couple extra metadata fields to entries for ratings and to mark them as read. I’ve used these fields for years and find them super useful for keeping track of how much I like articles and for remembering which ones I’ve actually finished.\nInternally, BibDesk stores this data as entries in the raw BibTex:\n@article{the_citekey_for_this_entry,\n    author = {Whoever},\n    title = {Whatever},\n    ...\n    rating = {4},\n    read = {1}}\nThese fields are preserved and transferred to Zotero when you import the file, but they show up in the “Extra” field and aren’t easily filterable or sortable there:\n\n\n\nExtra fields from a BibTeX file\n\n\nI decided to treat these as Zotero tags, which BibDesk calls keywords. I considered making some sort of programmatic solution and writing a script to convert all the rating and read fields to keywords, but that seemed like too much work—many entries have existing keywords and parsing the file and concatenating ratings and read status to the list of keywords would be hard.\nSo instead I sorted all my entries in BibDesk by rating, selected all the 5 star ones and added a zzzzz tag, selected all the 4 star ones and added a zzzz tag, and so on (so that 1 star entries got a z) tag. I then sorted the entries by read status and assigned xxx to all the ones I’ve read. These tag names were just temporary—in Zotero I changed these to emojis (⭐️⭐️⭐️ and ✅), but because I was worried about transferring complex Unicode characters like emojis across programs, I decided to simplify things by temporarily just using ASCII characters.\n\n\nFiles\n\nA note on BibDesk’s stored filename\nBibDesk can autofile attached PDFs and manage their location. To keep track of where the files are, it stores their path as a base64-encoded path in a bdsk-file-N field in the .bib file, like this:\n@article{HeissKelley:2017,\n    author = {Andrew Heiss and Judith G. Kelley},\n    doi = {10.1086/691218},\n    journal = {Journal of Politics},\n    month = {4},\n    number = {2},\n    pages = {732--41},\n    title = {Between a Rock and a Hard Place: International {NGOs} and the Dual Pressures of Donors and Host Governments},\n    volume = {79},\n    year = {2017},\n    bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBcUGFwZXJzL0hlaXNzS2VsbGV5MjAxNyAtIEJldHdlZW4gYSBSb2NrIGFuZCBhIEhhcmQgUGxhY2UgSW50ZXJuYXRpb25hbCBOR09zIGFuZCB0aGUgRHVhbC5wZGZPEQJ8AAAAAAJ8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgQ51QkQAAf////8fSGVpc3NLZWxsZXkyMDE3IC0gI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9T5sk0AAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAHwvOlVzZXJzOmFuZHJldzpEcm9wYm94OlJlYWRpbmdzOlBhcGVyczpIZWlzc0tlbGxleTIwMTcgLSBCZXR3ZWVuIGEgUm9jayBhbmQgYSBIYXJkIFBsYWNlIEludGVybmF0aW9uYWwgTkdPcyBhbmQgdGhlIER1YWwucGRmAA4ArABVAEgAZQBpAHMAcwBLAGUAbABsAGUAeQAyADAAMQA3ACAALQAgAEIAZQB0AHcAZQBlAG4AIABhACAAUgBvAGMAawAgAGEAbgBkACAAYQAgAEgAYQByAGQAIABQAGwAYQBjAGUAIABJAG4AdABlAHIAbgBhAHQAaQBvAG4AYQBsACAATgBHAE8AcwAgAGEAbgBkACAAdABoAGUAIABEAHUAYQBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB6VXNlcnMvYW5kcmV3L0Ryb3Bib3gvUmVhZGluZ3MvUGFwZXJzL0hlaXNzS2VsbGV5MjAxNyAtIEJldHdlZW4gYSBSb2NrIGFuZCBhIEhhcmQgUGxhY2UgSW50ZXJuYXRpb25hbCBOR09zIGFuZCB0aGUgRHVhbC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAgwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMD}}\nZotero doesn’t parse that gnarly field—it needs a field named file—and it doesn’t decode that messy string into a plain text file path, so the attached PDF won’t get imported correctly.\nHowever, thanks to Emiliano Heyns, the Better BibTeX add-on will automatically convert these base64-encoded paths to plain text fields that Zotero can work with just fine. All PDFs will import automatically!\n\n\nCustomizing Zotero’s renaming rules\nI wanted all the PDFs that Zotero would manage to have nice predictable filenames. In BibDesk, I used this pattern:\ncitekey - First few words of title.pdf\nThat’s been fine, but it uses spaces in the file name and doesn’t remove any punctuation or special characters, so it was a little trickier to work with in the terminal or with scripts or for easy consistent searching (especially when searching in the iPad Dropbox app when looking for a PDF to read). But because I set up that pattern in 2008, path dependency kind of locked me in and I’ve been unwilling to change it since.\nSince I’m starting with a whole new reference manager, I figured it was time to adopt a better PDF naming system. In the ZotFile preferences, I set this pattern:\n{%a-}{%y-}{%t}\n…which translates to\nup_to_three_last_names-year-first_few_characters_of_title.pdf\n(see this for a list of all the possible wildcards)\n…with - separating the three logical units (authors, year, title), and _ separating all the words within each unit (which follows Jenny Bryan’s principles of file naming). In practice, the pattern looks like this:\nheiss_kelley-2017-between_a_rock_and_a_hard_place.pdf\nI had to tweak a few other renaming settings too. Here’s the final set of preferences:\n\n\n\nZotFile preferences\n\n\nI wanted to switch the roles of - and _ and do\nheiss-kelley_2017_between-a-rock-and-a-hard-place.pdf\n…but Zotero and/or ZotFile seems to hardwire _ as the space replacement in its titles. Oh well.\n\n\n\nCitekeys\nIn BibDesk, I’ve had a citation key pattern that I’ve used for years: Lastname:Year, with up to three last names for coauthored things, and an incremental lowercase letter in the case of duplicates:\nHeissKelley:2017\nHeissKelley:2017a\nImbens:2021\nLundbergJohnsonStewart:2021\nZotero and Better BibTeX preserve citekeys when you import a .bib file, but I wanted to make sure I keep using this system for new items I add going forward, so I changed the Better BibTeX preferences to use the same pattern:\nauth(0,1) + auth(0,2) + auth(0,3) + \":\" + year\n\n\n\nBetter BibTeX settings"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#post-import-tweaks",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#post-import-tweaks",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Post-import tweaks",
    "text": "Post-import tweaks\nWith all that initial prep work done, I imported the .bib file into my Zotero library (File &gt; Import…). I made sure “Place imported collections and items to new collection” was checked and that files were copied to the Zotero storage folder:\n\n\n\nZotero’s import dialog\n\n\n\nRatings and read status\nThe Tags panel in Zotero then showed all the project/class-specific keywords from BibDesk, in addition to the ratings and read status tags I added previously:\n\n\n\nTags before renaming\n\n\nI renamed each of the zzz* rating tags to use emoji stars and renamed the xxx read tag to use ✅.\n\n\n\nTags after renaming\n\n\nZotero has the ability to assign tags specific colors and pin them in a specific order, which also makes the tags display in the main Zotero library list. Following advice from the Zotero Tag extension, I pinned the read status ✅ tag as the first tag, the 5-star rating as the second tag, the 4-star rating as the third tag, and so on.\nNow the read status and ratings tags are easily accessible and appear directly in the main Zotero library list!\n\n\n\nZotero library with read status and ratings tags\n\n\n\n\nTags to collections\nZotero has two different methods for categorizing entries—tags and collections—while BibDesk / BibTeX only uses keywords, which Zotero treats as tags.\nI decided that in Zotero I’d use both tags and collections. Tags are reserved for things like general topics, ratings, to-read designations, etc., while collections represent specific projects or classes.\nI already assigned project- and class-specific keywords in BibDesk, so I just needed to move those keyworded entries into Zotero collections. There’s no way (that I could find) to include collection information in the .bib file and have it import into Zotero, so I ended up manually creating collections for each of the imported keywords. I filtered the library to only show items from one of the future collections, selected all the items, right-clicked, and chose “Add to collection” &gt; “New collection…” and created a new collection. I then deleted the tag.\nFor instance, here’s what Zotero looked like after I assigned these 6 items, tagged as “Polsci 733”, to the new “Polsci 733” collection (shown in the folder in the sidebar). I just had to delete the tag after:\n\n\n\nExample of the Polsci 733 tag after being converted to a collection\n\n\n\n\nincollection / inbook and crossref\n\n\n\n\n\n\nTip\n\n\n\nThis used to cause problems with child references not importing fields from their parents, but thanks to Emiliano Heynes, this all works flawlessly if you have verison 6.7.47+ of Better BibTeX installed.\n\n\nBibDesk natively supports the crossref field, which biber and biblatex use when working with LaTeX. This field lets you set up child/parent relationships with items, where children inherit fields from their parents. For instance, consider these two items—an edited book with lots of chapters from different authors and a chapter from that book:\n@inbook{El-HusseiniToeplerSalamon:2004,\n    author = {Hashem El-Husseini and Stefan Toepler and Lester M. Salamon},\n    chapter = {12},\n    crossref = {SalamonSokolowski:2004},\n    pages = {227--32},\n    title = {Lebanon}}\n\n@book{SalamonSokolowski:2004,\n    address = {Bloomfield, CT},\n    editor = {Lester M. Salamon and S. Wojciech Sokolowski},\n    publisher = {Kumarian Press},\n    title = {Global Civil Society: Dimensions of the Nonprofit Sector},\n    volume = {2},\n    year = {2004}}\nIn BibDesk, the chapter displays like this:\n\n\n\nBibDesk editor window for a book chapter that inherits its parent book’s attributes\n\n\nFields like book title, publisher, year, etc., are all greyed out because they’re inherited from the parent book, with the citekey SalamonSokolowski:2004\nIf you install version 6.7.47+ of the Better BibTeX add-on, the chapter will inherit all the information from its parent book—the book title, date, publisher, etc., will all be imported correctly:\n\n\n\nCross referenced parent attributes in Zotero are imported correctly\n\n\n\n\nAll done!\nAnd with that, I have a complete version of my 15-year-old references.bib file inside Zotero!\n\n\n\nComplete Zotero library"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#example-workflow-with-quarto-r-markdown-pandoc",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#example-workflow-with-quarto-r-markdown-pandoc",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Example workflow with Quarto / R Markdown / pandoc",
    "text": "Example workflow with Quarto / R Markdown / pandoc\nPart of the reason I’ve been hesitant to switch away from BibDesk for so long is because I couldn’t figure out a way to connect a Markdown document to my Zotero database. With documents that get parsed through pandoc (like R Markdown or Quarto), you add a line in the YAML front matter to specify what file contains your references:\n---\ntitle: Whatever\nauthor: Whoever\nbibliography: references.bib\n---\nSince Zotero keeps everything in one big database, I didn’t see a way to add something like bibliography: My Zotero Database to the YAML front matter—pandoc requires that you point to a plain text file like .bib or .json or .yml, not a Zotero database.\nHowever, the magical Better BibTeX add-on clarified everything for me and makes it super easy to point pandoc at a single file that contains a collection of reference items.\n\nExport collection to .bib file\nFirst, create a collection of items that you want to cite in your writing project. Since collections are like playlists and items can belong to multiple collections, there’s no need to manage duplicate entries or anything (like I was running into with Problem 2 above).\nRight click on the collection name and choose “Export collection…”.\n\n\n\nExporting a Zotero collection\n\n\nChange the format to “Better BibLaTeX”, check “Keep updated”, and choose a place to save the resulting .bib file.\n\n\n\nChanging the export format\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou could also export it as “Better CSL JSON” or “Better CSL YAML”, which would create a .json or .yml file that you could then point to in your YAML front matter, which would keep everything in CSL format instead of converting things to .bib and back again (see Problem 3 above). However, in my academic writing projects I still like to let LaTeX, BibLaTeX, and biber handle the citation generation instead of pandoc for PDFs, so I still rely on .bib files. But if you’re not converting to PDF, or if you’re letting the CSL style template handle the citations instead of BibLaTeX, you should probably keep everything as JSON or YAML instead of .bib.\n\n\nThe “Keep updated” option is the magical part of this whole thing. If you add an item or edit an existing item in the collection in Zotero, Better BibTeX will automatically re-export the collection to the .bib file. You can have one central repository of citations and lots of dynamically updated plain text .bib files that you don’t have to edit or keep track of. Truly magical.\n\n\nPoint the .qmd / .Rmd / .md to the exported file\nYou’ll now have a .bib file that contains all the references that you can cite. Put that filename in your front matter (use .json or .yml if you export the file as JSON or YAML instead):\n---\ntitle: Whatever\nauthor: Whoever\nbibliography: name_of_file_you_exported_from_zotero.bib\n---\n\n\nCite things\nCite things like normal.\nBecause the front matter is pointed at a plain text .bib file that contains all the bibliographic references, it’ll generate the citations correctly. And because Better BibTeX is configured to automatically update the exported plain text file, any changes you make in Zotero will automatically be reflected. Again, this is magic.\n\n\n\nVisual Studio Code with a Quarto Markdown file configured to look at an auto-updating .bib file exported from Zotero\n\n\n\n\nRStudio-based alternative\nAlternatively, if you write in RStudio, you can connect RStudio to your Zotero database and have it do a similar auto-export thing. You can also tell it to use Better BibTeX to keep things automatically synced:\n\n\n\nRStudio preferences pane for enabling Zotero\n\n\n(See here for more details about Zotero citations in RStudio)\nOne extra nice thing about using RStudio is its fancy Insert Citation dialog, which makes adding citations in Markdown just like adding citations in Word or Google Docs. It only works in the Visual Markdown Editor, though, which I don’t normally use, so I just use Better BibTeX alone rather than RStudio’s Zotero connection when I write in RStudio."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "",
    "text": "Pandoc-flavored Markdown makes it really easy to cite and reference things. You can write something like this (assuming you use this references.bib BibTeX file):\nAnd it’ll convert to this after running the document through pandoc:\nThis is all great and ideal when working with documents that have a single bibliography at the end."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#the-limits-of-default-in-text-citations",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#the-limits-of-default-in-text-citations",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "The limits of default in-text citations",
    "text": "The limits of default in-text citations\nSome documents—like course syllabuses and readings lists—don’t have a final bibliography. Instead they have lists of things people should read. However, if you try to insert citations like normal, you’ll get the inline references and a final bibliography:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n\n\n\n\n\n\nRendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\n(Lovelace 1842)\n(Turing 1936)\n\n\nWeek 2\n\n\n(Keynes 1937)\n\nReferences\n\n\nKeynes, John Maynard. 1937. “The General Theory of Employment.” The Quarterly Journal of Economics 51 (2): 209–23.\n\n\nLovelace, Augusta Ada. 1842. “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator.” Taylor’s Scientific Memoirs 3: 666–731.\n\n\nTuring, Alan Mathison. 1936. “On Computable Numbers, with an Application to the Entscheidungsproblem.” Journal of Math 58 (345-363): 230–65.\n\n\n\n\nThe full citations are all in the document, but not in a very convenient location. Readers have to go to the back of the document to see what they actually need to read (especially if there’s a website or DOI URL they need to click on)."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#making-note-based-styles-appear-in-the-text",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#making-note-based-styles-appear-in-the-text",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "Making note-based styles appear in the text",
    "text": "Making note-based styles appear in the text\nIt would be great if the full citation could be included in the lists in the document instead of at the end of the document.\nAnd it’s possible, with just a minor tweak to the Citation Style Language (CSL) style file that you’re using (thanks to adam.smith at StackOverflow for pointing out how).\nBy default pandoc uses Chicago author-date for bibiliographic references—hence the (Lovelace 1842) style of references. You can download any other CSL file from Zotero’s searchable style repository, from the Citation Styles project’s searchable list, or clone the full massive GitHub repository of styles to find others, like Chicago notes, APA, MLA, and so on.\nThe easiest way to get full citations inline is to find a CSL that uses note-based citations, like the Chicago full note style and edit the CSL file to tell it to be an inline style instead of a note style.\nThe second line of all CSL files contains a &lt;style&gt; XML element with a class attribute. Inline styles like APA and Chicago author date have class=\"in-text\":\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (author-date)&lt;/title&gt;\n    ...\n…while note-based styles like Chicago notes have class=\"note\":\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"note\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (full note)&lt;/title&gt;\n    ...\nIf you download a note-based CSL style and manually change it to be in-text, the footnotes that it inserts will get inserted in the text itself instead of as foonotes.\nHere I downloaded Chicago full note, edited the second line to say class=\"in-text\", and saved it as chicago-syllabus.csl:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (full note, but in-text)&lt;/title&gt;\n    ...\nI can then tell pandoc to use that CSL when rendering the document:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\ncsl: chicago-syllabus.csl\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n…and the full references are included in the document itself!\n\n\n\n\n\n\nRendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\nAugusta Ada Lovelace, “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator,” Taylor’s Scientific Memoirs 3 (1842): 666–731.\nAlan Mathison Turing, “On Computable Numbers, with an Application to the Entscheidungsproblem,” Journal of Math 58, no. 345-363 (1936): 230–65.\n\n\nWeek 2\n\n\nJohn Maynard Keynes, “The General Theory of Employment,” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23.\n\nReferences\n\n\nKeynes, John Maynard. “The General Theory of Employment.” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23.\n\n\nLovelace, Augusta Ada. “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator.” Taylor’s Scientific Memoirs 3 (1842): 666–731.\n\n\nTuring, Alan Mathison. “On Computable Numbers, with an Application to the Entscheidungsproblem.” Journal of Math 58, no. 345-363 (1936): 230–65."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#a-few-minor-tweaks-to-perfect-the-output",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#a-few-minor-tweaks-to-perfect-the-output",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "A few minor tweaks to perfect the output",
    "text": "A few minor tweaks to perfect the output\nThis isn’t quite perfect, though. There are three glaring problems with this:\n\nWe have a bibliography at the end, since Chicago notes-bibliography requires it. This makes sense for regular documents where you have footnotes throughout the body of the text with a list of references at the end, but it’s not necessary here.\nThe in-text references all have hyperlinks to their corresponding references in the final bibliography. We don’t need those since the linked text is the bibliography.\nIf you render this in Quarto, you get helpful popups that contain the full reference when you hover over the link. But again, the link is the full reference, so that extra hover information is redundant.\n\n\n\n\nCitation reference hovering popup\n\n\nAll these problems are easy to fix with some additional YAML settings that suppress the final bibliography, turn off citation links, and disable Quarto’s hovering:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\ncsl: chicago-syllabus.csl\nsuppress-bibliography: true\nlink-citations: false\ncitations-hover: false\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\nPerfect!\n\n\n\n\n\n\nPerfect final rendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\nAugusta Ada Lovelace, “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator,” Taylor’s Scientific Memoirs 3 (1842): 666–731.\nAlan Mathison Turing, “On Computable Numbers, with an Application to the Entscheidungsproblem,” Journal of Math 58, no. 345-363 (1936): 230–65.\n\n\nWeek 2\n\n\nJohn Maynard Keynes, “The General Theory of Employment,” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#using-other-styles",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#using-other-styles",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "Using other styles",
    "text": "Using other styles\nThis is all great and super easy if you (like me) are fond of Chicago. What if you want to use APA, though? Or MLA? Or any other style that doesn’t use footnotes?\nFor APA, you’re in luck! There’s an APA (curriculum vitae) CSL style that you can use, and you don’t need to edit it beforehand—it just works:\n---\ntitle: \"Some course syllabus with APA\"\nbibliography: references.bib\ncsl: apa-cv.csl\nsuppress-bibliography: true\nlink-citations: false\ncitations-hover: false\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n\n\n\n\n\n\nFinal rendered document using APA CV\n\n\n\n\nSome course syllabus with APA\n\n\nCourse schedule\n\n\nWeek 1\n\n\nLovelace, A. A. (1842). Sketch of the analytical engine invented by Charles Babbage, by LF Menabrea, officer of the military engineers, with notes upon the memoir by the translator. Taylor’s Scientific Memoirs, 3, 666–731.\nTuring, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. Journal of Math, 58(345-363), 230–265.\n\n\nWeek 2\n\n\nKeynes, J. M. (1937). The general theory of employment. The Quarterly Journal of Economics, 51(2), 209–223.\n\n\n\nFor any other style though, you’re (somewhat) out of luck. The simple trick of switching class=\"note\" to class=\"in-text\" doesn’t work if the underlying style is already in-text like APA or Chicago author-date. You’d have to do some major editing and rearranging in the CSL file to force the bibliography entries to show up as inline citations, which goes beyond my skills.\nAs a workaround you can use the {RefManageR} package in R to read the bibliography file with R and output the bibliography part of the citations as Markdown. Steve Miller has a helpful guide for this here."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html",
    "title": "Making Middle Earth maps with R",
    "section": "",
    "text": "I’ve taught a course on data visualization with R since 2017, and it’s become one of my more popular classes, especially since it’s all available asynchronously online with hours of Creative Commons-licensed videos and materials. One of the most popular sections of the class (as measured by my server logs and by how often I use it myself) is a section on GIS-related visualization, or how to work with maps in {ggplot2}. Nowadays, since the advent of the {sf} package, I find that making maps with R is incredibly easy and fun.\nI’m also a huge fan of J. R. R. Tolkien and his entire Legendarium (as evidenced by my previous blog post here simulating Aragorn’s human-scale age based on an obscure footnote in Tolkien’s writings about Númenor).\nBack in 2020, as I was polishing up my data visualization course page on visualizing spatial data, I stumbled across a set of shapefiles for Middle Earth, meaning that it was possible to use R and ggplot to make maps of Tolkien’s fictional world. I whipped up a quick example and tweeted about it back then, but then kind of forgot about it.\nWith Twitter dying, and with my recent read of The Fall of Númenor, Middle Earth maps have been on my mind again, so I figured I’d make a more formal didactic blog post about how to make and play with these maps. So consider this blog post a fun little playground for learning more about doing GIS work with {sf} and ggplot, and learn some neat data visualization tricks along the way.\nLet’s put the R in “J. R. R.”"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#getting-geographic-data",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#getting-geographic-data",
    "title": "Making Middle Earth maps with R",
    "section": "Getting geographic data",
    "text": "Getting geographic data\nShapefiles are everywhere. They’re one of the de facto standard formats for GIS data, and most government agencies provide them for their jurisdictions (see here for a list of some different sources). You can view and edit them graphically with the free and open source QGIS or with the expensive and industry-standard ArcGIS.\nWe’ve already seen how to load shapefiles into R with sf::read_sf(), and that works great. But doing that requires that you go and find and download the shapefiles that you want, which can involve hunting through complicated websites. There are also lots of different R packages that let you get shapefiles directly from different websites’ APIs.\nFor example, we’ve already loaded the 2022 US Census maps by downloading and unzipping the shapefile and using read_sf(). We could have also used the {tigris} package to access the data directly from the Census, like this:\n\n\nCode\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;% \n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\n\nFor world-level data, Natural Earth has incredibly well-made shapefiles. We could download the 1:50m cultural data from their website, unzip it, and load it with read_sf():\n\n\nCode\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;% \n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n\nOr we can use the {rnaturalearth} package to do the same thing:\n\n\nCode\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;% \n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThroughout this post, I use {rnaturalearth} for world-level shapefiles and downloaded shapefiles for the US, but that’s just for the sake of illustration. Both can be done with packages or through downloading.\n\n\nAnd finally, for fun, here are some examples of different maps and projections and ggplot tinkering. I’m perpetually astounded by how easy it is to plot GIS data with geom_sf()! That geometry list column is truly magical.\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- ggplot() + \n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  labs(title = \"NAD83 projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() + \n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  labs(title = \"Albers projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"EPSG:3395\")) +  # Mercator\n  labs(title = \"Mercator projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  labs(title = \"Robinson projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\nExamples of different North American and world map projections"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-the-united-states",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-the-united-states",
    "title": "Making Middle Earth maps with R",
    "section": "In the United States",
    "text": "In the United States\nFirst, let’s stick a scaled-up version of Middle Earth in the United States. For fun, we’ll put the Shire in the geographic center of the US, and we’ll calculate the coordinates for that with R just to show that it’s possible.\nCurrently we have a dataset with 49 rows (48 states + DC). We can use the st_centroid() function to find the center of geographic areas, but if we use it on our current data, we’ll get 49 separate centers. So instead, we’ll melt all the states into one big geographic shape with group_by() and summarize() (using summarize() on the geometry column in an sf dataset combines the geographic areas), and then use st_centroid() on that:\n\n\nCode\n# Melt the lower 48 states into one big shape first, then use st_centroid()\nus_dissolved &lt;- lower_48 %&gt;% \n  mutate(country = \"US\") %&gt;%  # Create new column with the country name \n  group_by(country) %&gt;%  # Group by that country name column\n  summarize()  # Collapse all the geographic data into one big blob\nus_dissolved\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -125 ymin: 24.5 xmax: -66.9 ymax: 49.4\n## Geodetic CRS:  WGS 84\n## # A tibble: 1 × 2\n##   country                                                                                geometry\n##   &lt;chr&gt;                                                                        &lt;MULTIPOLYGON [°]&gt;\n## 1 US      (((-68.9 43.8, -68.9 43.8, -68.8 43.8, -68.9 43.9, -68.9 43.9, -68.9 43.8)), ((-71.6...\n\nus_center &lt;- us_dissolved %&gt;% \n  st_geometry() %&gt;%  # Extract the geometry column\n  st_centroid()  # Find the center\nus_center\n## Geometry set for 1 feature \n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -99 ymin: 39.8 xmax: -99 ymax: 39.8\n## Geodetic CRS:  WGS 84\n## POINT (-99 39.8)\n\n\nAccording to these calculations, the center of the contiguous US is 39.751441°N -98.965620°W. Technically that’s not 100% correct—the true location is at 39.833333°N -98.583333°W, but this is close enough (according to Google, it’s 25 miles off). I’m guessing the discrepancy is due to differences in the shapefile—I’m not using the highest resolution possible, and there might be islands I need to account for (or not account for). Who knows.\nHere’s where that is. I’m using the {leaflet} package just for fun here (this post is a showcase of different R-based GIS things, so let’s showcase!):\n\n\nCode\nus_center_plot &lt;- us_dissolved %&gt;% \n  st_centroid() %&gt;% \n  mutate(fancy_coords = format_coords(geometry)) %&gt;% \n  mutate(label = glue(\"&lt;span style='display: block; text-align: center;'&gt;&lt;strong&gt;Roughly of the center of the contiguous US&lt;/strong&gt;\",\n                      \"&lt;br&gt;{fancy_coords}&lt;/span&gt;\"))\n\nleaflet(us_center_plot) %&gt;% \n  setView(lng = st_geometry(us_center_plot)[[1]][1], \n          lat = st_geometry(us_center_plot)[[1]][2], \n          zoom = 4) %&gt;%\n  addTiles() %&gt;% \n  addCircleMarkers(label = ~htmltools::HTML(label),\n                   labelOptions = labelOptions(noHide = TRUE, \n                                               direction = \"top\", \n                                               textOnly = FALSE))\n\n\n\n\n\n\nNext, we need to transform the Middle Earth data so that it fits on the US map. We need to do a few things to make this work:\n\nDouble all the distances so they match Real World miles\nChange the projection of each of the Middle Earth-related datasets to match the projection of lower_48, or WGS 84, or EPSG:4326\nShift the Middle Earth-related datasets so that Hobbiton aligns with the center of the US.\n\nChanging the projection of an {sf}-enabled dataset is super easy with st_transform(). Let’s first transform the CRS for the Hobbiton coordinates:\n\n\nCode\nhobbiton_in_us &lt;- hobbiton %&gt;% \n  st_transform(st_crs(lower_48))\n\nhobbiton_in_us %&gt;% st_geometry()\n## Geometry set for 1 feature \n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 3.15 ymin: 9.44 xmax: 3.15 ymax: 9.44\n## Geodetic CRS:  WGS 84\n## POINT (3.15 9.44)\n\n\nNote how the coordinates are now on the decimal degrees scale (3.15, 9.44) instead of the meter scale (515948, 1043820). That’s how the US map is set up, so now we can do GIS math with the two maps.\nNext, we need to calculate the offset from the center of the US and Hobbiton by finding the difference between the two sets of coordinates:\n\n\nCode\nme_to_us &lt;- st_coordinates(us_center) - st_coordinates(hobbiton_in_us)\nme_to_us\n##         X    Y\n## [1,] -102 30.3\n\n\nNow we can use that offset to redefine the geometry column in any Middle Earth-related {sf}-enabled dataset we have. Here’s the process for the places data—it’ll be the same for any of the other shapefiles.\n\n\nCode\nme_places_in_us &lt;- places %&gt;% \n  # Make the Middle Earth data match the US projection\n  st_transform(st_crs(lower_48)) %&gt;%\n  # Just look at a handful of places\n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\")) %&gt;% \n  # Double the distances\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_us)) * 2 + st_geometry(hobbiton_in_us)) %&gt;% \n  # Shift everything around so that Hobbiton is in the center of the US\n  st_set_geometry(st_geometry(.) + me_to_us) %&gt;% \n  # All the geometry math made us lose the projection metadata; set it again\n  st_set_crs(st_crs(lower_48))\n\n\nWe can now stick this US-transformed set of place locations insde a map of the US. (Note the ±70000 values for nudging. I have no idea what scale these are on—they’re not meters or miles (maybe feet? maybe decimal degrees?). I had to tinker with different values until it looked okay.)\n\n\nCode\nggplot() + \n  geom_sf(data = lower_48, fill = \"#FF851B\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = me_places_in_us) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()\n\n\n\n\n\nMiddle Earth locations placed in the US\n\n\n\n\nAssuming the Shire is in the middle of Kansas, Rivendell would be near the Mississippi River in Missouri. Rohan is down in southern Arkansas, while Gondor is in southern Alabama.\nWe could be even fancier and reshift all the Middle Earth shapefiles to fit in the US, and then overlay all of Middle Earth on the US, but I won’t do that here. I’ll just stick the coastline on so we can compare the relative sizes of the US and Middle Earth:\n\n\nCode\ncoastline_in_us &lt;- coastline %&gt;% \n  st_transform(st_crs(lower_48)) %&gt;%\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_us)) * 2 + st_geometry(hobbiton_in_us)) %&gt;% \n  st_set_geometry(st_geometry(.) + me_to_us) %&gt;% \n  st_set_crs(st_crs(lower_48))\n\nggplot() + \n  geom_sf(data = lower_48, fill = \"#FF851B\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = coastline_in_us, linewidth = 0.25) +\n  geom_sf(data = me_places_in_us) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()\n\n\n\n\n\nMiddle Earth locations and borders placed in the US"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-europe",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-europe",
    "title": "Making Middle Earth maps with R",
    "section": "In Europe",
    "text": "In Europe\nSticking Middle Earth in the US makes sense because I live in the US, so these relative distances are straightforward to me. (I’m in Georgia, which is the middle of Mordor in the maps above).\nBut Tolkien was from England and lived in Oxford—at 20 Northmoor Road to be precise, or at 51.771004°N -1.260142°W to be even more precise (I found this by going to Google Maps, right clicking on Tolkien’s home, and copying the coordinates). Here’s where that is:\n\n\nCode\ntolkien_home &lt;- tribble(\n  ~place, ~lat, ~long,\n  \"Tolkien's home\", 51.771003605142724, -1.2601418874304429\n) %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\")) \n\ntolkien_home_plot &lt;- tolkien_home %&gt;% \n  mutate(fancy_coords = format_coords(geometry)) %&gt;% \n  mutate(label = glue(\"&lt;span style='display: block; text-align: center;'&gt;&lt;strong&gt;{place}&lt;/strong&gt;\",\n                      \"&lt;br&gt;{fancy_coords}&lt;/span&gt;\"))\n\nleaflet(tolkien_home_plot) %&gt;% \n  setView(lng = st_geometry(tolkien_home_plot)[[1]][1], \n          lat = st_geometry(tolkien_home_plot)[[1]][2], \n          zoom = 14) %&gt;%\n  addTiles() %&gt;% \n  addCircleMarkers(label = ~htmltools::HTML(label),\n                   labelOptions = labelOptions(noHide = TRUE, \n                                               direction = \"top\", \n                                               textOnly = FALSE))\n\n\n\n\n\n\nWe can put Hobbiton in Tolkien’s home and then see the relative distances to the rest of Middle Earth from Oxford.\nWe’ll use the Natural Earth data that we loaded at the beginning of this post. We could theoretically filter it to only look at European countries, since it includes a column for continent, but doing so causes all sorts of issues:\n\nRussia is huuuuge\nFrench Guiana is officially part of France, so the map includes a part of South America\nOther countries like Denmark, Norway, and the UK have similar overseas province-like territories, so the map gets even more expanded\n\n\n\nCode\neurope &lt;- world_map %&gt;% \n  filter(continent == \"Europe\")\n\nggplot() +\n  geom_sf(data = europe)\n\n\n\n\n\nUgly map of all European countries\n\n\n\n\nWe could do some fancy filtering and use more detailed data that splits places like France into separate subdivisions (i.e. one row for continental Europe France, one row for French Guiana, etc.), but that’s a lot of work. So instead, we’ll use coord_sf() to define a window so we can zoom in on just a chunk of Europe. Before, we added some arbitrary number of miles around the coordinates for Hobbiton. This time we’ll use a helpful tool from OpenStreetMap that lets you draw a bounding box on a world map to get coordinates to work with:\n\n\n\n\n\nOpenStreetMap’s site for exporting bounding box coordinates\n\n\n\n\nWe can then create a little matrix of coordinates. We’re ultimately going to use the PTRA08 / LAEA Europe projection, which is centered in Portugal and is a good Europe-centric projection, so we’ll convert the list of coordinates to that projection.\n\n\nCode\neurope_window &lt;- st_sfc(\n  st_point(c(-12.4, 29.31)),  # left (west), bottom (south)\n  st_point(c(44.74, 64.62)),  # right (east), top (north)\n  crs = st_crs(\"EPSG:4326\")   # WGS 84\n) %&gt;% \n  st_transform(crs = st_crs(\"EPSG:5633\")) %&gt;%  # LAEA Europe, centered in Portugal\n  st_coordinates()\neurope_window\n##            X       Y\n## [1,] 2135398 1019399\n## [2,] 5912220 5020959\n\n\nNow we can plot the full world map data and use coord_sf() to limit it to just this window:\n\n\nCode\nggplot() +\n  geom_sf(data = world_map) +\n  coord_sf(crs = st_crs(\"EPSG:5633\"),\n           xlim = europe_window[, \"X\"],\n           ylim = europe_window[, \"Y\"],\n           expand = FALSE)\n\n\n\n\n\nWorld map cropped to just show part of Europe\n\n\n\n\nNeat. Now that we know how to zoom in on Europe, we can go through the same process we did with the US—we’ll convert the Middle Earth shapefiles to the European projection, center Hobbiton on Tolkien’s home in Oxford, double all the distances, and shift everything around.\n\n\nCode\n# Convert the Tolkien home coordinates to European coordinates\ntolkien_home &lt;- tolkien_home %&gt;% \n  st_transform(crs = st_crs(\"EPSG:5633\"))\n\n# Convert the Hobbiton coordinates to European coordinates\nhobbiton_in_europe &lt;- hobbiton %&gt;% \n  st_transform(st_crs(\"EPSG:5633\"))\n\n# Find the offset between Tolkien's home and Hobbiton\nme_to_europe &lt;- st_coordinates(tolkien_home) - st_coordinates(hobbiton_in_europe)\n\nme_places_in_europe &lt;- places %&gt;% \n  # Make the Middle Earth data match the Europe projection\n  st_transform(st_crs(\"EPSG:5633\")) %&gt;%\n  # Just look at a handful of places\n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\")) %&gt;% \n  # Double the distances\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_europe)) * 2 + st_geometry(hobbiton_in_europe)) %&gt;% \n  # Shift everything around so that Hobbiton is in Oxford\n  st_set_geometry(st_geometry(.) + me_to_europe) %&gt;% \n  # All the geometry math made us lose the projection metadata; set it again\n  st_set_crs(st_crs(\"EPSG:5633\"))\n\ncoastline_in_europe &lt;- coastline %&gt;% \n  st_transform(st_crs(\"EPSG:5633\")) %&gt;%\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_europe)) * 2 + st_geometry(hobbiton_in_europe)) %&gt;% \n  st_set_geometry(st_geometry(.) + me_to_europe) %&gt;% \n  st_set_crs(st_crs(\"EPSG:5633\"))\n\n\n\n\nCode\nggplot() + \n  geom_sf(data = world_map, fill = \"#39CCCC\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = coastline_in_europe, linewidth = 0.25) +\n  geom_sf(data = me_places_in_europe) +\n  geom_sf_label(data = filter(me_places_in_europe, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_europe, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"EPSG:5633\"),\n           xlim = europe_window[, \"X\"],\n           ylim = europe_window[, \"Y\"],\n           expand = FALSE) +\n  theme_void()\n\n\n\n\n\nMiddle Earth locations and borders placed in Europe\n\n\n\n\nWith Hobbiton in Oxford, Rivendell is in north central Germany (near Hanover?), with Rohan in Switzerland and Gondor on the border of Croatia and Bosnia."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#paths",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#paths",
    "title": "Making Middle Earth maps with R",
    "section": "Paths",
    "text": "Paths\nIt would be really cool to be able to plot the pathways different characters took in each of the books (Bilbo and Thorin’s company; Frodo and Sam; Aragorn, Legolas, and Gimli, etc.). This data exists! The LOTR Project has detailed maps with the pathways of all of the main characters’ journeys. However, it’s not (as far as I can tell) open source or Creative Commons-licensed, and I don’t think the coordinates are directly comparable to the shapefiles from the ME-GIS project. Alas."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#first-and-second-ages",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#first-and-second-ages",
    "title": "Making Middle Earth maps with R",
    "section": "First and Second Ages",
    "text": "First and Second Ages\nIn addition to The Lord of the Rings and The Hobbit, Tolkien wrote a ton about other ages in Middle Earth (see this super quick crash course in the Ages of Arda from my post on Númenorean ages). A separate mapping project—Arda-Maps—has shapefiles for all three ages of Tolkien’s world, including Valinor, Beleriand, and Númenor.\nHowever, the maps aren’t as detailed as the ME-GIS project, and they’re on a completely different scale. For example, here’s the island of Númenor (featured in Amazon’s The Rings of Power). I downloaded the shapefiles from their GitHub repository—the Second Age shapefiles are buried in QGIS/second age/arda2\nHere I use st_bbox() to create a bounding box of coordinates that I then use to crop the underlying data. This is different from what we did with Europe, where we plotted the whole world map and then zoomed in on just a chunk of western Europe. Here, st_crop() cuts out the geographic data that doesn’t fall within the box (similar to filtering).\n\n\nCode\nnumenor_box &lt;- st_bbox(c(xmin = 0.007, xmax = 0.017, ymin = -0.025, ymax = -0.015))\n\nnumenor_outlines &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/poly_outline.shp\") %&gt;% \n  filter(name == \"Numenor\")\n\nnumenor_rivers &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/line_river.shp\") %&gt;% \n  st_crop(numenor_box)\n\nnumenor_cities &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/point_city.shp\") %&gt;% \n  st_crop(numenor_box)\n\nggplot() +\n  geom_sf(data = numenor_outlines, fill = \"#F2CB9B\") +\n  geom_sf(data = numenor_rivers, linewidth = 0.4, color = clr_blue) +\n  geom_sf(data = numenor_cities) +\n  # Use geom_label_repel with the geometry column!\n  ggrepel::geom_label_repel(\n    data = numenor_cities, \n    aes(label = eventname, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234,\n    family = \"Overpass ExtraBold\") +\n  annotation_scale(location = \"tl\", bar_cols = c(\"grey30\", \"white\"),\n                   text_family = \"Overpass\",\n                   unit_category = \"imperial\") +\n  annotation_north_arrow(\n    location = \"tl\", pad_y = unit(1.5, \"lines\"),\n    style = north_arrow_fancy_orienteering(fill = c(\"grey30\", \"white\"), \n                                           line_col = \"grey30\",\n                                           text_family = \"Overpass\")) +\n  labs(title = \"Númenor\") +\n  theme_void() +\n  theme(plot.background = element_rect(fill = clr_yellow),\n        plot.title = element_text(family = \"Aniron\", size = rel(2), \n                                  hjust = 0.02))\n\n\n\n\n\nFancy map of Númenor\n\n\n\n\nThe map looks fantastic! But notice the scale bar in the top left corner—in this data, Númenor is only a couple thousand feet wide—less than half a mile. The distances are all way wrong. I could probably scale it up by comparing the projection distances in the Arda Maps’ version of regular Middle Earth with the ME-GIS project’s version of regular Middle Earth and then do some fancy math, but that goes beyond my skills."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "",
    "text": "In a couple days, I’m going to drive across the country to Utah, my home state. I haven’t been out west with my whole family in four years—not since 2019 when we moved from Spanish Fork, Utah to Atlanta, Georgia. According to Google Maps, it’s a mere 1,900 miles (3,000 kilometers) through the middle of the United States and should take 28 hours, assuming no stops.\nFor bonus fun though, my wife and I decided to make this trip even more adventurous and hit as many exciting stops as possible. So rather than drive through the middle of Kansas, we’re going along the southern border on the way there, visiting New Orleans, Louisiana; San Antonio, Texas; Carlsbad Caverns, New Mexico; the Grand Canyon in Arizona; and then camping for a couple days at Capitol Reef National Park in Southern Utah before finally arriving at my aunt’s house in Spanish Fork, Utah. On the way home, we’re going across the northern part of the United States, visiting my sister in Idaho before heading towards Yellowstone in Wyoming; Devil’s Tower in Wyoming; Mount Rushmore in South Dakota; and Nauvoo, Illinois (Mormons!). It’s going to be an awesome—but way longer—mega road trip.\nTo help keep six kids entertained beyond just plugging them into iPads, my wife made some road trip journals to make the experience more memorable and educational, and we’re bringing a bunch of books and extra materials about each of the stops we’ll be making so they can do research along the way.\nI just finished revamping my online data visualization class this week since the summer semester is starting next week, so visualizing data through maps has been on my mind. In my session on visualizing maps, I added a section on making and visualizing your own geocoded data (i.e. plotting specific cities on a map), so I figured it would be neat to visualize this huge road trip somehow to make some maps to include in the kids’ journals.\nGetting the latitude and longitude coordinates for specific cities or addresses is super easy—you can either right click on Google Maps and copy specific coordinates, or you can automate it with the {tidygeocoder} package in R.\nGetting route coordinates is a lot trickier though, since it involves all sorts of complex algorithms (accounting for road locations, speed limits, traffic, etc.). This is why Google Maps is so invaluable—it does an excellent job of figuring this out. But Google’s data is proprietary.\nThere’s an R package named {mapsapi} that makes it really easy to get data from the Google Maps API, and if you get an API key from Google (following these instructions), you can extract geocoded {sf}-compatible route data from Google with the mp_directions() function. It’s neat and quick and easy, but it’s expensive. When I first started playing with the maps API in R, I racked up $0.50 in charges just with testing a bunch of routing options. The Google Maps API gives you $200 in free credits every month and I was well below that amount, but it still was a little nervewracking to see that it was that expensive in just an hour of tinkering. Also, setting up the API key was fairly complicated (creating a Google Cloud project, setting up billing, enabling specific services, storing the API key securely on my computer, etc.), and I don’t want to go through that hassle in the future.\nI’m a fan of the OpenStreetMap project, and it’s one of the backends (through Nominatim) for {tidygeocoder} (and I used it in a previous blog post showing some coordinates in interactive Leaflet maps). In searching around the internet for open source alternatives to the Google Maps API, I discovered that OpenStreetMap can also do directions and routing through the Open Source Routing Machine (OSRM) project. You can use OSRM’s public demo server for small-scale routing things, or you can install your own local instance through Docker. And super conveniently, there’s also an R package called {osrm} for accessing the OSRM API. This means that it’s possible to pull geocoded routing and directions data into R in an open source (and free!) way.\nSo let’s see how to use {osrm} and make some neat road trip maps with {sf} and {ggplot2}!"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#who-this-post-is-for",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#who-this-post-is-for",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Who this post is for",
    "text": "Who this post is for\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly {dplyr} and {ggplot2}).\nYou’re somewhat familiar with {sf} for working with geographic data. I have a whole tutorial here and a simplified one here and the {sf} documentation has a ton of helpful vignettes and blog posts, and there are also two free books about it: Spatial Data Science and Geocomputation with R. Also check this fantastic post out to learn more about the anatomy of a geometry column with {sf}."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#packages-and-functions",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#packages-and-functions",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Packages and functions",
    "text": "Packages and functions\nBefore officially getting started, let’s load all the packages we need and create some helpful functions and variables:\n\n\nCode\nlibrary(tidyverse)     # ggplot, dplyr, and friends\nlibrary(sf)            # Handle spatial data in R in a tidy way\nlibrary(tigris)        # Access geographic data from the US Census\nlibrary(tidygeocoder)  # Automated geocoding\nlibrary(osrm)          # Access OSRM through R\nlibrary(ggrepel)       # Nicer non-overlapping labels\nlibrary(glue)          # Easier string interpolation\nlibrary(scales)        # Nicer labeling functions\nlibrary(patchwork)     # Combine plots nicely\nlibrary(ggspatial)     # Nicer map features like scale bars\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Overpass\ntheme_roadtrip &lt;- function() {\n  theme_void(base_family = \"Overpass Light\") +\n    theme(\n      plot.title = element_text(family = \"Overpass\", face = \"bold\", hjust = 0.5),\n      strip.text = element_text(\n        family = \"Overpass ExtraBold\", face = \"plain\",\n        size = rel(1.1), hjust = 0.5)\n    )\n}\n\n# Make labels use Overpass by default\nupdate_geom_defaults(\"label_repel\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\nupdate_geom_defaults(\"label\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\n\nupdate_geom_defaults(\"text_repel\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\nupdate_geom_defaults(\"text\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\n\n# Yellowstone colors\nclrs &lt;- NatParksPalettes::natparks.pals(\"Yellowstone\")\n\n\n\n\nCode\n#' Format duration in minutes and hours\n#'\n#' This function takes a numeric input \\code{x} representing a duration in minutes,\n#' rounds it to the nearest 15 minutes, and formats the result as a string\n#' indicating the number of hours and minutes in the duration.\n#'\n#' @param x A numeric input representing a duration in minutes.\n#' @return A character vector of formatted duration strings.\n#' @examples\n#' fmt_duration(c(93, 1007, 3056))\nfmt_duration &lt;- function(x) {\n  # Round to the nearest 15 minutes\n  n_seconds &lt;- round(seconds(x * 60) / (15 * 60)) * (15 * 60)\n  n_seconds &lt;- seconds_to_period(n_seconds)\n  \n  out &lt;- map_chr(n_seconds, \\(n) {\n    if (seconds(n) &lt;= 59) {\n      # If this is less than an hour, don't format anything with hours\n      glue(\"{MM} minutes\", MM = minute(n))\n    } else {\n      # I only want to format this as a number of hours. If the duration is\n      # longer than 24 hours, seconds_to_period() rolls over into days (i.e.\n      # seconds_to_period(60 * 60 * 24) returns \"1d 0H 0M 0S\"), and it shows\n      # zero hours. So we extract the day part of the period, multiply it by 24,\n      # and add it to the hour component that we want to display\n      extra_day_hours &lt;- day(n) * 24\n  \n      glue(\"{HH} hour{s} {MM} minutes\",\n        HH = scales::label_comma()(hour(n) + extra_day_hours),\n        MM = minute(n),\n        s = ifelse(hour(n) == 1, \"\", \"s\")\n      )\n    }\n  })\n  \n  return(out)\n}\n\nfmt_miles &lt;- scales::label_number(accuracy = 10, suffix = \" miles\", big.mark = \",\")\n\nmiles_to_meters &lt;- function(x) {\n  x * 1609.344\n}\n\nmeters_to_miles &lt;- function(x) {\n  x / 1609.344\n}\n\nkm_to_miles &lt;- function(x) {\n  meters_to_miles(x * 1000)\n}"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-state-highlighting",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-state-highlighting",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Fixing state highlighting",
    "text": "Fixing state highlighting\nFirst we need to change how we identify the states we’ll cross through. Before, we kept the unique state names, but this stripped away the details about the direction of the trip, so we’ll find all the distinct combinations of direction and state and then join that little dataset to lower_48 to make the visited column:\n\n\nCode\nstates_crossed_through_better &lt;- st_intersection(\n  st_transform(lower_48, st_crs(routes_geocoded)),\n  routes_geocoded\n) %&gt;% \n  distinct(direction, NAME)\nstates_crossed_through_better\n##       direction         NAME\n## 4         There      Georgia\n## 10        There    Louisiana\n## 32        There  Mississippi\n## 41        There      Alabama\n## 1         There        Texas\n## 36        There   New Mexico\n## 47        There      Arizona\n## 35        There         Utah\n## 12   Back again        Idaho\n## 35.2 Back again         Utah\n## 15   Back again      Montana\n## 25   Back again      Wyoming\n## 23   Back again South Dakota\n## 16   Back again    Minnesota\n## 14   Back again     Illinois\n## 18   Back again         Iowa\n## 3    Back again     Kentucky\n## 4.1  Back again      Georgia\n## 7    Back again     Missouri\n## 9    Back again    Tennessee\n\nlower_48_highlighted_better &lt;- lower_48 %&gt;% \n  left_join(states_crossed_through_better, by = join_by(NAME)) %&gt;% \n  mutate(visited = !is.na(direction))\nas_tibble(lower_48_highlighted_better)\n## # A tibble: 51 × 12\n##    STATEFP STATENS  AFFGEOID    GEOID STUSPS NAME       LSAD         ALAND      AWATER direction                                                                                 geometry visited\n##    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;                                                                           &lt;MULTIPOLYGON [°]&gt; &lt;lgl&gt;  \n##  1 48      01779801 0400000US48 48    TX     Texas      00    676685555821 18974391187 There      (((-107 31.9, -107 32, -107 32, -107 32, -106 32, -106 32, -106 32, -106 32, -105 32... TRUE   \n##  2 06      01779778 0400000US06 06    CA     California 00    403673617862 20291712025 &lt;NA&gt;       (((-119 33.5, -118 33.5, -118 33.4, -118 33.4, -118 33.3, -118 33.3, -118 33.3, -118... FALSE  \n##  3 21      01779786 0400000US21 21    KY     Kentucky   00    102266581101  2384240769 Back again (((-89.5 36.6, -89.5 36.6, -89.4 36.6, -89.4 36.6, -89.3 36.6, -89.3 36.6, -89.3 36.... TRUE   \n##  4 13      01705317 0400000US13 13    GA     Georgia    00    149486268417  4418716153 There      (((-85.6 35, -85.5 35, -85.4 35, -85.4 35, -85.3 35, -85.3 35, -85 35, -85 35, -85 3... TRUE   \n##  5 13      01705317 0400000US13 13    GA     Georgia    00    149486268417  4418716153 Back again (((-85.6 35, -85.5 35, -85.4 35, -85.4 35, -85.3 35, -85.3 35, -85 35, -85 35, -85 3... TRUE   \n##  6 55      01779806 0400000US55 55    WI     Wisconsin  00    140292518676 29343193162 &lt;NA&gt;       (((-86.9 45.4, -86.8 45.5, -86.8 45.4, -86.9 45.4, -86.9 45.3, -87 45.4, -86.9 45.4)... FALSE  \n##  7 41      01155107 0400000US41 41    OR     Oregon     00    248630319014  6169061220 &lt;NA&gt;       (((-125 42.8, -124 43, -124 43, -124 43.1, -124 43.2, -124 43.2, -124 43.3, -124 43.... FALSE  \n##  8 29      01779791 0400000US29 29    MO     Missouri   00    178052253239  2487526202 Back again (((-95.8 40.6, -95.5 40.6, -95.4 40.6, -95.3 40.6, -95.2 40.6, -95.1 40.6, -94.9 40.... TRUE   \n##  9 51      01779803 0400000US51 51    VA     Virginia   00    102258178227  8528072639 &lt;NA&gt;       (((-76 37.3, -76 37.4, -76 37.4, -75.9 37.5, -75.9 37.6, -75.9 37.6, -75.9 37.7, -75... FALSE  \n## 10 47      01325873 0400000US47 47    TN     Tennessee  00    106792368794  2322190840 Back again (((-90.3 35, -90.3 35, -90.2 35.1, -90.2 35.1, -90.2 35.1, -90.1 35.1, -90.1 35.2, -... TRUE   \n## # ℹ 41 more rows\n\n\nLet’s see how it works by adding a new geom_sf() layer for the highlighted states. We need to double up here with both lower_48 and lower_48_highlighted_better because if we only plot the highlighted states, each facet will only contain those states and not the underlying US map, which we don’t want.\n\n\nCode\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = na.omit(lower_48_highlighted_better), aes(fill = visited)) +\n  geom_sf(data = routes_geocoded, color = clrs[1]) +\n  geom_sf(data = all_stops_unique) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip() +\n  facet_wrap(vars(direction), ncol = 1)\n\n\n\n\n\nEach facet now only shows the states corresponding to each direction\n\n\n\n\nThere, we fixed issue #1. ✅\nIssue #2—the missing endpoint in each facet—is a little trickier to deal with, but still doable. Before messing with the real data, let’s look at a simpler toy example first. Here’s a little dataset with three different “routes” between cities. The coordinates here aren’t actually coordinates—they’re just some arbitrary numbers. But that’s okay—this basically mirrors what we have in routes_geocoded"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-missing-destination-cities",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-missing-destination-cities",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Fixing missing destination cities",
    "text": "Fixing missing destination cities\n\n\nCode\nexample &lt;- tribble(\n  ~day, ~origin_city, ~destination_city, ~origin_coords, ~destination_coords,\n  1,    \"Atlanta\",    \"Boston\",          1234,           5678,\n  2,    \"Boston\",     \"Chicago\",         5678,           91011,\n  3,    \"Chicago\",    \"Denver\",          91011,          131415\n) \nexample\n## # A tibble: 3 × 5\n##     day origin_city destination_city origin_coords destination_coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                    &lt;dbl&gt;              &lt;dbl&gt;\n## 1     1 Atlanta     Boston                    1234               5678\n## 2     2 Boston      Chicago                   5678              91011\n## 3     3 Chicago     Denver                   91011             131415\n\n\nThe reason we’re not getting the final point in each facet or subset is because there are only three rows here, but four cities. If we plotted the origin_coords column, Atlanta would be missing; if we plotted the destination_coords column, Denver would be missing. We need to manipulate this data so that it has 4 rows (Atlanta, Boston, Chicago, Denver), with the correct coordinates for each city.\nThere’s an easy way to do this with pivot_longer() from {tidyr}. We can pivot with multiple columns that follow a pattern in their names. Here, all four of the columns we care about are are prefixed destination or origin, followed by a _. If we specify these name settings in pivot_longer(), it’ll automatically create a column named type for destination and origin and it’ll put the rest of the data in corresponding columns:\n\n\nCode\nexample %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_coords, destination_coords),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  )\n## # A tibble: 6 × 4\n##     day type        city    coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n## 1     1 origin      Atlanta   1234\n## 2     1 destination Boston    5678\n## 3     2 origin      Boston    5678\n## 4     2 destination Chicago  91011\n## 5     3 origin      Chicago  91011\n## 6     3 destination Denver  131415\n\n\nNow we have one row per city, which is close to what we need. Boston and Chicago are duplicated (since they’re both destination and origin cities), so we need to filter out duplicate cities. There are lots of ways to do this—my preferred way is to group by city and keep the first row in each group using slice():\n\n\nCode\nexample %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_coords, destination_coords),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(city) %&gt;% \n  slice(1)\n## # A tibble: 4 × 4\n## # Groups:   city [4]\n##     day type        city    coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n## 1     1 origin      Atlanta   1234\n## 2     1 destination Boston    5678\n## 3     2 destination Chicago  91011\n## 4     3 destination Denver  131415\n\n\nWoohoo! A dataset with four rows and the correct coordinates for each city. If we could plot this, we’d get both endpoints (Atlanta and Denver).\nThis same double pivot approach (magically!!) works with the special geometry columns in our actual routes data:\n\n\nCode\nall_stops_endpoints &lt;- routes_geocoded %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_geometry, destination_geometry),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(direction, city) %&gt;% \n  slice(1) %&gt;% \n  arrange(direction, day, desc(type)) %&gt;% \n  # Use \"geometry\" as the default geometry column instead of the routes\n  st_set_geometry(\"geometry\")\n\nall_stops_endpoints\n## Simple feature collection with 15 features and 11 fields\n## Active geometry column: geometry\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -112 ymin: 29.4 xmax: -84.4 ymax: 44.6\n## Geodetic CRS:  WGS 84\n## # A tibble: 15 × 13\n## # Groups:   direction, city [15]\n##    direction    day route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text       type        city                             geometry\n##    &lt;fct&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;                         &lt;POINT [°]&gt;\n##  1 There          1 src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes  origin      Atlanta, Georgia             (-84.4 33.7)\n##  2 There          1 src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes  destination New Orleans, Louisiana         (-90.1 30)\n##  3 There          2 src       dst                 596.           870. (-90.1 30, -90.3 30, -90.5 30.1, -90.9 30.2, -91.1 30.4, -91.3 30.5, -92.1 30.2, -93.2...           541. 540 miles     10 hours 0 minutes  destination San Antonio, Texas           (-98.5 29.4)\n##  4 There          3 src       dst                 458.           727. (-98.5 29.4, -98.7 29.7, -98.9 30, -99.5 30.3, -99.8 30.5, -99.9 30.5, -100 30.4, -101...           452. 450 miles     7 hours 45 minutes  destination Carlsbad, New Mexico          (-104 32.4)\n##  5 There          4 src       dst                 749.          1100. (-104 32.4, -104 32.5, -104 32.6, -104 32.9, -104 32.9, -104 33.3, -105 33.4, -105 33....           684. 680 miles     12 hours 30 minutes destination Grand Canyon NP, Arizona      (-112 36.1)\n##  6 There          5 src       dst                 510.           628. (-112 36.1, -112 36, -112 36, -112 36, -112 36, -112 35.9, -112 35.9, -112 35.9, -111 ...           390. 390 miles     8 hours 30 minutes  destination Grover, Utah                  (-111 38.2)\n##  7 There          6 src       dst                 197.           273. (-111 38.2, -111 38.2, -111 38.3, -112 38.3, -112 38.4, -112 38.4, -112 38.4, -112 38....           169. 170 miles     3 hours 15 minutes  destination Spanish Fork, Utah            (-112 40.1)\n##  8 Back again     1 src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes  origin      Spanish Fork, Utah            (-112 40.1)\n##  9 Back again     1 src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes  destination Shelley, Idaho                (-112 43.4)\n## 10 Back again     1 src       dst                 199.           240. (-112 43.4, -112 43.8, -112 43.9, -112 43.9, -112 44, -112 44, -111 44.1, -111 44.2, -...           149. 150 miles     3 hours 15 minutes  destination Yellowstone NP, Wyoming       (-111 44.5)\n## 11 Back again     2 src       dst                 552.           694. (-111 44.5, -111 44.4, -111 44.5, -111 44.5, -110 44.5, -110 44.6, -110 44.5, -110 44....           431. 430 miles     9 hours 15 minutes  destination Devil's Tower, Wyoming        (-105 44.6)\n## 12 Back again     2 src       dst                 161.           215. (-105 44.6, -105 44.6, -105 44.6, -105 44.6, -105 44.5, -105 44.5, -105 44.5, -105 44....           133. 130 miles     2 hours 45 minutes  destination Mount Rushmore, South Dakota  (-103 43.9)\n## 13 Back again     3 src       dst                 548.           866. (-103 43.9, -103 43.9, -103 44, -103 44, -103 44.1, -103 44.1, -103 44.1, -102 44.1, -...           538. 540 miles     9 hours 15 minutes  destination Albert Lea, Minnesota        (-93.4 43.6)\n## 14 Back again     4 src       dst                 359.           487. (-93.4 43.6, -93.3 43.1, -92.7 43.1, -92.7 43, -92.7 43, -92.6 43, -92.5 42.7, -92.5 4...           303. 300 miles     6 hours 0 minutes   destination Nauvoo, Illinois             (-91.4 40.6)\n## 15 Back again     5 src       dst                 847.          1199. (-91.4 40.6, -91.4 40.4, -91.6 40.4, -91.5 39.7, -91.4 39.7, -91.4 39.6, -91 39.2, -90...           745. 750 miles     14 hours 0 minutes  destination Atlanta, Georgia             (-84.4 33.7)\n\n\nAnd plotted:\n\n\nCode\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = na.omit(lower_48_highlighted_better), aes(fill = visited)) +\n  geom_sf(data = routes_geocoded, color = clrs[1]) +\n  geom_sf(data = all_stops_endpoints) +\n  geom_label_repel(\n    data = all_stops_endpoints,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234,\n    size = 3, segment.color = clrs[3], min.segment.length = 0\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip() +\n  facet_wrap(vars(direction), ncol = 1)\n\n\n\n\n\nEach facet now shows the destination city\n\n\n\n\nIssue #2 fixed. ✅"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#one-day-of-the-trip",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#one-day-of-the-trip",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "One day of the trip",
    "text": "One day of the trip\nFirst we’ll extract the route, cities, and states for the first day of the trip out there:\n\n\nCode\nroute_day1 &lt;- routes_geocoded %&gt;% \n  filter(direction == \"There\", day == 1)\n\nstops_day1 &lt;- all_stops_endpoints %&gt;% \n  filter(direction == \"There\", day == 1)\n\nstates_crossed_day1 &lt;- st_intersection(\n  st_transform(lower_48, st_crs(route_day1)),\n  route_day1\n) %&gt;% \n  distinct(NAME)\n\nlower_48_highlighted_day1 &lt;- lower_48 %&gt;% \n  mutate(visited = NAME %in% states_crossed_day1$NAME)\n\n\nLet’s plot these to make sure it worked:\n\n\nCode\nggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    nudge_y = miles_to_meters(80)\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip()\n\n\n\n\n\nDrive for the first day on the full US map\n\n\n\n\nYep, it worked. But we don’t need to show the whole map—we want to zoom in on just the area around the route for the day. To do this, we can use st_bbox() to extract a rectangle of coordinates around the route, creating a bounding box for the map that we can then use with coord_sf() to zoom in.\nst_bbox() returns a named vector of numbers with the x- and y-axis limits:\n\n\nCode\nbbox &lt;- st_bbox(route_day1)\nbbox\n##  xmin  ymin  xmax  ymax \n## -90.1  30.0 -84.4  33.7\n\n\nSince the elements are all named, we can use them to specify the different values in coord_sf(). First we’ll temporarily stop using the Albers projection, since the numbers we’ve extracted with st_bbox() are on the WGS 84 (−180 to 180) scale and Albers uses meters, which will make the map not work. (But we’ll fix that in a minute!)\n\n\nCode\nggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're not using Albers, so this isn't measured in meters; it's decimal degrees\n    nudge_y = 0.15\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox[\"xmin\"], bbox[\"xmax\"]), \n    ylim = c(bbox[\"ymin\"], bbox[\"ymax\"])\n  ) +\n  theme_roadtrip()\n\n\n\n\n\nTrip for the first day, but zoomed in a little too far\n\n\n\n\nWe’re zoomed in (yay) but the edges of the bounding box window are too close to the points and the route, and the labels are cropped funny. Also, to shift the labels up we had to think in decimal degrees instead of meters, which I can’t do naturally. And also, we can’t change projections—because we’re specifying the bounding box coordinates in decimal degrees, we’re stuck with WGS 84 instead of Albers or any other projection. These are all fixable issues, fortunately.\nTo fix all this, we’ll expand the bounding box window by 150 miles on all sides using st_buffer() and then convert the coordinates to Albers before extracting the coordinates of the window for plotting:\n\n\nCode\nbbox_nice &lt;- route_day1 %&gt;% \n  st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n  st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n  st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n  st_transform(\"ESRI:102003\") %&gt;%  # Switch to Albers\n  st_bbox()  # Extract the bounding box of the expanded box\nbbox_nice\n##     xmin     ymin     xmax     ymax \n##   301851 -1071395  1366321  -105313\n\n\nThese are no longer in decimal degrees, so we can use them with the Albers projection. We’ve also added a 150-mile buffer around the window, so we should have room for all the labels now. Let’s check it:\n\n\nCode\nggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're in Albers again, so we can work with meters (or miles)\n    nudge_y = miles_to_meters(30)\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\",\n    width_hint = 0.4\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]), \n    ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n    crs = st_crs(\"ESRI:102003\")\n  ) +\n  theme_roadtrip()\n\n\n\n\n\nTrip for the first day, correctly zoomed and using the Albers projection\n\n\n\n\nHeck yeah.\nHowever, the shapes look a little curved and distorted because we’re zoomed in so much. Albers is great for big countries, but on a small scale like this, WGS 84 is probably fine. That’s what Google Maps does—it only starts getting weird and curvy along horizontal latitude lines when you zoom out really far. Let’s do the first day map one last time without the Albers conversion:\n\n\nCode\nbbox_nice &lt;- route_day1 %&gt;% \n  st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n  st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n  st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n  st_bbox()  # Extract the bounding box of the expanded box\n\nggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're in WGS 84, so these are decimal degrees\n    nudge_y = 0.5\n  ) +\n  geom_label_repel(\n    data = route_day1,\n    aes(label = distance_text, geometry = route_geometry),\n    stat = \"sf_coordinates\", seed = 12345,\n    family = \"Overpass ExtraBold\", fontface = \"plain\",\n    size = 3.5, fill = colorspace::lighten(clrs[5], 0.8), color = clrs[1], \n    segment.color = \"grey50\", min.segment.length = 0\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\",\n    width_hint = 0.3\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]), \n    ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n    crs = st_crs(\"EPSG:4326\")\n  ) +\n  theme_roadtrip()\n\n\n\n\n\nTrip for the first day, correctly zoomed and using the WGS 84 projection\n\n\n\n\nNeat. The northern borders of Georgia, Alabama, and Mississippi are all flat here, which is great."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#all-the-days-of-the-trip",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#all-the-days-of-the-trip",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "All the days of the trip",
    "text": "All the days of the trip\nWe successfully plotted one day of the trip. But we’ll be driving for 11 days (!) and need 11 plots. I don’t want to copy/paste this all code 11 times, so we’ll stick each major step into a function:\n\n\nCode\n# Take a set of routes and do some pivoting to create a dataset that includes\n# the start and end points\nexpand_endpoints &lt;- function(routes) {\n  routes %&gt;% \n    pivot_longer(\n    cols = c(origin_city, destination_city, origin_geometry, destination_geometry),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(city) %&gt;% \n  slice(1) %&gt;% \n  arrange(desc(type)) %&gt;% \n  st_set_geometry(\"geometry\")\n}\n\n# Take a set of routes and return a dataset with a flag marking which states they cross\nfind_states &lt;- function(routes) {\n  # st_intersection() complains about innocuous things so suppress the warnings\n  suppressWarnings({\n    states_crossed &lt;- st_intersection(\n      st_transform(lower_48, st_crs(routes)),\n      routes\n    ) %&gt;% \n      distinct(NAME)\n  })\n  \n  lower_48 %&gt;% \n    mutate(visited = NAME %in% states_crossed$NAME)\n}\n\n# Use routes, stopes, and states to build a plot\nbuild_day_map &lt;- function(routes, stops, states, direction, day) {\n  bbox_nice &lt;- routes %&gt;%\n    st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n    st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n    st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n    st_bbox()  # Extract the bounding box of the expanded box\n\n  ggplot() +\n    geom_sf(data = states, aes(fill = visited)) +\n    geom_sf(data = routes, color = clrs[1]) +\n    geom_sf(data = stops) +\n    geom_sf_label(\n      data = stops,\n      aes(label = city),\n      # We're in WGS 84, so these are decimal degrees\n      nudge_y = 0.4\n    ) +\n    geom_label_repel(\n      data = routes,\n      aes(label = distance_text, geometry = route_geometry),\n      stat = \"sf_coordinates\", seed = 12345,\n      family = \"Overpass ExtraBold\", fontface = \"plain\",\n      size = 3.5, fill = colorspace::lighten(clrs[5], 0.8), color = clrs[1], \n      segment.color = \"grey50\", min.segment.length = 0\n    ) +\n    annotation_scale(\n      location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n      unit_category = \"imperial\", text_family = \"Overpass\",\n      width_hint = 0.4\n    ) +\n    scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n    coord_sf(\n      xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]),\n      ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n      crs = st_crs(\"EPSG:4326\")\n    ) +\n    labs(title = glue(\"{direction}, day {day}\")) +\n    theme_roadtrip()\n}\n\n\nWith everything functionalized, we can do some super wild {purrr} magic. If we take routes_geocoded and group it by direction and day (so there’s a group per driving day), we’ll get a list column that contains the geocded coordinates for each day:\n\n\nCode\ndaily_plots &lt;- routes_geocoded %&gt;% \n  group_by(direction, day) %&gt;% \n  nest(.key = \"routes\")\n\ndaily_plots\n## # A tibble: 11 × 3\n## # Groups:   direction, day [11]\n##    direction    day routes       \n##    &lt;fct&gt;      &lt;dbl&gt; &lt;list&gt;       \n##  1 There          1 &lt;sf [1 × 12]&gt;\n##  2 There          2 &lt;sf [1 × 12]&gt;\n##  3 There          3 &lt;sf [1 × 12]&gt;\n##  4 There          4 &lt;sf [1 × 12]&gt;\n##  5 There          5 &lt;sf [1 × 12]&gt;\n##  6 There          6 &lt;sf [1 × 12]&gt;\n##  7 Back again     1 &lt;sf [2 × 12]&gt;\n##  8 Back again     2 &lt;sf [2 × 12]&gt;\n##  9 Back again     3 &lt;sf [1 × 12]&gt;\n## 10 Back again     4 &lt;sf [1 × 12]&gt;\n## 11 Back again     5 &lt;sf [1 × 12]&gt;\n\n# Check the first day\ndaily_plots$routes[[1]]\n## Simple feature collection with 1 feature and 9 fields\n## Active geometry column: route_geometry\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -90.1 ymin: 30 xmax: -84.4 ymax: 33.7\n## Geodetic CRS:  WGS 84\n## # A tibble: 1 × 12\n##   origin_city      origin_geometry destination_geometry destination_city       route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text     \n##   &lt;chr&gt;                &lt;POINT [°]&gt;          &lt;POINT [°]&gt; &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             \n## 1 Atlanta, Georgia    (-84.4 33.7)           (-90.1 30) New Orleans, Louisiana src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes\n\n# Check the first day of the return trip\ndaily_plots$routes[[7]]\n## Simple feature collection with 2 features and 9 fields\n## Active geometry column: route_geometry\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -112 ymin: 40.1 xmax: -111 ymax: 44.7\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 12\n##   origin_city        origin_geometry destination_geometry destination_city        route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text     \n##   &lt;chr&gt;                  &lt;POINT [°]&gt;          &lt;POINT [°]&gt; &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             \n## 1 Spanish Fork, Utah     (-112 40.1)          (-112 43.4) Shelley, Idaho          src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes\n## 2 Shelley, Idaho         (-112 43.4)          (-111 44.5) Yellowstone NP, Wyoming src       dst                 199.           240. (-112 43.4, -112 43.8, -112 43.9, -112 43.9, -112 44, -112 44, -111 44.1, -111 44.2, -...           149. 150 miles     3 hours 15 minutes\n\n\nWe can then use purrr::map() and purrr::pmap() to feed that nested list-column data frame through the different functions to expand endpoints, find crossed-through states, and build the daily plot.\n\n\nCode\ndaily_plots &lt;- routes_geocoded %&gt;% \n  group_by(direction, day) %&gt;% \n  nest(.key = \"routes\") %&gt;% \n  mutate(\n    stops = map(routes, expand_endpoints),\n    states = map(routes, find_states),\n    plot = pmap(list(routes, stops, states, direction, day), build_day_map)\n  )\n\n\nBefore looking at the plots, check out all these nested datasets! Each day has a dataset for the routes, stops, states, and final plot, and everything is contained here in a data frame. MAGICAL.\n\n\nCode\ndaily_plots\n## # A tibble: 11 × 6\n## # Groups:   direction, day [11]\n##    direction    day routes        stops         states         plot  \n##    &lt;fct&gt;      &lt;dbl&gt; &lt;list&gt;        &lt;list&gt;        &lt;list&gt;         &lt;list&gt;\n##  1 There          1 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  2 There          2 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  3 There          3 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  4 There          4 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  5 There          5 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  6 There          6 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  7 Back again     1 &lt;sf [2 × 12]&gt; &lt;sf [3 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  8 Back again     2 &lt;sf [2 × 12]&gt; &lt;sf [3 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  9 Back again     3 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n## 10 Back again     4 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n## 11 Back again     5 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;\n\n\nWe can extract the plot for any single day with indexing:\n\n\nCode\ndaily_plots$plot[[10]]\n\n\n\n\n\nTrip for the day 4 of the return trip\n\n\n\n\nOr we can filter, pull, and pluck like good denizens of the tidyverse:\n\n\nCode\ndaily_plots %&gt;% \n  filter(direction == \"Back again\", day == 5) %&gt;% \n  pull(plot) %&gt;% pluck(1)\n\n\n\n\n\nTrip for the day 5 of the return trip\n\n\n\n\nHere’s all of them simultaneously inside a Quarto tabset, just for fun:\n\nThe trip thereThe trip back again\n\n\n\nDay 1Day 2Day 3Day 4Day 5Day 6\n\n\n\n\nCode\ndaily_plots$plot[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[4]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[6]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\nCode\ndaily_plots$plot[[7]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[8]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[9]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[10]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_plots$plot[[11]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr, instead of extracting each plot individually like this, we can use wrap_plots() from {patchwork} to combine a whole list of plots automatically, though we have a lot less control over the plots this way—some of the maps use a landscape orientation and some use a portrait orientation, so they’re a big mess when combined like this:\n\n\nCode\ndaily_plots %&gt;%\n  pull(plot) %&gt;% \n  wrap_plots()\n\n\n\n\n\n\n\n\n\n\nTime to add some maps to road trip journals and finish packing!"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "",
    "text": "In my research, I study international nongovernmental organizations (INGOs) and look at how lots of different institutional and organizational factors influence INGO behavior. For instance, many authoritarian regimes have passed anti-NGO laws and engaged in other forms of legal crackdown, which has forced NGOs to change their programming strategies and their sources of funding.\nThis is that guide.\nI explore three main things here:\nThroughout this example, I’ll use data from a political candidate conjoint experiment from Hainmueller, Hopkins, and Yamamoto (2014). The data is available at the Harvard Dataverse and it’s public domain, so you can also download it here if you want to follow along:\nLet’s load some packages and data, create some helper functions and nice theme stuff, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(haven)            # Read Stata files\nlibrary(broom)            # Convert model objects to tidy data frames\nlibrary(cregg)            # Automatically calculate frequentist conjoint AMCEs and MMs\nlibrary(survey)           # Panel-ish regression models\nlibrary(scales)           # Nicer labeling functions\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(broom.helpers)    # Add empty reference categories to tidy model data frames\nlibrary(ggforce)          # For facet_col()\nlibrary(brms)             # The best formula-based interface to Stan\nlibrary(tidybayes)        # Manipulate Stan results in tidy ways\nlibrary(ggdist)           # Fancy distribution plots\nlibrary(patchwork)        # Combine ggplot plots\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Jost\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Jost\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Jost\", face = \"bold\"),\n          axis.title = element_text(family = \"Jost Medium\"),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 1),\n          strip.text = element_text(family = \"Jost\", face = \"bold\",\n                                    size = rel(0.75), hjust = 0),\n          strip.background = element_rect(fill = \"grey90\", color = NA))\n}\n\n# Set default theme and font stuff\ntheme_set(theme_nice())\nupdate_geom_defaults(\"text\", list(family = \"Jost\", fontface = \"plain\"))\nupdate_geom_defaults(\"label\", list(family = \"Jost\", fontface = \"plain\"))\n\n# Party colors from the Urban Institute's data visualization style guide, for fun\n# http://urbaninstitute.github.io/graphics-styleguide/\nparties &lt;- c(\"#1696d2\", \"#db2b27\")\n\n# Functions for formatting things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\n\nlabel_amce &lt;- label_number(accuracy = 0.1, scale = 100, suffix = \" pp.\", \n                           style_negative = \"minus\", style_positive = \"plus\")\n\n# Data from https://doi.org/10.7910/DVN/THJYQR\n# It's public domain\ncandidate &lt;- read_stata(\"data/candidate.dta\") %&gt;% \n  as_factor()  # Convert all the Stata categories to factors\n\n# Make a little lookup table for nicer feature labels\nvariable_lookup &lt;- tribble(\n  ~variable,    ~variable_nice,\n  \"atmilitary\", \"Military\",\n  \"atreligion\", \"Religion\",\n  \"ated\",       \"Education\",\n  \"atprof\",     \"Profession\",\n  \"atmale\",     \"Gender\",\n  \"atinc\",      \"Income\",\n  \"atrace\",     \"Race\",\n  \"atage\",      \"Age\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#average-marginal-component-effect-amce",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#average-marginal-component-effect-amce",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Average marginal component effect (AMCE)",
    "text": "Average marginal component effect (AMCE)\nThis distinction between causal and descriptive estimands makes sense if we look at the notation for the estimands themselves. In the world of do-calculus, causal questions are asked using the \\(\\operatorname{do}()\\) operator, which represents a direct intervention into a data generating process. In this case, the researcher randomly sets the attributes to specific levels—the respondent does not self-select into different conditions or decide for themselves that Candidate 1 is a Catholic lawyer or that Candidate 2 is a Jewish farmer. This thus eliminates selection bias and other external confounding, leaving us with an average causal effect.\nWe can calculate (1) the average outcome when a feature is set to a level we’re interested in (e.g., when religion = Mormon), (2) the average outcome when a feature is set to some reference level (e.g., when religion = none), and (3) find the difference between the two:\n\\[\n\\begin{aligned}\n\\theta =\\ &P [\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{none})]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{Mormon})]\n\\end{aligned}\n\\]\nPractically speaking, the easiest way to think about the average marginal component effect (AMCE) is as categorical coefficients in a linear regression model.\nIn my favorite analogy for thinking about regression, model covariates can either be sliders or switches:\n\n\n\n\n\n\n\n\n\nNumeric covariates are sliders—a one unit change in X is associated with a \\(\\beta\\) unit change in Y. You can slide that X variable up and down and influence Y accordingly (a 10 unit change in X is associated with a \\(10 \\times \\beta\\) change in Y; a −1 unit change in X is associated with a \\(-\\beta\\) change in Y; and so on). The \\(\\beta\\) is a slider that you can move up and down and see what happens to the outcome.\nCategorical covariates, on the other hand, are switches. One of the categories is omitted and represents the baseline average for that category, or the average when the switch is off. The other category coefficients represent shifts in that baseline, or what happens when the switch is flipped on.\nHere’s a quick super basic reference example with data from {palmerpenguins} (this is not actual conjoint data!). We’ll model penguin weight based on penguin species and sex:\n\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins %&gt;% drop_na(sex)\n\npenguin_model &lt;- lm(body_mass_g ~ species + sex, data = penguins)\ntidy(penguin_model)\n## # A tibble: 4 × 5\n##   term             estimate std.error statistic   p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)        3372.       31.4   107.    4.34e-258\n## 2 speciesChinstrap     26.9      46.5     0.579 5.63e-  1\n## 3 speciesGentoo      1378.       39.1    35.2   1.05e-113\n## 4 sexmale             668.       34.7    19.2   8.73e- 56\n\nThere are three species and two sexes of penguins, but we only get coefficients for two species (Chinstrap and Gentoo) and one sex (male) because of how regression works—one category is omitted. The coefficients here represent changes in average body mass when switching on the corresponding category. For sex, the omitted category is “female,” so the coefficient for sexmale shows that male penguins are 667 grams heavier than female penguins, on average. Imagine a “sex” switch—when it’s flipped up to the male position, body mass goes up. The same idea works for species—the omitted species is Adélie, so on average Chinstraps are 27 grams heavier than Adélies, and Gentoos are 1,378 grams heavier than Adélies. We can flip the “Chinstrap” or “Gentoo” switches on and increase body mass accordingly.\nThe nice thing about the slider and switch analogy is that it makes it easier to think about holding everything constant—we have switches for both species and sex, but if we just tinker with one, we’ll get the effect of being a Chinstrap or a Gentoo or a male. It’s like a mixer board:\n\n\n\n\n\n\n\n\n\nThat’s all standard regression-with-categorical-variables stuff.\nThe magic of AMCEs is that in the case of ordinary least squares (OLS) linear regression without any squared or nonlinear terms, AMCEs are just coefficients. It’s a little more complicated with non-linear terms or models with nonlinear link functions like logistic regression—we’d need to calculate marginal effects to get the AMCEs in those cases (but I’ll explore that a lot more below)."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nThe descriptive estimand, on the other hand, does not have a \\(\\operatorname{do}()\\) operator, which means that there’s no experimental intervention or causal effect. Instead, we’re working with the observed averages of different levels.\nFor instance, if we wanted to know what proportion of respondents support Mormon candidates, we could calculate this estimand:\n\\[\n\\theta = P(\\text{Candidate selection} \\mid \\text{Religion = Mormon})\n\\]\nOr if we wanted to know the percentage point difference between the probabilities of Mormon and Catholic candidates, we could calculate this estimand:\n\\[\n\\begin{aligned}\n\\theta =\\ &P[\\text{Candidate selection} \\mid \\text{Religion = Mormon}]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\text{Religion = Catholic}]\n\\end{aligned}\n\\]\nImportantly these aren’t causal estimands—they’re descriptive. They’re also not relative to any baseline level. They’re not regression-style “switches” but instead are group averages.\nWe can see this really quick with the penguin data (again, this isn’t related to conjoint stuff! this is just to help with the intuition). To find these marginal means, we calculate the category-specific means. We can do that without regression with some grouping and summarizing:\n\n# Marginal means for species\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(avg = mean(body_mass_g))\n## # A tibble: 3 × 2\n##   species     avg\n##   &lt;fct&gt;     &lt;dbl&gt;\n## 1 Adelie    3706.\n## 2 Chinstrap 3733.\n## 3 Gentoo    5092.\n\n# Marginal means for sex\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  summarize(avg = mean(body_mass_g))\n## # A tibble: 2 × 2\n##   sex      avg\n##   &lt;fct&gt;  &lt;dbl&gt;\n## 1 female 3862.\n## 2 male   4546.\n\nOr we could use a couple intercept-free models to get the same values. Mathematically this is the same as grouping and summarizing, since regression is ultimately just fancy averaging.\n\n\n\n\n\n“Many things got easier once I accepted that regression is just fancy averaging.” —@ajeanstevenson\n\n\n\n\n\nbind_rows(\n  tidy(lm(body_mass_g ~ 0 + species, data = penguins)),\n  tidy(lm(body_mass_g ~ 0 + sex, data = penguins))\n)\n## # A tibble: 5 × 5\n##   term             estimate std.error statistic   p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 speciesAdelie       3706.      38.1      97.2 6.88e-245\n## 2 speciesChinstrap    3733.      55.9      66.8 8.16e-194\n## 3 speciesGentoo       5092.      42.2     121.  6.31e-275\n## 4 sexfemale           3862.      56.8      68.0 1.70e-196\n## 5 sexmale             4546.      56.3      80.7 8.39e-220\n\nOr even better, we can use marginal_means() from {marginaleffects} (way more on that below!)\n\nmarginal_means(penguin_model, newdata = c(\"species\", \"sex\"), wts = \"cells\")\n## \n##     Term     Value Mean Std. Error     z Pr(&gt;|z|) 2.5 % 97.5 %\n##  species Adelie    3706       26.2 141.4   &lt;0.001  3655   3758\n##  species Chinstrap 3733       38.4  97.2   &lt;0.001  3658   3808\n##  species Gentoo    5092       29.0 175.5   &lt;0.001  5036   5149\n##  sex     female    3862       24.6 156.7   &lt;0.001  3814   3911\n##  sex     male      4546       24.4 186.1   &lt;0.001  4498   4594\n## \n## Results averaged over levels of: species, sex \n## Columns: term, value, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nRegardless of how we calculate these, the numbers are the same, and they represent average penguin weights in each of the species and both of the sexes. We can answer descriptive questions about the average weight of female penguins or the difference in average weights between Gentoo and Chinstrap penguins. There’s no reference level—there are no regression switches or sliders—these are just averages.\n\n\n\n\n\n\nBonus: Market simulations\n\n\n\nThere’s an extra third option that I didn’t include in Table 1 because it’s not used often in my worlds of public policy and political science, but it is used a lot in marketing. In this approach, researchers use conjoint experiments to simulate the data generating process for an overall “market” of “products” (or candidates in this running example).\nResearchers build rich multilevel models to capture all the dynamics of the different respondent-level characteristics and the experimental features and levels and then create hypothetical “purchasers” with different covariate levels and see which combinations of individual characteristics and product characteristics influence the overall market share of products. Using the candidate experiment example, the market simulation would model the effect of different candidate features (religion, military experience, education, and so on) on the probability of choosing a candidate across individual respondent characteristics like ideology, political preferences, age, education, and so.\nFrom what I’ve seen, this market simulation-based approach is really rare in social science. In fact, this paper (Chaudhry, Dotson, and Heiss 2021) coauthored by Suparna Chaudhry, Marc Dotson, and me is the only political science-y one I know of! In it, we were interested in a host of different factors that drive individual preferences for charitable donations. We generated dozens of hypothetical donor personas and looked at how different categories of respondents felt about different features and how those preferences then influenced the market share of hypothetical nonprofits/NGOs.\nFor example, here we can see the average predicted donation market shares across all donor personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships. NGOs with friendly relationships with their host governments will receive a greater share of donations from the market, regardless of individual political ideology or political knowledge and travelling experience.\n\n\n\nFigure 4 from Chaudhry, Dotson, and Heiss (2021)\n\n\nReturning to the slider + switch analogy, this kind of market simulation is like an ultimate-super-mega-huge mixer board. We had to create a Shiny dashboard (accessible here) to explore all the different possible outcomes. It’s wild.\n\n\n\nScreenshot of market simulator"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#relationship-between-amces-and-marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#relationship-between-amces-and-marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Relationship between AMCEs and marginal means",
    "text": "Relationship between AMCEs and marginal means\nTechnically AMCEs and marginal means measure the same thing, just in different ways. Thinking about this in regression terms helps—with categorical marginal effects, the intercept for the model represents the average outcome for the omitted category, while the coefficient represents the offset from the average. The coefficient is the switch—turning it on changes the baseline reference category average by \\(\\beta\\) amount.\n\nPenguin example\nHere’s one last example from the non-conjoint penguins data, just to drive the point home. Let’s make a super basic model that predicts weight based on sex only:\n\nmodel_sex_only &lt;- lm(body_mass_g ~ sex, data = penguins)\ntidy(model_sex_only)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    3862.      56.8     68.0  1.70e-196\n## 2 sexmale         683.      80.0      8.54 4.90e- 16\n\nThe intercept shows average weight for female penguins (3862 grams), while the coefficient for sexmale shows the change from that average when the “male” switch is turned on (683 more grams, on average).\nWe can actually use these two pieces of information to find the average penguin weight across both sexes: females are 3862.3 grams, males are 3862.3 + 683.4 = 4546 grams. We can confirm this with a quick group_by() %&gt;% summarize():\n\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  summarize(avg_weight = mean(body_mass_g))\n## # A tibble: 2 × 2\n##   sex    avg_weight\n##   &lt;fct&gt;       &lt;dbl&gt;\n## 1 female      3862.\n## 2 male        4546.\n\nVisualizing this should help even more with the general intuition. The horizontal distance between these two points is the same in each panel (683 grams). In the left panel, female weight is set at 0 since it’s the omitted reference category. In the right panel, both males and females have specific group averages. These two panels are analgous to the conjoint idea of AMCEs and marginal means.\n\n\nCode\np1 &lt;- model_sex_only %&gt;%\n  tidy_and_attach() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  ggplot(aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  annotate(\n    geom = \"errorbar\",\n    x = 0, xmin = 0, xmax = 683.4, y = 1.5,\n    width = 0.1, color = \"grey70\"\n  ) +\n  annotate(\n    geom = \"label\",\n    x = 683.4 / 2, y = 1.5,\n    label = \"683.4 grams\",\n    size = 3\n  ) +\n  labs(\n    x = \"Grams\", y = NULL,\n    title = \"Relative shift in average\",\n    subtitle = \"Similar to AMCEs\"\n  )\n\np2 &lt;- lm(body_mass_g ~ 0 + sex, data = penguins) %&gt;%\n  tidy(conf.int = TRUE) %&gt;%\n  ggplot(aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  annotate(\n    geom = \"errorbar\",\n    x = 3862, xmin = 3862, xmax = 4546, y = 1.5,\n    width = 0.1, color = \"grey70\"\n  ) +\n  annotate(\n    geom = \"label\",\n    x = 4546 - (683.4/2), y = 1.5,\n    label = \"683.4 grams\",\n    size = 3\n  ) +\n  scale_x_continuous(labels = label_comma()) +\n  labs(\n    x = \"Grams\", y = NULL, \n    title = \"Group averages\",\n    subtitle = \"Similar to marginal means\"\n  )\n\np1 | p2\n\n\n\n\n\n\n\n\n\n\n\nConjoint AMCEs and marginal means (finally!)\nOkay, with that intuition nailed down, we can finally look at conjoint results. We’ll look at the results from the candidate experiment in Hainmueller, Hopkins, and Yamamoto (2014), which Leeper, Hobolt, and Tilley (2020) replicate and explore in their paper distinguishing between AMCEs and marginal means, and which we explored at the beginning of this post to show how conjoints work (i.e. there are seven candidate attributes like military service history, religion, gender, age, and so on).\nHere’s an excerpt from Figure 1 in Leeper, Hobolt, and Tilley (2020), which shows both the published AMCEs and the Leeper, et al.-calculated marginal means from Hainmueller, Hopkins, and Yamamoto’s original candidate study:\n\n\n\n\n\n\n\n\n\n\n\nThe AMCEs in the left panel have a direct causal interpretation. In this case, holding all else equal, changing a candidate from not having military service to serving in the military increases the probability of support (or overall favorability) by 0.09, or 9 percentage points. Similarly, changing from a nonreligious candidate to a Mormon candidate decreases the probability of support by 14 percentage points (!!!).4 There is no sex-based effect—changing a candidate from male to female has an AMCE of 0.0.4 This study was written and published in 2013, right after the 2012 presidential election between Barack Obama and Mitt Romney, a Mormon who did surprisingly well considering longstanding anti-Mormon sentiment in American politics (for more on Mormons and American politics, see Reeve (2015) and McBride, Rogers, and Erekson (2020)).\nThe marginal means in the right panel don’t rely on a reference category and are all centered around 50%—if there’s no difference between the the levels in a two-level feature, we’d expect the averages for each level to be 50%. We can see this with sex, where both male and female candidates have a 50% probability of selection (or 50% favorability, or however we want to interpret the outcome here).\nWith the AMCEs, we saw a 9 percentage point increase in favorability caused by military service. That same 9-point difference is visible in the marginal means: candidates without military service history have 46% favorability compared to the 54% favorability among those with military history (54−46 = 9).\nThe presence of a reference category doesn’t matter much when dealing with binary treatments like sex and military service here. If we reversed the reference category, we’d get the same causal effect but in reverse—not serving in the military causes a 9 percentage point drop in favorability.\nThe reference category matters a lot, however, in attributes with more than two levels, like religion. In the AMCE panel, all the causal effects are in reference to a candidate with no religion. Being Mormon causes a 14 percentage point drop from having no religion; being Evangelical causes a 12 percentage point drop from having no religion; and so on. Deciding which level to use as the omitted reference category matters and can seriously change the causal interpretation of the AMCEs. For instance, here it looks like there’s a serious penalty for being Mormon or Evangelical, while being Protestant, Catholic, or Jewish doesn’t matter. But that’s only the case from a certain point of view. If an Evangelical candidate were the reference category, there wouldn’t be any significant Mormon effect (but there would be a Protestant, Catholic, Jewish, and None effect).\n\n\n\n\n\n“What I told you was true… from a certain point of view”—Obi-Wan Kenobi\n\n\nThe reference category gets in the way of making descriptive statements like “Mormon candidates see 42% favorability, while Jewish candidates see 52% favorability.” You can’t get numbers like that from the AMCEs alone unless you know the underlying favorability of the reference category and then reconstruct the other categories’ averages by hand. But researchers working with conjoint experiments try to do this with AMCEs all the time. Leeper, Hobolt, and Tilley (2020) argue that\n\nAMCEs are relative, not absolute, statements about preferences. As such, there is simply no predictable connection between subgroup causal effects and the levels of underlying subgroup preferences. Yet, analysts and their readers frequently interpret differences in conditional AMCEs as differences in underlying preferences (Leeper, Hobolt, and Tilley 2020, 214).\n\nThe key point is this: AMCEs are relative statements, not absolute statements. If we want to talk about the causal effect of moving one attribute level (None → Mormon, None → Catholic, Male → Female, etc.), we can use AMCEs. If we want to talk about general preferences or favorabilities or probabilities or average outcomes, we need to talk about marginal means."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-and-marginal-means-across-subgroups",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-and-marginal-means-across-subgroups",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs and marginal means across subgroups",
    "text": "AMCEs and marginal means across subgroups\nThis issue with relative vs. absolute estimands is especially complex when thinking about subgroups or controlling for respondent-level characteristics like political ideology, sex, education, and so on. Suppose we want to know if this Mormon penalty is bigger among Democratic respondents than Republican respondents. We could control for respondent ideology, or calculate two different AMCEs—one when Democrat = true and one when Republican = true. This seems all good and logical. And it it’s fine if you’re talking about causal effects. But once you start trying to compare overall trends across respondent-level subgroups, AMCEs will give you wrong estimates! The main argument in Leeper, Hobolt, and Tilley (2020) is that looking at AMCEs across respondent subgroups doesn’t work people think it does because AMCEs are relative and not absolute. The difference between the relative Mormon candidate AMCE among Democratic-leaning and Republican-leaning respondents isn’t really comparable to the party-based differences in other levels (Evangelicals, Catholics, etc.).\nLeeper, Hobolt, and Tilley (2020) illustrate this idea in a neat way in their Figure 2 (included below). This figure recreates the results for the “candidate sex” feature in a different candidate conjoint experiment done by Teele, Kalla, and Rosenbluth (2018). In this case, unlike our running example from Hainmueller, Hopkins, and Yamamoto (2014)’s study, there is a sex effect—a candidate being female causes a 4.5 percentage point increase in favorability. We can see this in the top two panels, both as an AMCE of 0.045 and as marginal means (female = 0.52; male = 0.48; difference = AMCE = 0.045).\n\n\n\n\n\n\n\n\n\nThe third panel shows two different AMCEs conditional on respondent political party, and it appeared in the published study. Because AMCEs require a reference category as the baseline, and because AMCEs are relative quantities, it looks like there’s a huge party-based difference in favorability toward women candidates—being a woman causes a 7ish percentage point boost in favorability among Democrats, while it does nothing (or maybe something negative) among Republicans. These are conditional AMCEs (or CAMCEs), or the causal effect of turning on the “female” switch for a hypothetical candidate across Republican and Democratic respondents.\nConditional AMCEs are fine for causal work, but what trips people up often is that it’s tempting to use those conditional effects to describe actual overall patterns of preferences. Because there’s a reference category involved (male candidates), we can’t really say anything about the general respondent-party-ID-based preferences for male and female candidates. The conditional AMCE here combines the party-based difference in favorability toward female candidates (53.7% among Democrats; 49.2% among Republicans; difference of 4.5 percentage points) and the party-based difference in favorability toward male candidates (46.3% among Democrats; 50.8% among Republicans; difference of 4.5 percentage points). According to Leeper, et al.:\n\nBecause Democrats and Republicans actually differ in their views of profiles containing the reference (male) category, AMCEs sum the true differences in preferences for a given feature level with the difference in preferences toward the reference category. Visual or numerical similarity of subgroup AMCEs is therefore an analytical artifact, not an accurate statement of the similarity of patterns of preferences (Leeper, Hobolt, and Tilley 2020, 215).\n\nIf we’re interested not in describing a causal effect (i.e. the effect of switching from male → female) but instead describing general preferences (i.e. the difference in overall candidate sex favorability between Republicans and Democrats), we have to look at marginal means (and the difference in marginal means) instead of conditional AMCEs. It is really tempting to look at the distance between Republicans and Democrats in the “Female” level in the third panel and say that there’s a nearly 10 percentage point difference in favorability across parties, but that’s wrong! It only looks that way because the effect sizes are all relative to the reference “Male” level.\n\n\n\n\n\n\n\n\n\nIn reality, Democrats are 4.5 percentage points less likely to select a male candidate and 4.5 percentage points more likely to select a female candidate. The differences in marginal means within party subgroups would look like this:\n\n\nCode\n# Teele, Kalla, and Rosenbluth 2018 data from https://doi.org/10.7910/DVN/FVCGHC\ncandidate_parties &lt;- read_stata(\"data/conjoint_data.dta\") %&gt;% \n  as_factor() %&gt;%\n  mutate(female = factor(orig_cand_gender_string)) %&gt;% \n  mutate(female = fct_relevel(female, \"Male\")) %&gt;% \n  mutate(party_respondent = case_when(\n    democrat_respondent == 1 ~ \"Democrat\",\n    republican_respondent == 1 ~ \"Republican\"\n  )) %&gt;% \n  mutate(party_respondent = factor(party_respondent)) %&gt;% \n  filter(sample == \"usa voter\")\n\nmm_diffs(\n  candidate_parties,\n  winner ~ female,\n  by = ~party_respondent\n) %&gt;%\n  ggplot(aes(x = estimate, y = level)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(\n    xmin = lower, xmax = upper, \n    color = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = \"#85144b\") +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means between\\nRepublican and Democratic respondents\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Candidate sex\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\n\n\n\n\n\n\n\n\n(For more about this, see Leeper, Hobolt, and Tilley (2020) for a few other examples of the substantive differences between subgroup conditional AMCEs and subgroup differences-between-marginal-means.)\nLong story short: because AMCEs are relative estimands, they get weird (and can’t really be used) when using them for descriptive estimands across subgroups or when controlling for other respondent characteristics. To account for this weirdness, calculate marginal means instead and find the subgroup differences in marginal means for each level."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nFirst we’ll calculate the average marginal component effects (AMCEs), which are the partial derivatives (or coefficients) from a linear regression model. These represent the causal effect of switching from some reference level to a level of interest, while holding everything else constant.\n\nAutomatic estimates with cregg::amce()\nThe amce() function from {cregg} calculates AMCEs automatically and returns them in a nice tidy data frame:\n\nmodel_candidate_amce &lt;- amce(\n  candidate,\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  id = ~resID\n)\n\nmodel_candidate_amce %&gt;% as_tibble()\n## # A tibble: 40 × 10\n##    outcome  statistic feature          level                  estimate std.error      z            p   lower   upper\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;fct&gt;            &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 selected amce      Military Service Did Not Serve            0        NA      NA     NA           NA      NA     \n##  2 selected amce      Military Service Served                   0.0873    0.0177  4.95   0.000000761  0.0527  0.122 \n##  3 selected amce      Religion         None                     0        NA      NA     NA           NA      NA     \n##  4 selected amce      Religion         Jewish                  -0.0373    0.0263 -1.42   0.156       -0.0889  0.0143\n##  5 selected amce      Religion         Catholic                -0.0156    0.0278 -0.561  0.575       -0.0702  0.0389\n##  6 selected amce      Religion         Mainline protestant     -0.0149    0.0308 -0.484  0.629       -0.0753  0.0455\n##  7 selected amce      Religion         Evangelical protestant  -0.117     0.0309 -3.78   0.000157    -0.178  -0.0563\n##  8 selected amce      Religion         Mormon                  -0.137     0.0307 -4.46   0.00000838  -0.197  -0.0767\n##  9 selected amce      College          No BA                    0        NA      NA     NA           NA      NA     \n## 10 selected amce      College          Baptist college          0.139     0.0289  4.82   0.00000143   0.0827  0.196 \n## # ℹ 30 more rows\n\nIt also provides an automatic plot function. Compare this with the original—the results are identical.\n\nplot(cregg::amce())Original AMCE coefficent plot\n\n\n\nplot(model_candidate_amce) + \n  guides(color = \"none\") +\n  theme_nice() +\n  labs(title = \"AMCEs from cregg::amce()\")\n\n\n\n\n\n\n\n\n\n\nRight panel of Figure 1 in Leeper, Hobolt, and Tilley (2020):\n\n\n\n\n\n\nOLS coefficients\nBehind the scenes, {cregg} uses survey::svyglm() to run OLS with ID-specific adjustments to standard errors:\n\ncandidate_svy_design &lt;- svydesign(\n  ids = ~resID,\n  weights = ~1,\n  data = candidate\n)\n\nmodel_svy &lt;- svyglm(\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  design = candidate_svy_design\n)\n\ntidy(model_svy)\n## # A tibble: 33 × 5\n##    term                             estimate std.error statistic  p.value\n##    &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 (Intercept)                        0.397     0.0484     8.20  9.18e-15\n##  2 atmilitaryServed                   0.0873    0.0177     4.95  1.32e- 6\n##  3 atreligionJewish                  -0.0373    0.0263    -1.42  1.58e- 1\n##  4 atreligionCatholic                -0.0156    0.0278    -0.561 5.75e- 1\n##  5 atreligionMainline protestant     -0.0149    0.0308    -0.484 6.29e- 1\n##  6 atreligionEvangelical protestant  -0.117     0.0309    -3.78  1.92e- 4\n##  7 atreligionMormon                  -0.137     0.0307    -4.46  1.22e- 5\n##  8 atedBaptist college                0.139     0.0289     4.82  2.36e- 6\n##  9 atedCommunity college              0.150     0.0290     5.17  4.44e- 7\n## 10 atedState university               0.188     0.0277     6.77  7.80e-11\n## # ℹ 23 more rows\n\nThese estimates and standard errors are the same results that we get from cregg::amce():\n\n# Combine the estimates from cregg::amce() with the estimates from\n# survey::svglm() just to check that they're the same (they are)\namce_estimates &lt;- model_candidate_amce %&gt;% \n  as_tibble() %&gt;% \n  drop_na(std.error) %&gt;% \n  select(level, amce_estimate = estimate, amce_std.error = std.error)\n\nsvy_estimates &lt;- model_svy %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  select(svy_estimate = estimate, svy_std.error = std.error)\n\nbind_cols(amce_estimates, svy_estimates)\n## # A tibble: 32 × 5\n##    level                  amce_estimate amce_std.error svy_estimate svy_std.error\n##    &lt;fct&gt;                          &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n##  1 Served                        0.0873         0.0177       0.0873        0.0177\n##  2 Jewish                       -0.0373         0.0263      -0.0373        0.0263\n##  3 Catholic                     -0.0156         0.0278      -0.0156        0.0278\n##  4 Mainline protestant          -0.0149         0.0308      -0.0149        0.0308\n##  5 Evangelical protestant       -0.117          0.0309      -0.117         0.0309\n##  6 Mormon                       -0.137          0.0307      -0.137         0.0307\n##  7 Baptist college               0.139          0.0289       0.139         0.0289\n##  8 Community college             0.150          0.0290       0.150         0.0290\n##  9 State university              0.188          0.0277       0.188         0.0277\n## 10 Small college                 0.178          0.0273       0.178         0.0273\n## # ℹ 22 more rows\n\n\n\nMarginal effects instead of coefficients\nSo these OLS coefficients here are the AMCEs. That’s super straightforward.\nThat’s only true because we’re estimating a linear probability model (LPM) here. selected is a binary 0/1 variable, but we’re pretending it’s numeric and using OLS with it. In this special case, the marginal effects (slopes) are just the partial derivatives reported in the regression table.\nBut if we’re going to have interaction terms, or nonlinear terms (like polynomials), or if we’re going to use a model family that’s not OLS (like logistic regression), raw coefficients won’t work. Instead we need to calculate actual marginal effects (see this post and the “Get Started” page at the {marginaleffects} documentation for more details about these).\nAt one point, {cregg} supported this by using the {margins} package (which makes sense—Thomas Leeper developed both {cregg} and {margins}), but starting with {cregg} version 0.1.14, the {margins} support disappeared, leaving only support for OLS linear probability models.\nWe can use the {marginaleffects} package (the successor to {margins}) to calculate the average marginal effects/slopes for each of these effects. Here we’ll just hold all the predictors at their means (but there are a ton of different estimands we could calculate). In this case they’re the same as the raw coefficients, since it’s just a linear model:\n\nmfx_svy &lt;- model_svy %&gt;% \n  avg_slopes(newdata = \"mean\")\n\n# Combine the average marginal effects with the original coefficients just to\n# check that they're the same (they are)\nmfx_svy %&gt;% \n  select(contrast, mfx_estimate = estimate, mfx_std.error = std.error) %&gt;% \n  cbind(svy_estimates) %&gt;% \n  as_tibble()\n## # A tibble: 32 × 5\n##    contrast                      mfx_estimate mfx_std.error svy_estimate svy_std.error\n##    &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n##  1 45 - 36                            0.0259         0.0295       0.0873        0.0177\n##  2 52 - 36                            0.0236         0.0303      -0.0373        0.0263\n##  3 60 - 36                            0.00414        0.0288      -0.0156        0.0278\n##  4 68 - 36                           -0.0639         0.0294      -0.0149        0.0308\n##  5 75 - 36                           -0.146          0.0289      -0.117         0.0309\n##  6 Baptist college - No BA            0.139          0.0289      -0.137         0.0307\n##  7 Community college - No BA          0.150          0.0290       0.139         0.0289\n##  8 Ivy League university - No BA      0.269          0.0291       0.150         0.0290\n##  9 Small college - No BA              0.178          0.0273       0.188         0.0277\n## 10 State university - No BA           0.188          0.0277       0.178         0.0273\n## # ℹ 22 more rows\n\n\n\nCoefficient plots\nWhat about that neat coefficient plot with the reference categories? tidy() can’t return empty rows for the reference levels like cregg::amce() does, but we can make it work in a couple different ways: (1) automatically with broom.helpers::tidy_add_reference_rows(), or (2) manually with some data wrangling.\n\nAutomatically with {broom.helpers}\nThe easiest way to do this is to use broom.helpers::tidy_add_reference_rows(), which adds the reference levels automatically with estimates of 0, so we can use geom_pointrange() and make basically the same plot as {cregg}:\n\nplot_data_manual &lt;- model_svy %&gt;% \n  tidy_and_attach() %&gt;% \n  tidy_add_reference_rows() %&gt;% \n  tidy_add_estimate_to_reference_rows() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term_nice = str_remove(term, variable)) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(term_nice, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_manual,\n  aes(x = estimate, y = term_nice, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs plotted with tidy_add_reference_rows()\"\n  ) +\n  # Automatically resize each facet height with ggforce::facet_col()\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\n\n\n\nManually with {dplyr} wrangling magic\nHowever, tidy_add_reference_rows() doesn’t work with {marginaleffects} objects or multilevel models or Bayesian models, so ultimately I can’t use this in the real project I’m working on :(\nBut we can get the same thing with some fancy data wrangling:\n\n# Extract all the right-hand variables\nrhs_vars &lt;- all.vars(stats::update(formula(model_svy), 0 ~ .))\n\n# Make a data frame of all the levels of all the non-numeric things\nmodel_variable_levels &lt;- tibble(\n  variable = rhs_vars\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- candidate[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Extract model results\nmodel_results &lt;- tidy(model_svy, conf.int = TRUE)\n\n# Combine full dataset of factor levels with model results\nplot_data &lt;- model_variable_levels %&gt;%\n  left_join(model_results, by = join_by(term)) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from fancy data wrangling\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\n\nThe process is a little different with marginal effects because the term column is for the overarching variable (e.g., taxrate1) and the individual levels are built as contrasts in a column named contrast, like \"&lt;10k: 5% - &lt;10k: 0%\". We need to clean up that contrast column for joining with model_variable_levels.\n\n# Extract marginal effects\nmodel_results_mfx &lt;- model_svy %&gt;%\n  avg_slopes(newdata = \"mean\") %&gt;%\n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_data_mfx &lt;- model_variable_levels %&gt;%\n  left_join(\n    model_results_mfx,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_mfx,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from OLS marginal effects\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression instead of OLS\nWhat if you don’t want use a linear probability model and instead want to use a different family like logistic regression (which treats the outcome as actual 0 and 1 categories instead of numbers)? Or what if you have 3+ possible choices for outcomes and need to use a multinomial logistic regression model? We can still calculate AMCEs, but we can’t use raw regression coefficients. Instead we have to calculate response-scale (i.e. probability scale) marginal effects.\nLet’s make a logistic regression model:\n\nmodel_logit &lt;- survey::svyglm(\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  design = candidate_svy_design,\n  family = binomial(link = \"logit\")\n)\n\nCheck out those sweet sweet uninterpretable log odds:\n\ntidy(model_logit)\n## # A tibble: 33 × 5\n##    term                             estimate std.error statistic  p.value\n##    &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 (Intercept)                       -0.466     0.215     -2.17  3.07e- 2\n##  2 atmilitaryServed                   0.385     0.0775     4.96  1.22e- 6\n##  3 atreligionJewish                  -0.162     0.116     -1.40  1.62e- 1\n##  4 atreligionCatholic                -0.0651    0.123     -0.531 5.96e- 1\n##  5 atreligionMainline protestant     -0.0634    0.136     -0.468 6.40e- 1\n##  6 atreligionEvangelical protestant  -0.510     0.137     -3.72  2.44e- 4\n##  7 atreligionMormon                  -0.600     0.137     -4.39  1.64e- 5\n##  8 atedBaptist college                0.619     0.131      4.74  3.43e- 6\n##  9 atedCommunity college              0.663     0.131      5.06  7.73e- 7\n## 10 atedState university               0.828     0.126      6.57  2.43e-10\n## # ℹ 23 more rows\n\nWe can convert these log odds into probability-scale marginal effects with slopes() or avg_slopes() from {marginaleffects}. Conceptually this involves plugging in different covariate values—here we’re plugging in all the original values in the data into the model and then collapsing the predictions into averages. (Again, see this and this for more details about average marginal effects/slopes.)\n\nmfx_logit &lt;- model_logit %&gt;% \n  avg_slopes()\n\nmfx_logit %&gt;% as_tibble()\n## # A tibble: 32 × 8\n##    term  contrast                      estimate std.error statistic  p.value conf.low conf.high\n##    &lt;chr&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atage 45 - 36                        0.0258     0.0296     0.874 3.82e- 1  -0.0321   0.0838 \n##  2 atage 52 - 36                        0.0232     0.0303     0.765 4.44e- 1  -0.0362   0.0826 \n##  3 atage 60 - 36                        0.00406    0.0289     0.141 8.88e- 1  -0.0526   0.0607 \n##  4 atage 68 - 36                       -0.0639     0.0294    -2.17  3.00e- 2  -0.122   -0.00619\n##  5 atage 75 - 36                       -0.145      0.0289    -5.03  4.86e- 7  -0.202   -0.0888 \n##  6 ated  Baptist college - No BA        0.140      0.0290     4.82  1.45e- 6   0.0828   0.196  \n##  7 ated  Community college - No BA      0.150      0.0291     5.15  2.56e- 7   0.0928   0.207  \n##  8 ated  Ivy League university - No BA  0.269      0.0291     9.26  2.11e-20   0.212    0.326  \n##  9 ated  Small college - No BA          0.178      0.0274     6.50  8.07e-11   0.124    0.231  \n## 10 ated  State university - No BA       0.188      0.0277     6.80  1.04e-11   0.134    0.243  \n## # ℹ 22 more rows\n\nheck yes these percentage-point-scale estimates are basically the same as what we get from the LPM model!\n\n# Extract marginal effects\nmodel_results_logit_mfx &lt;- mfx_logit %&gt;%\n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_data_logit_mfx &lt;- model_variable_levels %&gt;%\n  left_join(\n    model_results_logit_mfx,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_logit_mfx,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from logistic regression marginal effects\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-1",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-1",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nNext we’ll calculate marginal means for these different attributes and levels, which are conditional averages and not regression coefficients. These are descriptive estimands/quantities of interest and they don’t rely on any reference category or baseline level.\n\nAutomatic estimates with cregg::mm()\nThe mm() function from {cregg} calculates marginal means automatically and returns them in a nice tidy data frame. I include the package namespace prefix here (i.e. cregg::mm() instead of just mm()) because I’ve loaded the {brms} package and it has its own mm() function that’s used for creating multi-membership grouping terms (whatever those are).\n\ncandidate_mms_auto &lt;- cregg::mm(\n  candidate,\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  id = ~ resID\n)\n\ncandidate_mms_auto %&gt;% as_tibble()\n## # A tibble: 40 × 10\n##    outcome  statistic feature          level                  estimate std.error     z         p lower upper\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;fct&gt;            &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 selected mm        Military Service Did Not Serve             0.458   0.00907  50.5 0         0.440 0.475\n##  2 selected mm        Military Service Served                    0.543   0.00914  59.4 0         0.525 0.560\n##  3 selected mm        Religion         None                      0.556   0.0200   27.8 1.88e-170 0.517 0.595\n##  4 selected mm        Religion         Jewish                    0.520   0.0186   28.0 2.77e-172 0.484 0.557\n##  5 selected mm        Religion         Catholic                  0.526   0.0177   29.7 2.82e-193 0.491 0.560\n##  6 selected mm        Religion         Mainline protestant       0.543   0.0194   28.0 3.93e-172 0.505 0.581\n##  7 selected mm        Religion         Evangelical protestant    0.437   0.0200   21.9 4.11e-106 0.398 0.476\n##  8 selected mm        Religion         Mormon                    0.417   0.0194   21.5 7.40e-103 0.379 0.455\n##  9 selected mm        College          No BA                     0.340   0.0186   18.3 1.12e- 74 0.303 0.376\n## 10 selected mm        College          Baptist college           0.482   0.0200   24.1 3.89e-128 0.443 0.521\n## # ℹ 30 more rows\n\n\nplot(cregg::mm())Original AMCE coefficent plot\n\n\n\nplot(candidate_mms_auto, vline = 0.5) + \n  guides(color = \"none\") +\n  theme_nice() +\n  labs(title = \"Marginal means from from cregg::mm()\")\n\n\n\n\n\n\n\n\n\n\nLeft panel of Figure 1 in Leeper, Hobolt, and Tilley (2020):\n\n\n\n\n\n\nQuick-and-dirty marginal means\nMarginal means are just conditional group averages, so we can actually get the same estimates with some basic {dplyr} grouping and summarizing. I’ll just show the averages for military and religion here for the sake of space, but the averages are the same as what we get with cregg::mm():\n\ncandidate %&gt;% \n  group_by(atmilitary) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 2 × 2\n##   atmilitary      avg\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 Did Not Serve 0.458\n## 2 Served        0.543\n\ncandidate %&gt;% \n  group_by(atreligion) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 6 × 2\n##   atreligion               avg\n##   &lt;fct&gt;                  &lt;dbl&gt;\n## 1 None                   0.556\n## 2 Jewish                 0.520\n## 3 Catholic               0.526\n## 4 Mainline protestant    0.543\n## 5 Evangelical protestant 0.437\n## 6 Mormon                 0.417\n\nBehind the scenes, cregg::mm() creates simple intercept-less models for each of the categorical terms in the model and then returns their coefficients. Again, for the sake of space, here are the regression-based averages for just military and religion:\n\nbind_rows(\n  tidy(lm(selected ~ 0 + atmilitary, data = candidate)),\n  tidy(lm(selected ~ 0 + atreligion, data = candidate))\n)\n## # A tibble: 8 × 5\n##   term                             estimate std.error statistic   p.value\n##   &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 atmilitaryDid Not Serve             0.458    0.0120      38.3 1.16e-267\n## 2 atmilitaryServed                    0.543    0.0120      45.3 0        \n## 3 atreligionNone                      0.556    0.0205      27.1 3.39e-147\n## 4 atreligionJewish                    0.520    0.0208      25.0 9.62e-127\n## 5 atreligionCatholic                  0.526    0.0205      25.6 1.34e-132\n## 6 atreligionMainline protestant       0.543    0.0209      26.0 1.36e-136\n## 7 atreligionEvangelical protestant    0.437    0.0209      20.9 1.53e- 91\n## 8 atreligionMormon                    0.417    0.0207      20.2 8.15e- 86\n\nRegardless of how we calculate them, we can see that these estimates are just group averages. 46% of respondents chose candidates who didn’t serve in the military; 54% chose candidates who did; and so on.\n\n\nMarginal means with {marginaleffects}\nCombining a bunch of smaller group_by() %&gt;% summarize() datasets, or combining a bunch of intercept-less models involves a bit of extra code and can get tedious. Plus it can get more complex when not using a linear model, or if you want interaction terms, or if you want group averages across multiple groups (i.e. average proportions for military across Republican and Democratic respondents). Additionally, the standard errors from these basic averages are wrong since they don’t take into account the nested structure of the data (i.e. respondents each have 6–12 responses).\nTo make life easier and more flexible, we can use marginal_means() from {marginaleffects} to calculate the unique categorical group means from a regression model.\n\nHow marginal_means() works\nBut first we need to look at a quick example to build the intuition behind what happens with marginal_means(). First we’ll make a simpler regresison model with just the military and religion features. We’ll then calculate predicted values for all the levels of those features using predictions()—this essentially involves plugging in all the unique values of atmilitary and atreligion into the model and finding the predicted values. We’ll then reshape the results a little so that we can see the average proportions of religion conditional on military service:\n\nmodel_candidate_simple &lt;- lm(\n  selected ~ atmilitary + atreligion, \n  data = candidate\n)\n\npredictions_simple &lt;- predictions(\n  model_candidate_simple,\n  by = c(\"atreligion\", \"atmilitary\")\n)\n\npredictions_simple_wide &lt;- predictions_simple %&gt;% \n  select(estimate, atmilitary, atreligion) %&gt;% \n  pivot_wider(names_from = \"atmilitary\", values_from = \"estimate\")\npredictions_simple_wide\n## # A tibble: 6 × 3\n##   atreligion             `Did Not Serve` Served\n##   &lt;fct&gt;                            &lt;dbl&gt;  &lt;dbl&gt;\n## 1 None                             0.514  0.598\n## 2 Jewish                           0.479  0.563\n## 3 Catholic                         0.483  0.567\n## 4 Mainline protestant              0.500  0.584\n## 5 Evangelical protestant           0.394  0.478\n## 6 Mormon                           0.376  0.460\n\nGreat. The average proportion for non-military Mormons is 37.6% and for military Mormons is 46%, and so on.\nIf we find the average of these averages and add a column and row in the literal right and bottom margins of the table, we’ll have marginal means of religion and military service:\n\npredictions_simple_wide %&gt;% \n  mutate(`Religion marginal mean` = (`Did Not Serve` + Served) / 2) %&gt;% \n  add_row(\n    atreligion = \"Military marginal mean\", \n    `Did Not Serve` = mean(predictions_simple_wide$`Did Not Serve`),\n    Served = mean(predictions_simple_wide$Served)\n  )\n## # A tibble: 7 × 4\n##   atreligion             `Did Not Serve` Served `Religion marginal mean`\n##   &lt;chr&gt;                            &lt;dbl&gt;  &lt;dbl&gt;                    &lt;dbl&gt;\n## 1 None                             0.514  0.598                    0.556\n## 2 Jewish                           0.479  0.563                    0.521\n## 3 Catholic                         0.483  0.567                    0.525\n## 4 Mainline protestant              0.500  0.584                    0.542\n## 5 Evangelical protestant           0.394  0.478                    0.436\n## 6 Mormon                           0.376  0.460                    0.418\n## 7 Military marginal mean           0.458  0.542                   NA\n\nTo me this is wild. The marginal mean for Mormons is 41%, which is basically what we found with group_by(atreligion) %&gt;% summarize() earlier. The marginal mean for candidates who served in the military is 54.2%, again roughly the same as what we found with group_by(atmilitary) %&gt;% summarize(...).\nInstead of manually creating these marginal rows and columns, the marginal_means() function will find those values automatically for us, based on a grid of values (in newdata) that it’ll plug into the model. Here it’ll plug all combinations of atreligion and atmilitary into the simple model. The wts = \"cells\" argument here makes it so that the marginal mean is weighted by the actual distribution of the levels of religion and military. For instance, imagine that only 30% of rows served in the military and 70% did not. Calculating the average of those two averages by just doing (Did Not Serve + Served) / 2 wouldn’t take that underlying distribution into account. Weighting by cells make it so that marginal_means() computes a weighted marginal mean proportional to each level’s frequency in the original data.\nLet’s let marginal_means() work its magic:\n\nmarginal_means_simple &lt;- marginal_means(\n  model_candidate_simple,\n  newdata = c(\"atreligion\", \"atmilitary\"),\n  wts = \"cells\"\n)\nmarginal_means_simple\n## \n##        Term                  Value  Mean Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n##  atmilitary Did Not Serve          0.458     0.0119 38.5   &lt;0.001 0.434  0.481\n##  atmilitary Served                 0.543     0.0119 45.5   &lt;0.001 0.519  0.566\n##  atreligion None                   0.556     0.0204 27.2   &lt;0.001 0.516  0.596\n##  atreligion Jewish                 0.520     0.0208 25.1   &lt;0.001 0.479  0.561\n##  atreligion Catholic               0.526     0.0205 25.7   &lt;0.001 0.485  0.566\n##  atreligion Mainline protestant    0.543     0.0208 26.1   &lt;0.001 0.502  0.584\n##  atreligion Evangelical protestant 0.437     0.0208 21.0   &lt;0.001 0.396  0.477\n##  atreligion Mormon                 0.417     0.0206 20.3   &lt;0.001 0.377  0.458\n## \n## Results averaged over levels of: atmilitary, atreligion \n## Columns: term, value, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nEt voilà, the averages here are basically the same as what we found in the manual version we did earlier. The only differences are due to the weighted averaging—by default marginal_means() assumes equal weights in the columns, so it’s literally going to just calculate (Did Not Serve + Served) / 2. With wts = \"cells\", it creates a weighted average: (Did Not Serve * cell_weight + Served * other_cell_weight) / 2.\n\n\nmarginal_means() with the full actual model\nNow that we understand what’s happening with marginal_means(), we can use it with the full model_svy model. We’ll find the marginal means for all the different features and make sure that we weight by cells so that we get weighted averages. Because there are so many combinations of attribute levels here, it takes a while to run:\n\n# This takes a while...\ntictoc::tic()\nmm_mfx &lt;- marginal_means(\n  model_svy, \n  newdata = c(\n    \"atmilitary\", \"atreligion\", \"ated\", \"atprof\", \n    \"atinc\", \"atrace\", \"atage\", \"atmale\"\n  ),\n  wts = \"cells\"\n)\ntictoc::toc()\n## 82.563 sec elapsed\n\nThe results are identical to what we find when using group_by() %&gt;% summarize(), but\n\nthe standard errors are correct (and actually present; we’d need to calculate those on our own with summarize() and that’s a pain), and\nwe have the ability to use other extra {marginaleffects} things like hypothesis tests, counterfactual estimates, p-value adjustments for multiple comparisons, clustered robust standard errors, and so on\n\n\nmarginal_means()group_by() %&gt;% summarize()\n\n\n\nmm_mfx %&gt;% as_tibble()\n## # A tibble: 40 × 8\n##    term       value                  estimate std.error statistic   p.value conf.low conf.high\n##    &lt;chr&gt;      &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atmilitary Did Not Serve             0.458   0.00913      50.1 0            0.440     0.476\n##  2 atmilitary Served                    0.543   0.00928      58.4 0            0.524     0.561\n##  3 atreligion None                      0.556   0.0195       28.5 1.64e-178    0.518     0.594\n##  4 atreligion Jewish                    0.520   0.0175       29.7 1.90e-194    0.486     0.554\n##  5 atreligion Catholic                  0.526   0.0177       29.7 3.14e-193    0.491     0.560\n##  6 atreligion Mainline protestant       0.543   0.0184       29.6 1.86e-192    0.507     0.579\n##  7 atreligion Evangelical protestant    0.437   0.0198       22.0 1.07e-107    0.398     0.475\n##  8 atreligion Mormon                    0.417   0.0189       22.1 8.22e-108    0.380     0.454\n##  9 ated       No BA                     0.340   0.0184       18.5 5.02e- 76    0.304     0.376\n## 10 ated       Baptist college           0.482   0.0198       24.4 1.63e-131    0.443     0.521\n## # ℹ 30 more rows\n\n\n\n\ncandidate %&gt;% \n  group_by(atmilitary) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 2 × 2\n##   atmilitary      avg\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 Did Not Serve 0.458\n## 2 Served        0.543\n\ncandidate %&gt;% \n  group_by(atreligion) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 6 × 2\n##   atreligion               avg\n##   &lt;fct&gt;                  &lt;dbl&gt;\n## 1 None                   0.556\n## 2 Jewish                 0.520\n## 3 Catholic               0.526\n## 4 Mainline protestant    0.543\n## 5 Evangelical protestant 0.437\n## 6 Mormon                 0.417\n\n\n\n\nSince there’s no reference level to deal with, plotting these these marginal means is pretty straightforward:\n\nplot_mm_mfx &lt;- mm_mfx %&gt;% \n  as_tibble() %&gt;% \n  mutate(value = fct_inorder(value)) %&gt;%\n  left_join(variable_lookup, by = join_by(term == variable)) %&gt;% \n  mutate(across(c(value, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_mm_mfx,\n  aes(x = estimate, y = value, color = variable_nice)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Marginal mean\",\n    y = NULL,\n    title = \"Marginal means with marginaleffects::marginal_means()\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences-in-amces-and-marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences-in-amces-and-marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Subgroup differences in AMCEs and marginal means",
    "text": "Subgroup differences in AMCEs and marginal means\nHainmueller, Hopkins, and Yamamoto (2014) did not include any individual respondent-level characteristics in their data, so we can’t look at how these causal effects differ across individual traits like party identification (Republicans and Democrats) or education or income or anything else like that.\nSo instead we’ll invent a pretend column for respondent-level party ID! To simplify life, we’ll look at differences between fake-party-ID within the military service history attribute. Arbitrarily (and super stereotypically), we’ll say that if a respondent selected a candidate more than 60% of the time when seeing that they had served in the military, there’s a 90% chance they’re a Republican. If they didn’t select the military candidate 60% of the time, there’s a 75% chance they’re a Democrat. AGAIN THESE PROBABILITIES ARE COMPLETELY ARBITRARY AND JUST CAME FROM MY HEAD—THEY ARE NOT REAL. We’ll make a new dataset called candidate_fake with a column named respondent_party for each respondent’s fake party:\n\n\nCode for generating fake respondent_party column\n# Wrap this in withr::with_seed() so that the randomness is reproducible but the\n# overall document seed doesn't get set or messed up\nwithr::with_seed(1234, {\n  respondents_party_military &lt;- candidate %&gt;%\n    group_by(resID, atmilitary) %&gt;%\n    # Find the proportion of times each respondent selected the candidate when\n    # military service history was \"Served\"\n    summarize(prob_select = mean(selected)) %&gt;%\n    filter(atmilitary == \"Served\") %&gt;%\n    select(-atmilitary) %&gt;%\n    ungroup() %&gt;%\n    # If a respondent selected the candidate more than 60% of the time when\n    # seeing that they had served in the military, there's a 90% chance they're\n    # a Republican. If they didn't select the military candidate 60% of the\n    # time, there's a 75% chance they're a Democrat.\n    mutate(respondent_party = case_when(\n      prob_select &gt;= 0.6 ~\n        sample(\n          c(\"Democrat\", \"Republican\"), n(),\n          replace = TRUE, prob = c(0.1, 0.9)\n        ),\n      prob_select &lt; 0.6 ~\n        sample(\n          c(\"Democrat\", \"Republican\"), n(),\n          replace = TRUE, prob = c(0.75, 0.25)\n        )\n    )) %&gt;%\n    mutate(respondent_party = factor(respondent_party))\n})\n\ncandidate_fake &lt;- candidate %&gt;% \n  left_join(respondents_party_military, by = join_by(resID))\n\n\nLet’s check the average of selected across both the candidate military attribute and our new fake respondent party to see how all that random assignment shook out:\n\ncandidate_fake %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 4 × 3\n## # Groups:   atmilitary [2]\n##   atmilitary    respondent_party   avg\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;\n## 1 Did Not Serve Democrat         0.551\n## 2 Did Not Serve Republican       0.378\n## 3 Served        Democrat         0.447\n## 4 Served        Republican       0.619\n\nCool cool. Republican respondents are way more favorable towards candidates who served in the military (61.9%) than Democratic respondents (44.8%). And that kind of interpretation is actually mathematically and conceptually legal and recommended by Leeper, Hobolt, and Tilley (2020)—technically these are subgroup marginal means and what we should be looking at for descriptive purposes already, but more on that below.\n\nConditional AMCEs\nLet’s first look at the two different party-based causal effects of switching a candidate from having no military history to having served in the military. For the sake of simplicity we’ll just use regular OLS instead of logistic regression, and we’ll use marginaleffects::avg_slopes() to find the marginal since the regression model involves interaction terms. We’ll also only look at the atmilitary feature instead of all the features, since it’s the only one where we built in a party effect.\nWe can find conditional AMCEs with {cregg} using the cj() function, which is a general function for all of the different estimands that {cregg} can calculate:\n\ncregg::cj(\n  candidate_fake, \n  selected ~ atmilitary, \n  id = ~resID, \n  estimate = \"amce\", \n  by = ~respondent_party\n)\n##           BY  outcome statistic          feature         level estimate std.error      z         p   lower    upper respondent_party\n## 1   Democrat selected      amce Military Service Did Not Serve   0.0000        NA     NA        NA      NA       NA         Democrat\n## 2   Democrat selected      amce Military Service        Served  -0.1032   0.02063 -5.003 5.655e-07 -0.1437 -0.06278         Democrat\n## 3 Republican selected      amce Military Service Did Not Serve   0.0000        NA     NA        NA      NA       NA       Republican\n## 4 Republican selected      amce Military Service        Served   0.2405   0.02246 10.710 9.095e-27  0.1965  0.28455       Republican\n\nWe can do the same thing using regression if we use an interaction term for the subgroup:\n\nmodel_military_party &lt;- lm(\n  selected ~ atmilitary * respondent_party, \n  data = candidate_fake\n)\n\ntidy(model_military_party)\n## # A tibble: 4 × 5\n##   term                                        estimate std.error statistic   p.value\n##   &lt;chr&gt;                                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)                                    0.551    0.0174     31.7  7.90e-194\n## 2 atmilitaryServed                              -0.103    0.0248     -4.16 3.21e-  5\n## 3 respondent_partyRepublican                    -0.172    0.0236     -7.28 3.97e- 13\n## 4 atmilitaryServed:respondent_partyRepublican    0.344    0.0335     10.3  2.48e- 24\n\nThose coefficients by themselves aren’t super informative without doing some tricky algebra to piece everything together. Instead, we can use avg_slopes() from {marginaleffects} to find the partial derivative (or AMCE) for atmilitary across each party. These are the same results we get from {cregg}:\n\nparty_amces &lt;- model_military_party %&gt;% \n  avg_slopes(\n    variables = \"atmilitary\",\n    by = \"respondent_party\"\n  )\nparty_amces\n## \n##        Term                           Contrast respondent_party Estimate Std. Error     z Pr(&gt;|z|)  2.5 %  97.5 %\n##  atmilitary mean(Served) - mean(Did Not Serve)       Democrat     -0.103     0.0248 -4.16   &lt;0.001 -0.152 -0.0546\n##  atmilitary mean(Served) - mean(Did Not Serve)       Republican    0.241     0.0226 10.66   &lt;0.001  0.196  0.2847\n## \n## Columns: term, contrast, respondent_party, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n\nClick to show the code since it’s so long; you can make a quick basic version of it with two calls to plot(amce(...))—one on data filtered to only include Democrats and one that only includes Republicans\n# Clean up marginal effects\nparty_amces_mfx &lt;- party_amces %&gt;%\n  # The contrast column contains values like this:\n  #   mean(Served) - mean(Did Not Serve)\n  # I could probably use some fancy regex to extract those things, but here I'll\n  # just brute force it and remove \"mean(\" and \")\" with two separate\n  # str_remove()s\n  mutate(\n    contrast = str_remove_all(contrast, \"mean\\\\(\"),\n    contrast = str_remove_all(contrast, \"\\\\)\")\n  ) %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_amces_party_mfx &lt;- expand_grid(\n  respondent_party = levels(candidate_fake$respondent_party),\n  filter(model_variable_levels, variable == \"atmilitary\")\n) %&gt;%\n  left_join(\n    party_amces_mfx,\n    by = join_by(variable == term, levels == variable_level, respondent_party)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_amce(estimate),\n    estimate == 0 ~ NA\n  ))\n\np_mfx &lt;- ggplot(\n  plot_amces_party_mfx, \n  aes(x = estimate, y = levels, color = respondent_party)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(\n    aes(xmin = conf.low, xmax = conf.high),\n    position = position_dodge(width = 0.15)\n  ) +\n  geom_label(\n    aes(label = estimate_nice), \n    position = position_dodge(width = -1.2),\n    size = 3.5, show.legend = FALSE\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = parties) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    color = NULL,\n    title = \"AMCEs by respondent party\"\n  ) +\n  facet_wrap(vars(variable_nice)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\np_mfx\n\n\n\n\n\n\n\n\n\nThese have a straightforward causal interpretation. Among Democratic-identifying respondents, flipping a hypothetical candidate’s military status from “did not serve” to “serve” causes a 10 percentage point drop in favorability (or in the probability of being selected). Among Republican-identifying respondents, a candidate’s military service causes a 24 percentage point increase in favorability. Both effects are statistically significantly different from zero.\n\n\nConditional marginal means\nIf we’re describing overall trends in the data, though, these conditional AMCEs are misleading. It’s really tempting to look at this and conclude that there’s a 34ish percentage point difference between Democrats and Republicans when they’re presented with a candidate with military service, since that’s the distance between the two AMCEs in the plot. But that’s wrong! That distance is an illusion—a mirage caused by the fact that the AMCEs are based on a reference category that is set to zero.\n\n\nCode\np_mfx +\n  annotate(\n    geom = \"errorbar\",\n    x = 0, xmin = -0.103, xmax = 0.241, y = 2,\n    width = 0.1, color = \"black\"\n  ) +\n  annotate(\n    geom = \"label\", \n    x = 0.241 - (0.241 - -0.103) / 2, y = 2,\n    label = \"This difference isn't\\nwhat you think it is!\", \n    size = 5,\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 0.05, xend = 0.01, y = 1, yend = 1,\n    arrow = arrow(angle = 30, length = grid::unit(0.5, \"lines\"))\n  ) +\n  annotate(\n    geom = \"label\", \n    x = 0, y = 1,\n    label = \"These 0s mess things up!\", \n    size = 5,\n    hjust = -0.2\n  ) \n\n\n\n\n\n\n\n\n\nTo get the correct party-based difference in support for candidates, we need to find the party-based marginal means of support for the different levels of the military feature and then talk about those differences. As discussed above, this technically just involves finding the conditional averages across groups—in this case the average outcome within the two levels of “Served” and “Did not serve” between Republican and Democratic respondents. We can find these marginal means with some basic group_by() %&gt;% summarize() and the difference between Republican and Democratic marginal means with some pivoting and subtracting:\n\nparty_mms_manual &lt;- candidate_fake %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  summarize(avg = mean(selected))\nparty_mms_manual\n## # A tibble: 4 × 3\n## # Groups:   atmilitary [2]\n##   atmilitary    respondent_party   avg\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;\n## 1 Did Not Serve Democrat         0.551\n## 2 Did Not Serve Republican       0.378\n## 3 Served        Democrat         0.447\n## 4 Served        Republican       0.619\n\nparty_mms_manual %&gt;% \n  pivot_wider(names_from = \"respondent_party\", values_from = \"avg\") %&gt;% \n  mutate(mm_diff = Republican - Democrat)\n## # A tibble: 2 × 4\n## # Groups:   atmilitary [2]\n##   atmilitary    Democrat Republican mm_diff\n##   &lt;fct&gt;            &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Did Not Serve    0.551      0.378  -0.172\n## 2 Served           0.447      0.619   0.172\n\nOr we can use cregg::cj(..., estimate = \"mm_diff\") to do that automatically:\n\ncj(\n  candidate_fake, \n  selected ~ atmilitary, \n  id = ~resID, \n  estimate = \"mm_diff\", \n  by = ~respondent_party\n)\n##                      BY     statistic  outcome          feature         level estimate std.error      z         p   lower   upper respondent_party\n## 1 Republican - Democrat mm_difference selected Military Service Did Not Serve  -0.1722   0.01531 -11.25 2.361e-29 -0.2022 -0.1422       Republican\n## 2 Republican - Democrat mm_difference selected Military Service        Served   0.1715   0.01574  10.89 1.219e-27  0.1407  0.2024       Republican\n\nOr, to be extra thorough and allow for any type of regression family (logisitic! multinomial!) with any other types of covariates, we can find these manually with our own regression model fed through marginaleffects::marginal_means(). By specifying cross = TRUE, we get all combinations of military service and party (without it, we’d get separate marginal means for just the two levels of military and the two levels of party).\n\nparty_mms_mfx &lt;- marginal_means(\n  model_military_party,\n  newdata = c(\"atmilitary\", \"respondent_party\"),\n  cross = TRUE,\n  wts = \"cells\"\n)\nparty_mms_mfx %&gt;% as_tibble()\n## # A tibble: 4 × 11\n##   rowid atmilitary    respondent_party estimate std.error statistic   p.value conf.low conf.high selected   wts\n##   &lt;int&gt; &lt;fct&gt;         &lt;fct&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Did Not Serve Democrat            0.551    0.0174      31.7 1.68e-220    0.517     0.585      0.5 0.231\n## 2     2 Did Not Serve Republican          0.378    0.0160      23.6 3.56e-123    0.347     0.410      0.5 0.271\n## 3     3 Served        Democrat            0.447    0.0177      25.3 3.11e-141    0.413     0.482      0.5 0.222\n## 4     4 Served        Republican          0.619    0.0159      39.0 0            0.588     0.650      0.5 0.276\n\nTo find the differences in those marginal means, we could pivot wider and subtract the Democrat column from the Republican column, or we can use the hypothesis argument in marginal_means() to have {marginaleffects} automatically calculate differences between categories for us without needing to pivot. If we were only concerned with one contrast, like Republican − Democrat, we could specify hypothesis = \"pairwise\" and it would subtract the two groups’ marginal means. However, we want two differences: Republican − Democrat in both the “served” and the “did not serve” levels. As seen above, party_mms_mfx has four rows in it. We want the differences between rows 2 and 1 (Republican − Democrat for “Did not serve”) and between rows 4 and 3 (Republican − Democrat for “Served”). We can control which rows are used when calculating differences with a vector of linear combinations. If we use a vector like c(-1, 1, 0, 0), {marginaleffects} will essentially make the first row negative, leave the second row as is, and give no weight to (or ignore) the third and fourth row, which will calculate the difference between rows 2 and 1. Similarly, c(0, 0, -1, 1) will ignore rows 1 and 2 and find the difference between row 4 and 3. If we feed marginal_means() a matrix of these two vectors, with each vector as a column, it’ll find the differences for us:\n\ngroup_diffs_terms &lt;- matrix(\n  c(-1, 1, 0, 0,\n    0, 0, -1, 1),\n  ncol = 2\n) %&gt;% \n  magrittr::set_colnames(levels(candidate_fake$atmilitary))\ngroup_diffs_terms\n##      Did Not Serve Served\n## [1,]            -1      0\n## [2,]             1      0\n## [3,]             0     -1\n## [4,]             0      1\n\nparty_mms_mfx_diff &lt;- marginal_means(\n  model_military_party,\n  newdata = c(\"atmilitary\", \"respondent_party\"),\n  cross = TRUE,\n  wts = \"cells\",\n  hypothesis = group_diffs_terms\n) %&gt;% \n  as_tibble()\nparty_mms_mfx_diff\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Did Not Serve   -0.172    0.0236     -7.28 3.22e-13   -0.219    -0.126\n## 2 Served           0.172    0.0238      7.22 5.23e-13    0.125     0.218\n\nWe can plot these conditional marginal means along with the party-based differences:\n\n\nClick to see all the plotting code\nmm_party_plot1 &lt;- party_mms_mfx %&gt;% \n  as_tibble() %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_percent()(estimate),\n    estimate == 0 ~ NA\n  )) %&gt;% \n  ggplot(aes(x = estimate, y = atmilitary, color = respondent_party)) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(\n    aes(label = estimate_nice), \n    position = position_dodge(width = -1.2),\n    size = 3.5, show.legend = FALSE\n  ) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_color_manual(values = parties) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Marginal means by respondent party\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_party_plot2 &lt;- party_mms_mfx_diff %&gt;% \n  mutate(estimate_nice = label_amce(estimate)) %&gt;% \n  ggplot(aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(\n    xmin = conf.low, xmax = conf.high, \n    color = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  geom_label(\n    aes(label = estimate_nice), size = 3.5,\n    nudge_y = 0.3, color = \"#85144b\"\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = \"#85144b\") +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means by respondent party\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_party_plot1 | mm_party_plot2\n\n\n\n\n\n\n\n\n\nThese marginal means have a direct descriptive interpretation. In the left panel above we can see that Democratic respondents tend to prefer candidates that haven’t served in the military (55%) to those that have (45%), but compared to Republican respondents, this divergence isn’t as dramatic. Republicans tend to strongly prefer candidates with a military history, with 62% expressing favorability. In general, Republicans are more extreme in their preferences for and against candidates with and without military service history.\nIn the right panel we can see the differences between the parties’ marginal means within each level. There’s a 17 percentage point distance between Republican and Democratic marginal means within the served level and a 17 percentage point distance between their marginal means in the did not serve level. Overall, Republicans prefer candidates with military service; Democrats prefer candidates without military service."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#the-overall-model",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#the-overall-model",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "The overall model",
    "text": "The overall model\nThe easiest way for me to think about all these estimands is as moving parts (marginal effects or partial derivatives) in a regression model. We can model the whole data-generating system (i.e. all the candidate features and levels + any subgroups or covariates we’re interested in) and then look at individual parts of that system for the different estimands we’re interested in.\nEach survey respondent saw multiple pairs of hypothetical candidates—some saw 4, some 5, some 6, and so on:\n\ncandidate %&gt;% \n  count(resID) %&gt;% \n  mutate(pairs_seen = n / 2)\n## # A tibble: 311 × 3\n##    resID              n pairs_seen\n##    &lt;fct&gt;          &lt;int&gt;      &lt;dbl&gt;\n##  1 A10ZOUOZZ3EOAJ    12          6\n##  2 A11F3HMX0N23V4    10          5\n##  3 A12H2RTXSAQPRH     8          4\n##  4 A13DPXX91VQ49Q    12          6\n##  5 A13KTOZC30NBS6    10          5\n##  6 A142W1RAF1TBWP    12          6\n##  7 A15A4X84A1CJPF    12          6\n##  8 A15Q5F9YWO45EI    12          6\n##  9 A1610EPXM31F9D    12          6\n## 10 A1650FELH3UL2F    10          5\n## # ℹ 301 more rows\n\nThis means that we have a natural multilevel structure in our data. Individual candidate selection choices are nested inside respondents:\n\n\n\n\n\nMultilevel conjoint data structure, with candidate choices \\(y\\) nested in respondents\n\n\n\n\nWe want to model candidate selection (selected) based on candidate characteristics (and maybe individual respondent characteristics, if we had those). We’ll use the subscript \\(i\\) to refer to individual candidate choices and \\(j\\) to refer to respondents, which each contain multiple \\(i\\)s.\nSince candidate selection selected is binary, we can model it as a Bernoulli process that has a probability \\(\\pi_{i_j}\\) of success. We’ll model that \\(\\pi_{i_j}\\) using a logistic regression model with covariates for each of the levels of each candidate feature. To account for respondent-level differences in probabilities, we’ll use respondent-specific offsets (\\(b_{0_j}\\)) from the global success rate, thus creating random intercepts. We’ll specify priors for each of the logit-scale coefficients/partial derivatives and the between respondent variability (\\(\\sigma_0\\)). If we really wanted we could specify priors for each individual coefficient, but for simplicity we’ll just use a normal distribution with a mean of 0 and a standard deviation of 1 for all of them (since logit-scale coefficients don’t ever get really big). We’ll use an exponential prior for (\\(\\sigma_0\\)) because we don’t know much about it.\nHere’s what that all looks like more formally:\n\n\\[\n\\begin{aligned}\n\\text{Selection}_{i_j} \\sim&\\ \\operatorname{Bernoulli}(\\pi_{i_j}) & \\text{Probability of selection for choice}_i \\text{ in respondent}_j \\\\\n\\operatorname{logit}(\\pi_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{Military[Served]}_{i_j} + & \\text{Model for probability}\\\\\n&\\ \\beta_2\\, \\text{Religion[Mormon]}_{i_j} + \\\\\n&\\ \\beta_3\\, \\text{Religion[Evangelical]}_{i_j} + \\\\\n&\\ \\dots +\\ \\\\\n&\\ \\beta_n\\, \\text{Sex[Female]}_{i_j} \\\\\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) & \\text{Respondent-specific offsets from global success rate} \\\\\n\\\\\n\\beta_{0_c} \\sim&\\ \\mathcal{N}(0, 1) & \\text{Prior for global average success rate} \\\\\n\\beta_1 \\dots \\beta_n \\sim&\\ \\mathcal{N}(0, 1) & \\text{Prior for candidate feature levels} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for between-respondent variability}\n\\end{aligned}\n\\]\n\nLet’s build the model!\n\npriors &lt;- c(\n  prior(normal(0, 1), class = Intercept),\n  prior(normal(0, 1), class = b),\n  prior(exponential(1), class = sd)\n)\n\nmodel_brms &lt;- brm(\n  bf(selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale +\n    (1 | resID)),\n  data = candidate,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"candidate_model_brms\"\n)"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-1",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-1",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nIf we’re interested in the causal effect of specific candidate levels, we need to find the average marginal component effect (AMCE). Here’s the formal definition of the no religion → Mormon AMCE, for example:\n\\[\n\\begin{aligned}\n\\theta =\\ &P [\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{none})]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{Mormon})]\n\\end{aligned}\n\\]\nWe can find this AMCE (and all the other AMCEs) by calculating the marginal effect/partial derivative of each candidate-level covariate. marginaleffects::avg_slopes() makes this easy and gives us percentage-point-scale estimates instead of logit-scale estimates\n\n\n\n\n\n\nDealing with respondent offsets\n\n\n\nWhen plugging values into avg_slopes (or predictions() or marginal_means() or any function that calculates predictions from a model), we have to decide how to handle the random respondent offsets (\\(b_{0_j}\\)). I have a whole other blog post guide about this and how absolutely maddening the nomenclature for all this is.\nBy default, avg_slopes() and friends will calculate the effects for a typical respondent, or a respondent where the random offset is set to 0. It invisibly uses the re_formula = NA argument to do this. This is also called a conditional effect.\nWe could also use re_formula = NULL to calculate the effect for respondents on average. This is also called a marginal effect. (ARGH I HATE THESE NAMES.). This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average respondent.\n\nConditional effect = average respondent = re_formula = NA (default)\nMarginal effect = respondents on average = re_formula = NULL + existing respondent levels or a new simulated respondent\n\nAgain, see this guide for way more about these distinctions. In this example here, we’ll just use conditional effects, or the effect for an average respondent.\n\n\n\nposterior_mfx &lt;- model_brms %&gt;% \n  avg_slopes(newdata = \"mean\", allow_new_levels = TRUE) %&gt;% \n  posteriordraws() \n\nposterior_mfx_nested &lt;- posterior_mfx %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\n# Combine full dataset of factor levels with model results\nplot_data_bayes &lt;- model_variable_levels %&gt;%\n  left_join(\n    posterior_mfx_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  guides(fill = \"none\") +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"Posterior AMCEs\"\n  )\n\n\n\n\n\n\n\n\nThe results here are all basically the same as what we found with all the frequentist approaches earlier, but now we have full posteriors for each of these AMCEs so we can do all sorts of neat Bayesian inference things, like calculating the probability that the effect is larger than zero. For instance, there’s a 100% posterior probability that the None → Mormon effect is negative. The Business owner → Lawyer effect, on the other hand, is generally negative, but sometimes positive—there’s a 78% posterior probability that it’s negative.\n\nexample_amces &lt;- posterior_mfx %&gt;% \n  filter(\n    term == \"atreligion\" & contrast == \"Mormon - None\" |\n    term == \"atprof\" & contrast == \"Lawyer - Business owner\"\n  )\n\nexample_amces_p_direction &lt;- example_amces %&gt;% \n  group_by(contrast) %&gt;% \n  summarize(prop_lt_0 = sum(draw &lt; 0) / n()) %&gt;% \n  mutate(\n    prob_nice = label_percent()(prop_lt_0),\n    label = glue::glue(\"P(θ &lt; 0) = {prob_nice}\")\n  )\n\nggplot(example_amces, aes(x = draw)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt; 0)), fill = \"grey50\") +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = example_amces_p_direction, \n    aes(x = -0.21, y = 0.9, label = label),\n    hjust = 0\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_ramp_discrete(from = \"#FF851B\", guide = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\", \n    y = NULL, \n    title = \"Two example AMCEs\"\n  ) +\n  facet_wrap(vars(fct_rev(contrast))) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-2",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-2",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nInstead of working with causal AMCEs, which are all relative to an omitted feature level, we can work with absolute marginal means to be more descriptive with our estimands. We could find the overall level of favorability of Mormon candidates…\n\\[\n\\theta = P(\\text{Candidate selection} \\mid \\text{Religion = Mormon})\n\\]\n…or the difference between Mormon and Catholic favorability…\n\\[\n\\begin{aligned}\n\\theta =\\ &P[\\text{Candidate selection} \\mid \\text{Religion = Mormon}]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\text{Religion = Catholic}]\n\\end{aligned}\n\\]\nUnfortunately marginaleffects::marginal_means() doesn’t work with brms models. BUT we can fake it by finding the posterior marginal means for each of the features individually and then combining them into one big data frame.\n\n\nClick to show the code. It’s hidden because it’s long and repetitive.\n# There's probably a more efficient way to do with with mapping or loops or\n# whatever but I don't want to figure it out right now, so we brute force it\nposterior_mms &lt;- bind_rows(\n  atmilitary = predictions(\n    model_brms,\n    by = \"atmilitary\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atmilitary) %&gt;% posteriordraws(),\n  atreligion = predictions(\n    model_brms,\n    by = \"atreligion\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atreligion) %&gt;% posteriordraws(),\n  ated = predictions(\n    model_brms,\n    by = \"ated\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = ated) %&gt;% posteriordraws(),\n  atprof = predictions(\n    model_brms,\n    by = \"atprof\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atprof) %&gt;% posteriordraws(),\n  atprof = predictions(\n    model_brms,\n    by = \"atprof\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atprof) %&gt;% posteriordraws(),\n  atinc = predictions(\n    model_brms,\n    by = \"atinc\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atinc) %&gt;% posteriordraws(),\n  atrace = predictions(\n    model_brms,\n    by = \"atrace\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atrace) %&gt;% posteriordraws(),\n  atage = predictions(\n    model_brms,\n    by = \"atage\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atage) %&gt;% posteriordraws(),\n  atmale = predictions(\n    model_brms,\n    by = \"atmale\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atmale) %&gt;% posteriordraws(),\n  .id = \"term\"\n) %&gt;% \n  as_tibble()\nposterior_mms\n## # A tibble: 184,000 × 7\n##    term       drawid  draw value         estimate conf.low conf.high\n##    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atmilitary 1      0.455 Did Not Serve    0.458    0.436     0.481\n##  2 atmilitary 1      0.546 Served           0.542    0.520     0.564\n##  3 atmilitary 2      0.459 Did Not Serve    0.458    0.436     0.481\n##  4 atmilitary 2      0.531 Served           0.542    0.520     0.564\n##  5 atmilitary 3      0.463 Did Not Serve    0.458    0.436     0.481\n##  6 atmilitary 3      0.543 Served           0.542    0.520     0.564\n##  7 atmilitary 4      0.450 Did Not Serve    0.458    0.436     0.481\n##  8 atmilitary 4      0.544 Served           0.542    0.520     0.564\n##  9 atmilitary 5      0.468 Did Not Serve    0.458    0.436     0.481\n## 10 atmilitary 5      0.554 Served           0.542    0.520     0.564\n## # ℹ 183,990 more rows\n\n\nHere are the marginal means for all the levels of all candidate features:\n\nplot_posterior_mms &lt;- posterior_mms %&gt;% \n  left_join(variable_lookup, by = join_by(term == variable)) %&gt;% \n  mutate(across(c(value, variable_nice), ~fct_inorder(.)))\n  \nggplot(\n  plot_posterior_mms,\n  aes(x = draw, y = value, fill = variable_nice)\n) +\n  geom_vline(xintercept = 0.5) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_percent()) +\n  guides(fill = \"none\") +\n  labs(\n    x = \"Marginal means of probabilities\",\n    y = NULL,\n    title = \"Posterior marginal means\"\n  )\n\n\n\n\n\n\n\n\nAnd here are the two we mentioned at the beginning of this section—overall favorability of Mormon candidates and the difference between Mormon and Catholic favorability. Mormon candidates have a median posterior favorability of 41.8%, with a 95% credible interval of 38–46%. The median posterior difference between Mormon and Catholic favorability is 10.8 percentage points, with a 95% credible interval of 5–16 percentage points. Neat.\n\nmms_mormon &lt;- posterior_mms %&gt;% \n  filter(term == \"atreligion\", value == \"Mormon\")\nmms_mormon %&gt;% median_qi(draw)\n## # A tibble: 1 × 6\n##    draw .lower .upper .width .point .interval\n##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0.418  0.381  0.456   0.95 median qi\n\nmms_mormon_catholic &lt;- posterior_mms %&gt;% \n  filter(term == \"atreligion\", value %in% c(\"Mormon\", \"Catholic\")) %&gt;% \n  select(drawid, draw, value) %&gt;% \n  pivot_wider(names_from = \"value\", values_from = \"draw\") %&gt;% \n  mutate(diff = Mormon - Catholic)\nmms_mormon_catholic %&gt;% median_qi(diff)\n## # A tibble: 1 × 6\n##     diff .lower  .upper .width .point .interval\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 -0.107 -0.160 -0.0537   0.95 median qi\n\n\nmm1 &lt;- ggplot(mms_mormon, aes(x = draw)) +\n  stat_halfeye(fill = \"#FF851B\") +\n  facet_wrap(vars(\"Mormon favorability\")) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Marginal mean\", y = NULL) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )\n\nmm2 &lt;- ggplot(mms_mormon_catholic, aes(x = diff)) +\n  stat_halfeye(fill = \"#B10DC9\") +\n  facet_wrap(vars(\"Difference between Mormons and Catholics\")) +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Difference in marginal means\", y = NULL) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )\n\nmm1 | mm2"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Subgroup differences",
    "text": "Subgroup differences\nFinally, if we’re interested in subgroup differences, we can run a separate model with an interaction term for the subgroups we’re interested in. Again, this candidate experiment didn’t include any respondent-level covariates, so we’ll use the made-up, fake respondent political party that we used earlier.\n\npriors &lt;- c(\n  prior(normal(0, 1), class = Intercept),\n  prior(normal(0, 1), class = b),\n  prior(exponential(1), class = sd)\n)\n\nmodel_brms_military_party &lt;- brm(\n  bf(selected ~ atmilitary * respondent_party + (1 | resID)),\n  data = candidate_fake,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"candidate_model_brms_interaction\"\n)\n\n\nConditional AMCEs\nWe can calculate two different causal estimands, the conditional AMCE across each respondent political party of switching a candidate from having no military history to having served in the military. Because this model (1) uses logit-scale coefficients and (2) splits the causal effects across a bunch of different regression terms, we’ll use marginaleffects::avg_slopes() to find the group-specific probability-scale marginal effects.\nAmong Republican respondents, the causal effect of a candidate switching from no military history to having military service history has a posterior median of 23.8 percentage points, with a 95% credible interval of 19–28 percentage points. For Democrats, the same causal effect is negative, with a posterior median of −9.8 percentage points and a 95% credible interval of −5 to −14.7 percentage points.\n\nposterior_party_amces &lt;- model_brms_military_party %&gt;% \n  avg_slopes(\n    newdata = \"mean\",\n    variables = \"atmilitary\",\n    by = \"respondent_party\",\n    allow_new_levels = TRUE\n  ) %&gt;% \n  posteriordraws()\n\nposterior_party_amces %&gt;% \n  group_by(respondent_party) %&gt;% \n  median_qi(draw)\n## # A tibble: 2 × 7\n##   respondent_party    draw .lower  .upper .width .point .interval\n##   &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Democrat         -0.0982 -0.148 -0.0481   0.95 median qi       \n## 2 Republican        0.238   0.194  0.279    0.95 median qi\n\n\n\nClick to show the code since it’s so long\nposterior_party_amces_wide &lt;- posterior_party_amces %&gt;%\n  # The contrast column contains values like this:\n  #   mean(Served) - mean(Did Not Serve)\n  # I could probably use some fancy regex to extract those things, but here I'll\n  # just brute force it and remove \"mean(\" and \")\" with two separate\n  # str_remove()s\n  mutate(\n    contrast = str_remove_all(contrast, \"mean\\\\(\"),\n    contrast = str_remove_all(contrast, \"\\\\)\")\n  ) %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_posterior_party_amces &lt;- expand_grid(\n  respondent_party = levels(candidate_fake$respondent_party),\n  filter(model_variable_levels, variable == \"atmilitary\")\n) %&gt;%\n  left_join(\n    posterior_party_amces_wide,\n    by = join_by(variable == term, levels == variable_level, respondent_party)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(draw, estimate),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_amce(estimate),\n    estimate == 0 ~ NA\n  ))\n\nggplot(\n  plot_posterior_party_amces, \n  aes(x = draw, y = levels, color = respondent_party, fill = respondent_party)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(position = position_dodge(width = 0.15)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(\n    values = parties,\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = parties),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(parties, 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    color = NULL,\n    title = \"Posterior AMCEs by respondent party\"\n  ) +\n  facet_wrap(vars(variable_nice)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nConditional marginal means\nAnd finally, we can calculate subgroup conditional marginal means to describe general party-based trends (since the relative conditional AMCEs create an illusion of difference). Again, since marginaleffects::marginal_means() doesn’t work with brms models, we can instead use marginaleffects::predictions(), which still works with the neat hypothesis argument for calculating contrasts.\nIn the left panel below, Democratic respondents prefer candidates that haven’t served in the military (55% median posterior; 95% credible interval: 51–58%) to those that have (45% median posterior; 95% credible interval: 41–48%). Republicans strongly prefer candidates with a military history, with a posterior median 62% expressing favorability (95% credible interval: 59–65%). In general, Republicans are more extreme in their preferences for and against candidates with and without military service history.\nIn the right panel we can see the differences between the parties’ marginal means within each level. There’s a posterior median 17 percentage point distance between Republican and Democratic marginal means within the served level and a posterior median 17 percentage point distance between their marginal means in the did not serve level (95% credible interval: 12–21 percentage points). Overall, Republicans prefer candidates with military service; Democrats prefer candidates without military service.\n\nposterior_party_mms &lt;- predictions(\n  model_brms_military_party,\n  by = c(\"atmilitary\", \"respondent_party\"),\n  allow_new_levels = TRUE\n) %&gt;% \n  posterior_draws()\n\nposterior_party_mms %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  median_qi(draw)\n## # A tibble: 4 × 8\n##   atmilitary    respondent_party  draw .lower .upper .width .point .interval\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Did Not Serve Democrat         0.548  0.513  0.582   0.95 median qi       \n## 2 Did Not Serve Republican       0.380  0.351  0.412   0.95 median qi       \n## 3 Served        Democrat         0.450  0.415  0.485   0.95 median qi       \n## 4 Served        Republican       0.618  0.587  0.648   0.95 median qi\n\ngroup_diffs_terms &lt;- matrix(\n  c(-1, 1, 0, 0,\n    0, 0, -1, 1),\n  ncol = 2\n) %&gt;% \n  magrittr::set_colnames(levels(candidate_fake$atmilitary))\n\nposterior_party_mms_diff &lt;- predictions(\n  model_brms_military_party,\n  by = c(\"atmilitary\", \"respondent_party\"),\n  allow_new_levels = TRUE,\n  hypothesis = group_diffs_terms\n) %&gt;% \n  posterior_draws()\n\nposterior_party_mms_diff %&gt;% \n  group_by(term) %&gt;% \n  median_qi(draw)\n## # A tibble: 2 × 7\n##   term            draw .lower .upper .width .point .interval\n##   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Did Not Serve -0.167 -0.213 -0.121   0.95 median qi       \n## 2 Served         0.168  0.120  0.215   0.95 median qi\n\n\n\nClick to show the code since it’s so long\nmm_posterior_party1 &lt;- ggplot(\n  posterior_party_mms, \n  aes(x = draw, y = atmilitary, color = respondent_party, fill = respondent_party)\n) +\n  geom_vline(xintercept = 0.5) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_color_manual(\n    values = parties,\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = parties),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(parties, 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Posterior marginal means by respondent party\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_posterior_party2 &lt;- ggplot(\n  posterior_party_mms_diff,\n  aes(x = draw, y = term)\n) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(\n    color = \"Republican marginal mean − Democrat marginal mean\",\n    fill = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(\n    values = \"#85144b\",\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = \"#85144b\"),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(\"#85144b\", 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means by respondent party\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_posterior_party1 | mm_posterior_party2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "",
    "text": "I recently posted a guide (mostly for future-me) about how to analyze conjoint survey data with R. I explore two different estimands that social scientists are interested in—causal average marginal component effects (AMCEs) and descriptive marginal means—and show how to find them with R, with both frequentist and Bayesian approaches.\nHowever, that post is a little wrong. It’s not wrong wrong, but it is a bit oversimplified.\nWhen political scientists, psychologists, economists, and other social scientists analyze conjoint data, they overwhelmingly do it with ordinary least squares (OLS) regression, or just standard linear regression (lm(y ~ x) in R; reg y x in Stata). Even if the outcome is binary, they’ll use OLS and call it a linear probability model. The main R package for working with conjoint data in a frequentist way ({cregg}) uses OLS and linear probability models. Social scientists (and economists in particular) adore OLS.\nIn my earlier guide, I showed how to analyze the data with logistic regression, but even that is still overly simplified. In reality, conjoint choice-based experiments are more complex than what regular old OLS regression—or even logistic regression—can handle (though I’m sure some econometrician somewhere has a proof showing that OLS works just fine for multinomial conjoint data :shrug:).\nA recent paper published in Political Science Research and Methods (Jensen et al. 2021) does an excellent job explaining the problem with using plain old OLS to estimate AMCEs and marginal means with conjoint data (access the preprint here). Their main argument boils down to this: OLS throws away too much useful information about (1) the relationships and covariance between the different combinations of feature levels offered to respondents, and (2) individual-specific differences in how respondents react to different feature levels.\nJensen et al. explain three different approaches to analyzing data that has a natural hierarchical structure like conjoint data (where lots of choices related to different “products” are nested within individuals). This also is the same argument from chapter 15 of Bayes Rules!.\nBy using a multilevel hierarchical model, Jensen et al. (2021) show that we can still find AMCEs and causal effects, just like in my previous guide, but we can take advantage of the far richer heterogeneity that we get from these complex statements. We can make cool statements like this (in an experiment that varied policies related to unions):\nUsing hierarchical models for conjoint experiments in political science is new and exciting and revolutionary and neat. That’s the whole point of Jensen et al.’s paper—it’s a call to stop using OLS for everything.\nI’ve been working on a conjoint experiment with my coauthors Marc Dotson and Suparna Chaudhry. Suparna and I are political scientists and this multilevel stuff in general is still relatively new and wildly underused in the discipline. Marc, though, is a marketing scholar. The marketing world has been using hierarchical models for conjoint experiments for a long time and it’s standard practice in that discipline. There’s a whole textbook about the hierarchical model approach in marketing (Chapman and Feit 2019), and these fancy conjoint multilevel models are used widely throughout the marketing industry.\nlol at political science, just now discovering this.\nSo, I need to expand my previous conjoint guide. That’s what this post is for.\nI’ll do three things in this guide:\nThroughout this example, I’ll use data from two different simulated conjoint choice experiments. You can download these files and follow along:\nAdditionally, in Part 3, I fit a huge Stan model with {brms} that takes ≈30 minutes to run on my fast laptop. If you want to follow along and not melt your CPU for half an hour, you can download an .rds file of that fitted model that I stuck in an OSF project. The code for brm() later in this guide will load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\nLet’s load some libraries, create some helper functions, load the data, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(broom)            # Convert model objects to tidy data frames\nlibrary(parameters)       # Show model results as nice tables\nlibrary(survey)           # Panel-ish frequentist regression models\nlibrary(mlogit)           # Frequentist multinomial regression models\nlibrary(dfidx)            # Structure data for {mlogit} models\nlibrary(scales)           # Nicer labeling functions\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(ggforce)          # For facet_col()\nlibrary(brms)             # The best formula-based interface to Stan\nlibrary(tidybayes)        # Manipulate Stan results in tidy ways\nlibrary(ggdist)           # Fancy distribution plots\nlibrary(patchwork)        # Combine ggplot plots\nlibrary(rcartocolor)      # Color palettes from CARTOColors (https://carto.com/carto-colors/)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://github.com/intel/clear-sans\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Clear Sans\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Clear Sans\", face = \"bold\"),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 1),\n          strip.text = element_text(family = \"Clear Sans\", face = \"bold\",\n                                    size = rel(0.75), hjust = 0),\n          strip.background = element_rect(fill = \"grey90\", color = NA))\n}\n\ntheme_set(theme_nice())\n\nclrs &lt;- carto_pal(name = \"Prism\")\n\n# Functions for formatting things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\nchocolate &lt;- read_csv(\"data/choco_candy.csv\") %&gt;% \n  mutate(\n    dark = case_match(dark, 0 ~ \"Milk\", 1 ~ \"Dark\"),\n    dark = factor(dark, levels = c(\"Milk\", \"Dark\")),\n    soft = case_match(soft, 0 ~ \"Chewy\", 1 ~ \"Soft\"),\n    soft = factor(soft, levels = c(\"Chewy\", \"Soft\")),\n    nuts = case_match(nuts, 0 ~ \"No nuts\", 1 ~ \"Nuts\"),\n    nuts = factor(nuts, levels = c(\"No nuts\", \"Nuts\"))\n  )\n\nminivans &lt;- read_csv(\"data/rintro-chapter13conjoint.csv\") %&gt;% \n  mutate(\n    across(c(seat, cargo, price), factor),\n    carpool = factor(carpool, levels = c(\"no\", \"yes\")),\n    eng = factor(eng, levels = c(\"gas\", \"hyb\", \"elec\"))\n  )"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these kinds of candies they’d want to buy. Respondents only see this question one time and all possible options are presented simultaneously.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\n\n\n\n\nChocolate\nMilk\nMilk\nMilk\nMilk\nDark\nDark\nDark\nDark\n\n\nCenter\nChewy\nChewy\nSoft\nSoft\nChewy\nChewy\nSoft\nSoft\n\n\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment looks like this, with one row for each possible alternative (so eight rows per person, or subj), with the alternative that was selected marked as 1 in choice. Here, Subject 1 chose option E (dark, chewy, no nuts). There were 10 respondents, with 8 rows each, so there are 10 × 8 = 80 rows.\n\nchocolate\n## # A tibble: 80 × 6\n##     subj choice alt   dark  soft  nuts   \n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  \n##  1     1      0 A     Milk  Chewy No nuts\n##  2     1      0 B     Milk  Chewy Nuts   \n##  3     1      0 C     Milk  Soft  No nuts\n##  4     1      0 D     Milk  Soft  Nuts   \n##  5     1      1 E     Dark  Chewy No nuts\n##  6     1      0 F     Dark  Chewy Nuts   \n##  7     1      0 G     Dark  Soft  No nuts\n##  8     1      0 H     Dark  Soft  Nuts   \n##  9     2      0 A     Milk  Chewy No nuts\n## 10     2      0 B     Milk  Chewy Nuts   \n## # ℹ 70 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown eight different options and asked to select one. While this seems like a binary yes/no choice that could work with just regular plain old logistic regression, we want to account for the features and levels in all the unchosen categories too. To do this, we can use multinomial logistic regression, where the outcome variable is an unordered categorical variable with more than two categories. In this case we have eight different possible outcomes: alternatives A through H.\n\nOriginal SAS model as a baseline\n\n\n\n\n\n\nlol SAS\n\n\n\nI know nothing about SAS. I have never opened SAS in my life. It is a mystery to me.\nI copied these results directly from p. 297 in SAS’s massive “Discrete Choice” technical note (Kuhfeld 2010).\nI only have this SAS output here as a baseline reference for what the actual correct coefficients are supposed to be.\n\n\nSAS apparently fits these models with proportional hazard survival-style models, which feels weird, but there’s probably a mathematical or statistical reason for it. You use PROC PHREG to do it:\nproc phreg data=chocs outest=betas;\n   strata subj set;\n   model c*c(2) = dark soft nuts / ties=breslow;\n   run;\nIt gives these results:\n                   Choice of Chocolate Candies\n\n                       The PHREG Procedure\n\n              Multinomial Logit Parameter Estimates\n              \n                      Parameter     Standard\n                 DF    Estimate        Error  Chi-Square   Pr &gt; ChiSq\nDark Chocolate   1      1.38629      0.79057      3.0749       0.0795\nSoft Center      1     -2.19722      1.05409      4.3450       0.0371\nWith Nuts        1      0.84730      0.69007      1.5076       0.2195\n\n\nSurvival model\nEw, enough SAS. Let’s do this with R instead.\nWe can recreate the same proportional hazards model with coxph() from the {survival} package. Again, this feels weird and not like an intended purpose of survival models and not like multinomial logit at all—in my mind it is neither (1) multinomial nor (2) logit, but whatever. People far smarter than me invented these things, so I’ll just trust them.\n\nmodel_chocolate_survival &lt;- coxph(\n  Surv(subj, choice) ~ dark + soft + nuts, \n  data = chocolate, \n  ties = \"breslow\"  # This is what SAS uses\n)\n\nmodel_parameters(model_chocolate_survival, digits = 4, p_digits = 4)\n## Parameter   | Coefficient |     SE |         95% CI |       z |      p\n## ----------------------------------------------------------------------\n## dark [Dark] |      1.3863 | 0.7906 | [-0.16,  2.94] |  1.7535 | 0.0795\n## soft [Soft] |     -2.1972 | 1.0541 | [-4.26, -0.13] | -2.0845 | 0.0371\n## nuts [Nuts] |      0.8473 | 0.6901 | [-0.51,  2.20] |  1.2279 | 0.2195\n\nThe coefficients, standard errors, and p-values are identical to the SAS output! The only difference is the statistic: in SAS they use a chi-square statistic, while survival:coxph() uses a z statistic. There’s probably a way to make coxph() use a chi-square statistic, but I don’t care about that. I never use survival models and I’m only doing this to replicate the SAS output and it just doesn’t matter.\n\n\nPoisson model\nAn alternative way to fit a multinomial logit model without resorting to survival models is to actually (mis?)use another model family. We can use a Poisson model, even though choice isn’t technically count data, because of obscure stats reasons. See here for an illustration of the relationship between multinomial and Poisson distributions; or see this 2011 Biometrika paper about using Poisson models to reduce bias in multinomial logit models. Richard McElreath has a subsection about this in Statistical Rethinking as well: “Multinomial in disguise as Poisson” (11.3.3). Or as he said over on the currently-walled-garden Bluesky, “All count distributions are just one or more Poisson distributions in a trench coat.”\nTo account for the repeated subjects in the data, we’ll use svyglm() from the {survey} package so that the standard errors are more accurate.\n\nmodel_chocolate_poisson &lt;- glm(\n  choice ~ dark + soft + nuts, \n  data = chocolate, \n  family = poisson()\n)\n\nmodel_parameters(model_chocolate_poisson, digits = 4, p_digits = 4)\n## Parameter   | Log-Mean |     SE |         95% CI |       z |      p\n## -------------------------------------------------------------------\n## (Intercept) |  -2.9188 | 0.8628 | [-4.97, -1.49] | -3.3829 | 0.0007\n## dark [Dark] |   1.3863 | 0.7906 | [ 0.00,  3.28] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-5.11, -0.53] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.43,  2.38] |  1.2279 | 0.2195\n\nLovely—the results are the same.\n\n\nmlogit model\nFinally, we can use the {mlogit} package to fit the model. Before using mlogit(), we need to transform our data a bit to specify which column represents the choice (choice) and how the data is indexed: subjects (subj) with repeated alternatives (alt).\n\nchocolate_idx &lt;- dfidx(\n  chocolate,\n  idx = list(\"subj\", \"alt\"),\n  choice = \"choice\",\n  shape = \"long\"\n)\n\nWe can then use this indexed data frame with mlogit(), which uses the familiar R formula interface, but with some extra features separated by |s\noutcome ~ features | individual-level variables | alternative-level variables\nIf we had columns related to individual-level characteristics or alternative-level characteristics, we could include those in the model—and we’ll do precisely that later in this post. (Incorporating individual-level covariates is the whole point of this post!)\nLet’s fit the model:\n\nmodel_chocolate_mlogit &lt;- mlogit(\n  choice ~ dark + soft + nuts | 0 | 0, \n  data = chocolate_idx\n)\n\nmodel_parameters(model_chocolate_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |         95% CI |       z |      p\n## -------------------------------------------------------------------\n## dark [Dark] |   1.3863 | 0.7906 | [-0.16,  2.94] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-4.26, -0.13] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.51,  2.20] |  1.2279 | 0.2195\n\nDelightful. All the results are the same as the survival model and the Poisson model.\n\n\nBayesian model\nWe can also fit this model in a Bayesian way using {brms}. Stan has a categorical distribution family for multinomial models, and we’ll use it in the next example. For now, for the sake of simplicity, we’ll use a Poisson family, since, as we saw above, that’s a legal way of parameterizing multinomial distributions.\nThe data has a natural hierarchical structure to it, with 8 choices (for alternatives A through H) nested inside each of the 10 subjects.\n\n\n\n\n\nMultilevel experimental structure, with candy choices \\(y_{\\text{A}\\dots\\text{H}}\\) nested in subjects\n\n\n\n\nWe want to model candy choice (choice) based on candy characteristics (dark, soft, and nuts). We’ll use the subscript \\(i\\) to refer to individual candy choices and \\(j\\) to refer to subjects.\nSince we can legally pretend that this multinomial selection process is actually Poisson, we’ll model it as a Poisson process that has a rate of \\(\\lambda_{i_j}\\). We’ll model that \\(\\lambda_{i_j}\\) with a log-linked regression model with covariates for each of the levels of each candy feature. To account for the multilevel structure, we’ll include subject-specific offsets (\\(b_{0_j}\\)) from the global average, thus creating random intercepts. We’ll specify fairly wide priors just because.\nHere’s the formal model for all this:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Probability of selection of alternative}_i \\textbf{ in subject}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Poisson}(\\lambda_{i_j}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\log(\\lambda_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Dark}_{i_j} + \\beta_2 \\text{Soft}_{i_j} + \\beta_3 \\text{Nuts}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\qquad\\quad \\text{Subject-specific offsets from global choice probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for global average choice probability} \\\\\n\\beta_1, \\beta_2, \\beta_3 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for candy feature levels} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\qquad \\text{Prior for between-subject variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model:\n\nmodel_chocolate_brms &lt;- brm(\n  bf(choice ~ dark + soft + nuts + (1 | subj)),\n  data = chocolate,\n  family = poisson(),\n  prior = c(\n    prior(normal(0, 3), class = Intercept),\n    prior(normal(0, 3), class = b),\n    prior(exponential(1), class = sd)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_chocolate_brms\"\n)\n\nThe results are roughly the same as what we found with all the other models—they’re slightly off because of random MCMC sampling.\n\nmodel_parameters(model_chocolate_brms)\n## # Fixed Effects\n## \n## Parameter   | Median |         95% CI |     pd |  Rhat |     ESS\n## ----------------------------------------------------------------\n## (Intercept) |  -3.04 | [-5.07, -1.60] |   100% | 1.000 | 3598.00\n## darkDark    |   1.35 | [-0.05,  3.16] | 96.92% | 1.000 | 4238.00\n## softSoft    |  -2.03 | [-4.35, -0.47] | 99.60% | 1.000 | 2867.00\n## nutsNuts    |   0.83 | [-0.40,  2.27] | 90.28% | 1.000 | 4648.00"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the SAS technical note example, they use the model to generated predicted probabilities of the choice of each of the options. In the world of marketing, this can also be seen as the predicted market share for each option. To do this, they plug each of the eight different different combinations of dark, soft, and nuts into the model and calculate the predicted output on the response (i.e. probability) scale. They get these results, where dark, chewy, and nuts is the most likely and popular option (commanding a 50% market share).\n      Choice of Chocolate Candies\n\nObs    Dark    Soft     Nuts       p\n\n  1    Dark    Chewy    Nuts       0.50400\n  2    Dark    Chewy    No Nuts    0.21600\n  3    Milk    Chewy    Nuts       0.12600\n  4    Dark    Soft     Nuts       0.05600\n  5    Milk    Chewy    No Nuts    0.05400\n  6    Dark    Soft     No Nuts    0.02400\n  7    Milk    Soft     Nuts       0.01400\n  8    Milk    Soft     No Nuts    0.00600\nWe can do the same thing with R.\n\nFrequentist predictions\n{mlogit} model objects have predicted values stored in one of their slots (model_chocolate_mlogit$probabilities), but they’re in a weird non-tidy matrix form and I like working with tidy data. I’m also a huge fan of the {marginaleffects} package, which provides a consistent way to calculate predictions, comparisons, and slopes/marginal effects (with predictions(), comparisons(), and slopes()) for dozens of kinds of models, including mlogit() models.\nSo instead of wrangling the built-in mlogit() probabilities, we’ll generate predictions by feeding the model the unique combinations of dark, soft, and nuts to marginaleffects::predictions(), which will provide us with probability- or proportion-scale predictions:\n\npreds_chocolate_mlogit &lt;- predictions(\n  model_chocolate_mlogit, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n)\n\npreds_chocolate_mlogit %&gt;% \n  # predictions() hides a bunch of columns; forcing it to be a tibble unhides them\n  as_tibble() %&gt;% \n  arrange(desc(estimate)) %&gt;% \n  select(group, dark, soft, nuts, estimate, std.error, statistic, p.value)\n## # A tibble: 8 × 8\n##   group dark  soft  nuts    estimate std.error statistic  p.value\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 F     Dark  Chewy Nuts     0.504     0.142       3.56  0.000373\n## 2 E     Dark  Chewy No nuts  0.216     0.112       1.93  0.0540  \n## 3 B     Milk  Chewy Nuts     0.126     0.0849      1.48  0.138   \n## 4 H     Dark  Soft  Nuts     0.0560    0.0551      1.02  0.309   \n## 5 A     Milk  Chewy No nuts  0.0540    0.0433      1.25  0.213   \n## 6 G     Dark  Soft  No nuts  0.0240    0.0258      0.929 0.353   \n## 7 D     Milk  Soft  Nuts     0.0140    0.0162      0.863 0.388   \n## 8 C     Milk  Soft  No nuts  0.00600   0.00743     0.808 0.419\n\nPerfect! They’re identical to the SAS output.\nWe can play around with these predictions to describe the overall market for candy. Chewy candies dominate the market…\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 2 × 2\n##   dark  share\n##   &lt;fct&gt; &lt;dbl&gt;\n## 1 Milk  0.200\n## 2 Dark  0.800\n\n…and dark chewy candies are by far the most popular:\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark, soft) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 4 × 3\n## # Groups:   dark [2]\n##   dark  soft   share\n##   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n## 1 Milk  Chewy 0.180 \n## 2 Milk  Soft  0.0200\n## 3 Dark  Chewy 0.720 \n## 4 Dark  Soft  0.0800\n\n\n\nBayesian predictions\n{marginaleffects} supports {brms} models too, so we can basically run the same predictions() function to generate predictions for our Bayesian model. Magical.\n\n\n\n\n\n\nlol subject offsets\n\n\n\nWhen plugging values into predictions() (or avg_slopes() or any function that calculates predictions from a model), we have to decide how to handle the random subject offsets (\\(b_{0_j}\\)). I have a whole other blog post guide about this and how absolutely maddening the nomenclature for all this is.\nBy default, predictions() and friends will calculate predictions for subjects on average by using the re_formula = NULL argument. This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average subject. When working with slopes, this is also called a marginal effect.\nWe could also use re_formula = NA to calculate predictions for a typical subject, or a subject where the random offset is set to 0. When working with slopes, this is also called a conditional effect.\n\nConditional predictions/effect = average subject = re_formula = NA\nMarginal predictions/effect = subjects on average = re_formula = NULL (default), using existing subject levels or a new simulated subject\n\nAgain, see this guide for way more about these distinctions. In this example here, we’ll just use the default marginal predictions/effects (re_formula = NULL), or the effect for subjects on average.\n\n\nThe predicted proportions aren’t identical to the SAS output, but they’re close enough, given that it’s a completely different modeling approach.\n\npreds_chocolate_brms &lt;- predictions(\n  model_chocolate_brms, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n) \n\npreds_chocolate_brms %&gt;% \n  as_tibble() %&gt;%\n  arrange(desc(estimate)) %&gt;% \n  select(dark, soft, nuts, estimate, conf.low, conf.high)\n## # A tibble: 8 × 6\n##   dark  soft  nuts    estimate conf.low conf.high\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Dark  Chewy Nuts     0.432   0.144       1.09  \n## 2 Dark  Chewy No nuts  0.186   0.0424      0.571 \n## 3 Milk  Chewy Nuts     0.110   0.0168      0.419 \n## 4 Dark  Soft  Nuts     0.0553  0.00519     0.279 \n## 5 Milk  Chewy No nuts  0.0465  0.00552     0.219 \n## 6 Dark  Soft  No nuts  0.0230  0.00182     0.141 \n## 7 Milk  Soft  Nuts     0.0136  0.00104     0.0881\n## 8 Milk  Soft  No nuts  0.00556 0.000326    0.0414\n\n\n\nPlots\nSince predictions() returns a tidy data frame, we can plot these predicted probabilities (or market shares or however we want to think about them) with {ggplot2}:\n\np1 &lt;- preds_chocolate_mlogit %&gt;% \n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = estimate, y = label)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high), color = clrs[7]) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Frequentist {mlogit} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())\n\np2 &lt;- preds_chocolate_brms %&gt;% \n  posterior_draws() %&gt;%  # Extract the posterior draws of the predictions\n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = draw, y = label)) +\n  stat_halfeye(normalize = \"xy\", fill = clrs[7])  +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Bayesian {brms} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +\n  # Make the x-axis match the mlogit plot\n  coord_cartesian(xlim = c(-0.05, 0.78))\n\np1 / p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nThe marketing world doesn’t typically look at coefficients or marginal effects, but the political science world definitely does. In political science, the estimand we often care about the most is the average marginal component effect (AMCE), or the causal effect of moving one feature level to a different value, holding all other features constant. I have a whole in-depth blog post about AMCEs and how to calculate them—go look at that for more details. Long story short—AMCEs are basically the coefficients in a regression model.\nInterpreting the coefficients is difficult with models that aren’t basic linear regression. Here, all these coefficients are on the log scale, so they’re not directly interpretable. The original SAS technical note also doesn’t really interpret any of these , they don’t really interpret these things anyway, since they’re more focused on predictions. All they say is this:\n\nThe parameter estimate with the smallest p-value is for soft center. Since the parameter estimate is negative, chewy is the more preferred level. Dark is preferred over milk, and nuts over no nuts, however only the p-value for Soft is less than 0.05.\n\nWe could exponentiate the coefficients to make them multiplicative (akin to odds ratios in logistic regression). For center = soft, \\(e^{-2.19722}\\) = 0.1111, which means that candies with a soft center are 89% less likely to be chosen than candies with a chewy center, relative to the average candy. But that’s weird to think about.\nSo instead we can turn to {marginaleffects} once again to calculate percentage-point scale estimands that we can interpret far more easily.\n\n\n\n\n\n\nlol marginal effects\n\n\n\nNobody is ever consistent about the word “marginal effect.” Some people use it to refer to averages; some people use it to refer to slopes. These are complete opposites. In calculus, averages = integrals and slopes = derivatives and they’re the inverse of each other.\nI like to think of marginal effects as what happens to the outcome when you move an explanatory variable a tiny bit. With continuous variables, that’s a slope; with categorical variables, that’s an offset in average outcomes. These correspond directly to how you normally interpret regression coefficients. Or returning to my favorite analogy about regression, with numeric variables we care what happens to the outcome when we slide the value up a tiny bit; with categorical variables we care about what happens to the outcome when we switch on a category.\nAdditionally, there are like a billion different ways to calculate marginal effects: average marginal effects (AMEs), group-average marginal effects (G-AMEs), marginal effects at user-specified values, marginal effects at the mean (MEM), and counterfactual marginal effects. See the documentation for {marginaleffects} + this mega blog post for more about these subtle differences.\n\n\n\nBayesian comparisons/contrasts\nWe can use avg_comparisons() to calculate the difference (or average marginal effect) for each of the categorical coefficients on the percentage-point scale, showing the effect of moving from milk → dark, chewy → soft, and nuts → no nuts.\n(Technically we can also use avg_slopes(), even though none of these coefficients are actually slopes. {marginaleffects} is smart enough to show contrasts for categorical variables and partial derivatives/slopes for continuous variables.)\n\navg_comparisons(model_chocolate_brms)\n## \n##  Term       Contrast Estimate    2.5 %  97.5 %\n##  dark Dark - Milk       0.139 -0.00551  0.3211\n##  nuts Nuts - No nuts    0.092 -0.05221  0.2599\n##  soft Soft - Chewy     -0.182 -0.35800 -0.0523\n## \n## Columns: term, contrast, estimate, conf.low, conf.high\n\nWhen holding all other features constant, moving from chewy → soft is associated with a posterior median 18 percentage point decrease in the probability of selection (or drop in market share if you want to think of it that way), on average.\n\n\nFrequentist comparisons/contrasts\nWe went out of order in this section and showed how to use avg_comparisons() with the Bayesian model first instead of the frequentist model. That’s because it was easy. mlogit() models behave strangely with {marginaleffects} because {mlogit} forces its predictions to use every possible value of the alternatives A–H. Accordingly, the estimate for any coefficients in the attributes section of the {mlogit} formula (dark, soft, and nuts here) will automatically be zero. Note how here there are 24 rows of comparisons instead of 3, since we get comparisons in each of the 8 groups, and note how the estimates are all zero:\n\navg_comparisons(model_chocolate_mlogit)\n## \n##  Group Term       Contrast  Estimate Std. Error         z Pr(&gt;|z|)     2.5 %   97.5 %\n##      A dark Dark - Milk    -2.78e-17   7.27e-13 -3.82e-05        1 -1.42e-12 1.42e-12\n##      B dark Dark - Milk     0.00e+00         NA        NA       NA        NA       NA\n##      C dark Dark - Milk     0.00e+00   1.62e-14  0.00e+00        1 -3.17e-14 3.17e-14\n##      D dark Dark - Milk    -6.94e-18   1.73e-13 -4.02e-05        1 -3.38e-13 3.38e-13\n##      E dark Dark - Milk    -2.78e-17   7.27e-13 -3.82e-05        1 -1.42e-12 1.42e-12\n##      F dark Dark - Milk     0.00e+00         NA        NA       NA        NA       NA\n##      G dark Dark - Milk     0.00e+00   1.62e-14  0.00e+00        1 -3.17e-14 3.17e-14\n##      H dark Dark - Milk    -6.94e-18   1.73e-13 -4.02e-05        1 -3.38e-13 3.38e-13\n##      A nuts Nuts - No nuts  1.39e-17         NA        NA       NA        NA       NA\n##      B nuts Nuts - No nuts  1.39e-17         NA        NA       NA        NA       NA\n##      C nuts Nuts - No nuts  0.00e+00   1.41e-14  0.00e+00        1 -2.77e-14 2.77e-14\n##      D nuts Nuts - No nuts  0.00e+00   1.41e-14  0.00e+00        1 -2.77e-14 2.77e-14\n##      E nuts Nuts - No nuts  0.00e+00   9.74e-13  0.00e+00        1 -1.91e-12 1.91e-12\n##      F nuts Nuts - No nuts  0.00e+00   9.74e-13  0.00e+00        1 -1.91e-12 1.91e-12\n##      G nuts Nuts - No nuts  0.00e+00   1.08e-13  0.00e+00        1 -2.11e-13 2.11e-13\n##      H nuts Nuts - No nuts  0.00e+00   1.08e-13  0.00e+00        1 -2.11e-13 2.11e-13\n##      A soft Soft - Chewy    6.94e-18   1.08e-13  6.42e-05        1 -2.12e-13 2.12e-13\n##      B soft Soft - Chewy    1.39e-17   2.16e-13  6.43e-05        1 -4.23e-13 4.23e-13\n##      C soft Soft - Chewy    6.94e-18   1.08e-13  6.42e-05        1 -2.12e-13 2.12e-13\n##      D soft Soft - Chewy    1.39e-17   2.16e-13  6.43e-05        1 -4.23e-13 4.23e-13\n##      E soft Soft - Chewy    1.39e-17   4.33e-13  3.21e-05        1 -8.48e-13 8.48e-13\n##      F soft Soft - Chewy    0.00e+00   4.52e-13  0.00e+00        1 -8.86e-13 8.86e-13\n##      G soft Soft - Chewy    1.39e-17   4.33e-13  3.21e-05        1 -8.48e-13 8.48e-13\n##      H soft Soft - Chewy    0.00e+00   4.52e-13  0.00e+00        1 -8.86e-13 8.86e-13\n## \n## Columns: group, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nIf we had continuous variables, we could work around this by specifying our own tiny amount of marginal change to compare across, but we’re working with categories and can’t do that. Instead, with categorical variables, we can return to predictions() and define custom aggregations of different features and levels.\nBefore making custom aggregations, though, it’ll be helpful to illustrate what exactly we’re looking at when collapsing these results. Remember that earlier we calculated predictions for all the unique combinations of dark, soft, and nuts:\n\npreds_chocolate_mlogit &lt;- predictions(\n  model_chocolate_mlogit, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n) \n\npreds_chocolate_mlogit %&gt;% \n  as_tibble() %&gt;% \n  select(group, dark, soft, nuts, estimate)\n## # A tibble: 8 × 5\n##   group dark  soft  nuts    estimate\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;\n## 1 A     Milk  Chewy No nuts  0.0540 \n## 2 B     Milk  Chewy Nuts     0.126  \n## 3 C     Milk  Soft  No nuts  0.00600\n## 4 D     Milk  Soft  Nuts     0.0140 \n## 5 E     Dark  Chewy No nuts  0.216  \n## 6 F     Dark  Chewy Nuts     0.504  \n## 7 G     Dark  Soft  No nuts  0.0240 \n## 8 H     Dark  Soft  Nuts     0.0560\n\nFour of the groups have dark = Milk and four have dark = Dark, with other varying characteristics across those groups (chewy/soft, nuts/no nuts). If we want the average proportion of all milk and dark chocolate options, we can group and summarize:\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(avg_pred = mean(estimate))\n## # A tibble: 2 × 2\n##   dark  avg_pred\n##   &lt;fct&gt;    &lt;dbl&gt;\n## 1 Milk    0.0500\n## 2 Dark    0.200\n\nThe average market share for milk chocolate candies, holding all other features constant, is 5% (\\(\\frac{0.0540 + 0.126 + 0.006 + 0.014}{2} = 0.05\\)); the average market share for dark chocolate candies is 20% (\\(\\frac{0.216 + 0.504 + 0.024 + 0.056}{2} = 0.2\\)). These values are the averages of the predictions from the four groups where dark is either Milk or Dark.\nInstead of calculating these averages manually (which would also force us to calculate standard errors and p-values manually, which, ugh), we can calculate these aggregate group means with predictions(). To do this, we can feed a little data frame to predictions() with the by argument. The data frame needs to contain columns for the features we want to collapse, and a by column with the labels we want to include in the output. For example, if we want to collapse the eight possible choices into those with milk chocolate and those with dark chocolate, we could create a by data frame like this:\n\nby &lt;- data.frame(dark = c(\"Milk\", \"Dark\"), by = c(\"Milk\", \"Dark\"))\nby\n##   dark   by\n## 1 Milk Milk\n## 2 Dark Dark\n\nIf we use that by data frame in predictions(), we get the same 5% and 20% from before, but now with all of {marginaleffects}’s extra features like standard errors and confidence intervals:\n\npredictions(\n  model_chocolate_mlogit,\n  by = by\n)\n## \n##  Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %   By\n##      0.20     0.0316 6.32   &lt;0.001  0.138  0.262 Dark\n##      0.05     0.0316 1.58    0.114 -0.012  0.112 Milk\n## \n## Columns: estimate, std.error, statistic, p.value, conf.low, conf.high, by\n\nEven better, we can use the hypothesis functionality of predictions() to conduct a hypothesis test and calculate the difference (or contrast) between these two averages, which is exactly what we’re looking for with categorical AMCEs. This shows the average causal effect of moving from milk → dark—holding all other features constant, switching the chocolate type from milk to dark causes a 15 percentage point increase in the probability of selecting the candy, on average.\n\npredictions(\n  model_chocolate_mlogit,\n  by = by,\n  hypothesis = \"revpairwise\"\n)\n## \n##         Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 % 97.5 %\n##  Milk - Dark    -0.15     0.0632 -2.37   0.0177 -0.274 -0.026\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can’t simultaneously specify all the contrasts we’re interested in single by argument, but we can do them separately and combine them into a single data frame:\n\namces_chocolate_mlogit &lt;- bind_rows(\n  dark = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(dark = c(\"Milk\", \"Dark\"), by = c(\"Milk\", \"Dark\")),\n    hypothesis = \"revpairwise\"\n  ),\n  soft = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(soft = c(\"Chewy\", \"Soft\"), by = c(\"Chewy\", \"Soft\")),\n    hypothesis = \"revpairwise\"\n  ),\n  nuts = predictions(\n    model_chocolate_mlogit,\n    by = data.frame(nuts = c(\"No nuts\", \"Nuts\"), by = c(\"No nuts\", \"Nuts\")),\n    hypothesis = \"revpairwise\"\n  ),\n  .id = \"variable\"\n)\namces_chocolate_mlogit\n## \n##            Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 % 97.5 %\n##  Milk - Dark       -0.15     0.0632 -2.37   0.0177 -0.274 -0.026\n##  Soft - Chewy      -0.20     0.0474 -4.22   &lt;0.001 -0.293 -0.107\n##  Nuts - No nuts     0.10     0.0725  1.38   0.1675 -0.042  0.242\n## \n## Columns: variable, term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n\n\nPlots\nPlotting these AMCEs requires a bit of data wrangling, but we get really neat plots, so it’s worth it. I’ve hidden all the code here for the sake of space.\n\n\nExtract variable labels\nchocolate_var_levels &lt;- tibble(\n  variable = c(\"dark\", \"soft\", \"nuts\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- chocolate[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nchocolate_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"dark\",    \"Type of chocolate\",\n  \"soft\",    \"Type of center\",\n  \"nuts\",    \"Nuts\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plot\namces_chocolate_mlogit_split &lt;- amces_chocolate_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    amces_chocolate_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(1, 3, 8)]) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\nCombine full dataset of factor levels with posterior draws and make {brms} plot\n# This is much easier than the mlogit mess because we can use avg_comparisons() directly\nposterior_mfx &lt;- model_chocolate_brms %&gt;% \n  avg_comparisons() %&gt;% \n  posteriordraws() \n\nposterior_mfx_nested &lt;- posterior_mfx %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\n# Combine full dataset of factor levels with model results\nplot_data_bayes &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    posterior_mfx_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  guides(fill = \"none\") +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(1, 3, 8)]) +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 | p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these minivans they’d want to buy, based on four different features/attributes with different levels:\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nPassengers\n6, 7, 8\n\n\nCargo area\n2 feet, 3 feet\n\n\nEngine\nGas, electric, hybrid\n\n\nPrice\n$30,000; $35,000; $40,000\n\n\n\nRespondents see this a question similar to this fifteen different times, with three options with randomly shuffled levels for each of the features.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption 1\nOption 2\nOption 3\n\n\n\n\nPassengers\n7\n8\n6\n\n\nCargo area\n3 feet\n3 feet\n2 feet\n\n\nEngine\nElectric\nGas\nHybrid\n\n\nPrice\n$40,000\n$40,000\n$30,000\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment has one row for each possible alternative (alt) within each set of 15 questions (ques), thus creating 3 × 15 = 45 rows per respondent (resp.id). There were 200 respondents, with 45 rows each, so there are 200 × 45 = 9,000 rows. Here, Respondent 1 chose a $30,000 gas van with 6 seats and 3 feet of cargo space in the first set of three options, a $35,000 gas van with 7 seats and 3 feet of cargo space in the second set of three options, and so on.\nThere’s also a column here for carpool indicating if the respondent carpools with others when commuting. It’s an individual respondent-level characteristic and is constant throughout all the questions and alternatives, and we’ll use it later.\n\nminivans\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt carpool seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 yes     6     2ft   gas   35         0\n##  2       1     1     2 yes     8     3ft   hyb   30         0\n##  3       1     1     3 yes     6     3ft   gas   30         1\n##  4       1     2     1 yes     6     2ft   gas   30         0\n##  5       1     2     2 yes     7     3ft   gas   35         1\n##  6       1     2     3 yes     6     2ft   elec  35         0\n##  7       1     3     1 yes     8     3ft   gas   35         1\n##  8       1     3     2 yes     7     3ft   elec  30         0\n##  9       1     3     3 yes     8     2ft   elec  40         0\n## 10       1     4     1 yes     7     3ft   elec  40         1\n## # ℹ 8,990 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown three different options and asked to select one. We thus have three possible outcomes: a respondent could have selected option 1, option 2, or option 3. Because everything was randomized, there shouldn’t be any patterns in which options people choose—we don’t want to see that the first column is more common, since that would indicate that respondents are just repeatedly selecting the first column to get through the survey. Since there are three possible outcomes (option 1, 2, and 3), we’ll use multinomial logistic regression.\n\nOriginal model as a baseline\nIn the example in their textbook, Chapman and Feit (2019) use {mlogit} to estimate this model and they find these results. This will be our baseline throughout this example.\n\n\n\nOriginal results from Chapman and Feit (2019) p. 371\n\n\n\n\nmlogit model\nThis data is a little more complex now, since there are alternatives nested inside questions inside respondents. To account for this panel structure when using {mlogit}, we need to define two index columns: one for the unique set of alternatives offered to the respondent and one for the respondent ID. We still do this with dfidx(), but need to create a new column with an ID number for each unique combination of respondent ID and question number:\n\nminivans_idx &lt;- minivans %&gt;% \n  # mlogit() needs a column with unique question id numbers\n  group_by(resp.id, ques) %&gt;% \n  mutate(choice.id = cur_group_id()) %&gt;% \n  ungroup() %&gt;% \n  # Make indexed data frame for mlogit\n  dfidx(\n    idx = list(c(\"choice.id\", \"resp.id\"), \"alt\"),\n    choice = \"choice\",\n    shape = \"long\"\n  )\n\nNow we can fit the model. Note the 0 ~ seat syntax here. That suppresses the intercept for the model, which behaves weirdly with multinomial models. Since there are three categories for the outcome (options 1, 2, and 3), there are two intercepts, representing cutpoints-from-ordered-logit-esque shifts in the probability of selecting option 1 vs. option 2 and option 2 vs. option 3. We don’t want to deal with those, so we’ll suppress them.\n\nmodel_minivans_mlogit &lt;- mlogit(\n  choice ~ 0 + seat + cargo + eng + price | 0 | 0, \n  data = minivans_idx\n)\nmodel_parameters(model_minivans_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |         95% CI |        z |           p\n## -------------------------------------------------------------------------\n## seat [7]    |  -0.5353 | 0.0624 | [-0.66, -0.41] |  -8.5837 | 9.1863e-18 \n## seat [8]    |  -0.3058 | 0.0611 | [-0.43, -0.19] |  -5.0032 | 5.6376e-07 \n## cargo [3ft] |   0.4774 | 0.0509 | [ 0.38,  0.58] |   9.3824 | 6.4514e-21 \n## eng [hyb]   |  -0.8113 | 0.0601 | [-0.93, -0.69] | -13.4921 | 1.7408e-41 \n## eng [elec]  |  -1.5308 | 0.0675 | [-1.66, -1.40] | -22.6926 | 5.3004e-114\n## price [35]  |  -0.9137 | 0.0606 | [-1.03, -0.79] | -15.0765 | 2.3123e-51 \n## price [40]  |  -1.7259 | 0.0696 | [-1.86, -1.59] | -24.7856 | 1.2829e-135\n\nThese are the same results from p. 371 in Chapman and Feit (2019), so it worked. Again, the marketing world doesn’t typically do much with these coefficients beyond looking at their direction and magnitude. For instance, in Chapman and Feit (2019) they say that the estimate for seat [7] here is negative, which means that a 7-seat option is less preferred than 6-seat option, and that the estimate for price [40] is more negative than the already-negative estimate for price [35], which means that (1) respondents don’t like the $35,000 option compared to the baseline $30,000 and that (2) respondents really don’t like the $40,000 option. We could theoretically exponentiate these things—like, seeing 7 seats makes it \\(e^{-0.5353}\\) = 0.5855 = 41% less likely to select the option compared to 6 seats—but again, that’s weird.\n\n\nBayesian model with {brms}\nWe can also fit this multinomial model in a Bayesian way using {brms}. Stan has a categorical family for dealing with mulitnomial/categorical outcomes. But first, we’ll look at the nested structure of this data and incorporate that into the model, since we won’t be using the weird {mlogit}-style indexed data frame. As with the chocolate experiment, the data has a natural hierarchy in it, with three questions nested inside 15 separate question sets, nested inside each of the 200 respondents.\n\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents\n\n\n\n\nCurrently, our main outcome variable choice is binary. If we run the model with choice as the outcome with a categorical family, the model will fit, but it will go slow and {brms} will complain about it and recommend switching to regular logistic regression. The categorical family in Stan requires 2+ outcomes and a reference category. Here we have three possible options (1, 2, and 3), and we can imagine a reference category of 0 for rows that weren’t selected.\nWe can create a new outcome column (choice_alt) that indicates which option each respondent selected: 0 if they didn’t choose the option and 1–3 if they chose the first, second, or third option. Because of how the data is recorded, this only requires multiplying alt and choice:\n\nminivans_choice_alt &lt;- minivans %&gt;% \n  mutate(choice_alt = factor(alt * choice))\n\nminivans_choice_alt %&gt;% \n  select(resp.id, ques, alt, seat, cargo, eng, price, choice, choice_alt)\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt seat  cargo eng   price choice choice_alt\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;     \n##  1       1     1     1 6     2ft   gas   35         0 0         \n##  2       1     1     2 8     3ft   hyb   30         0 0         \n##  3       1     1     3 6     3ft   gas   30         1 3         \n##  4       1     2     1 6     2ft   gas   30         0 0         \n##  5       1     2     2 7     3ft   gas   35         1 2         \n##  6       1     2     3 6     2ft   elec  35         0 0         \n##  7       1     3     1 8     3ft   gas   35         1 1         \n##  8       1     3     2 7     3ft   elec  30         0 0         \n##  9       1     3     3 8     2ft   elec  40         0 0         \n## 10       1     4     1 7     3ft   elec  40         1 1         \n## # ℹ 8,990 more rows\n\nWe can now use the new four-category choice_alt column as our outcome with the categorical() family.\nIf we realllly wanted, we could add random effects for question sets nested inside respondents, like (1 | resp.id / ques). We’d want to do that if there were set-specific things that could influences choices. Like maybe we want to account for the possibility that everyone’s just choosing the first option, so it behaves differently? Or maybe the 5th set of questions is set to an extra difficult level on a quiz or something? Or maybe we have so many sets that we think the later ones will be less accurate because of respondent fatigue? idk. In this case, question set-specific effects don’t matter at all. Each question set is equally randomized and no different from the others, so we won’t bother modeling that layer of the hierarchy.\nWe want to model the choice of option 1, 2, or 3 (choice_alt) based on minivan characteristics (seat, cargo, eng, price). With the categorical model, we actually get a set of parameters to estimate the probability of selecting each of the options, which Stan calls \\(\\mu\\), so we have a set of three probabilities: \\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\). We’ll use the subscript \\(i\\) to refer to individual minivan choices and \\(j\\) to refer to respondents. Here’s the fun formal model:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Seat[7]}_{i_j} + \\beta_2 \\text{Seat[8]}_{i_j} + \\beta_3 \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_4 \\text{Engine[hyb]}_{i_j} + \\beta_5 \\text{Engine[elec]}_{i_j} + \\beta_6 \\text{Price[35k]}_{i_j} + \\beta_7 \\text{Price[40k]}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\quad\\quad \\text{Respondent-specific offsets from global probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ \\ \\text{Prior for choice-level coefficients} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\quad \\text{Prior for between-respondent variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model. Notice the much-more-verbose prior section—because the categorical family in Stan estimates separate parameters for each of the categories (\\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\)), we have a mean and standard deviation for the probability of selecting each of those options. We need to specify each of these separately too instead of just doing something like prior(normal(0, 3), class = b). Also notice the refcat argument in categorical()—this makes it so that all the estimates are relative to not choosing an option (or when choice_alt is 0). And also notice the slightly different syntax for the random respondent intercepts: (1 | ID | resp.id). That new middle ID is special {brms} formula syntax that we can use when working with categorical or ordinal families, and it makes it so that the group-level effects for the different outcomes (here options 0, 1, 2, and 3) are correlated (see p. 4 of this {brms} vignette for more about this special syntax).\n\nmodel_minivans_categorical_brms &lt;- brm(\n  bf(choice_alt ~ 0 + seat + cargo + eng + price + (1 | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_minivans_categorical_brms\"\n)\n\nThis model gives us a ton of parameters! We get three estimates per feature level (i.e. mu1_cargo3ft, mu2_cargo3ft, and mu3_cargo3ft for the cargo3ft effect), since we’re actually estimating the effect of each covariate on the probability of selecting each of the three options.\n\nmodel_parameters(model_minivans_categorical_brms)\n## Parameter    | Median |         95% CI |     pd |  Rhat |     ESS\n## -----------------------------------------------------------------\n## mu1_seat6    |  -0.34 | [-0.52, -0.16] | 99.95% | 1.001 | 2673.00\n## mu1_seat7    |  -0.86 | [-1.04, -0.67] |   100% | 1.000 | 3167.00\n## mu1_seat8    |  -0.59 | [-0.77, -0.41] |   100% | 1.000 | 3373.00\n## mu1_cargo3ft |   0.46 | [ 0.32,  0.60] |   100% | 1.000 | 6314.00\n## mu1_enghyb   |  -0.76 | [-0.92, -0.60] |   100% | 1.001 | 4628.00\n## mu1_engelec  |  -1.51 | [-1.69, -1.33] |   100% | 0.999 | 4913.00\n## mu1_price35  |  -0.82 | [-0.99, -0.67] |   100% | 0.999 | 4574.00\n## mu1_price40  |  -1.74 | [-1.94, -1.56] |   100% | 1.000 | 4637.00\n## mu2_seat6    |  -0.39 | [-0.57, -0.20] |   100% | 1.000 | 2387.00\n## mu2_seat7    |  -0.95 | [-1.15, -0.77] |   100% | 1.001 | 2470.00\n## mu2_seat8    |  -0.67 | [-0.85, -0.49] |   100% | 1.001 | 2489.00\n## mu2_cargo3ft |   0.49 | [ 0.35,  0.63] |   100% | 1.000 | 4836.00\n## mu2_enghyb   |  -0.79 | [-0.95, -0.63] |   100% | 1.000 | 4421.00\n## mu2_engelec  |  -1.40 | [-1.57, -1.22] |   100% | 1.000 | 4261.00\n## mu2_price35  |  -0.79 | [-0.95, -0.63] |   100% | 1.001 | 3699.00\n## mu2_price40  |  -1.47 | [-1.65, -1.29] |   100% | 0.999 | 3978.00\n## mu3_seat6    |  -0.28 | [-0.46, -0.11] | 99.85% | 1.000 | 2077.00\n## mu3_seat7    |  -0.78 | [-0.96, -0.60] |   100% | 1.000 | 3025.00\n## mu3_seat8    |  -0.63 | [-0.81, -0.46] |   100% | 1.000 | 2483.00\n## mu3_cargo3ft |   0.36 | [ 0.23,  0.50] |   100% | 0.999 | 5327.00\n## mu3_enghyb   |  -0.73 | [-0.88, -0.58] |   100% | 1.000 | 4039.00\n## mu3_engelec  |  -1.41 | [-1.59, -1.23] |   100% | 1.001 | 3818.00\n## mu3_price35  |  -0.85 | [-1.01, -0.69] |   100% | 1.000 | 4315.00\n## mu3_price40  |  -1.56 | [-1.75, -1.39] |   100% | 0.999 | 4774.00\n\nImportantly, the estimates here are all roughly equivalent to what we get from {mlogit}: the {mlogit} estimate for cargo3ft was 0.4775, while the three median posterior {brms} estimates are 0.46 (95% credible interval: 0.32–0.60), 0.49 (0.35–0.63), and 0.36 (0.23–0.50)\nSince all the features are randomly shuffled between the three options each time, and each option is selected 1/3rd of the time, it’s probably maybe legal to pool these posterior estimates together (maaaaybeee???) so that we don’t have to work with three separate estimates for each parameter? To do this we’ll take the average of each of the three \\(\\mu\\) estimates within each draw, which is also called “marginalizing” across the three options.\nHere’s how we’d do that with {tidybayes}. The medians are all roughly the same now!\n\nminivans_cat_marginalized &lt;- model_minivans_categorical_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_cat_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi()\n## # A tibble: 8 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo3ft   0.439  0.340  0.532   0.95 median qi       \n## 2 engelec   -1.44  -1.56  -1.32    0.95 median qi       \n## 3 enghyb    -0.762 -0.871 -0.651   0.95 median qi       \n## 4 price35   -0.823 -0.935 -0.713   0.95 median qi       \n## 5 price40   -1.59  -1.72  -1.47    0.95 median qi       \n## 6 seat6     -0.337 -0.464 -0.208   0.95 median qi       \n## 7 seat7     -0.862 -0.994 -0.734   0.95 median qi       \n## 8 seat8     -0.629 -0.753 -0.503   0.95 median qi\n\nAnd for fun, here’s what the posterior for new combined/collapsed/marginalized cargo3ft looks like. Great.\n\nminivans_cat_marginalized %&gt;% \n  filter(.variable == \"cargo3ft\") %&gt;% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye(fill = clrs[4]) +\n  labs(x = \"Posterior distribution of β (logit-scale)\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nAs we saw in the first example with chocolates, the marketing world typically uses predictions from these kinds of models to estimate the predicted market share for products with different constellations of features. That was a pretty straightforward task with the chocolate model since respondents were shown all 8 options simultaneously. It’s a lot trickier with the minivan example where respondents were shown 15 sets of 3 options. Dealing with multinomial predictions is a bear of a task because these models are a lot more complex.\n\nFrequentist predictions\nWith the chocolate model, we could just use avg_predictions(model_chocolate_mlogit) and automatically get predictions for all 8 options. That’s not the case here:\n\navg_predictions(model_minivans_mlogit)\n## \n##  Group Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n##      1    0.333   0.000349  955   &lt;0.001 0.332  0.334\n##      2    0.333   0.000145 2291   &lt;0.001 0.333  0.334\n##      3    0.334   0.000376  889   &lt;0.001 0.333  0.335\n## \n## Columns: group, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe get three predictions, and they’re all 33ish%. That’s because respondents were presented with three randomly shuffled options and chose one of them. All these predictions tell us is that across all 15 iterations of the questions, 1/3 of respondents selected the first option, 1/3 the second, and 1/3 the third. That’s a good sign in this case—there’s no evidence that people were just repeatedly choosing the first option. But in the end, these predictions aren’t super useful.\nWe instead want to be able to get predicted market shares (or predicted probabilities) for any given mix of products. For instance, here are six hypothetical products with different combinations of seats, cargo space, engines, and prices:\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\nexample_product_mix\n## # A tibble: 6 × 4\n##   seat  cargo eng   price\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1 7     2ft   hyb   30   \n## 2 6     2ft   gas   30   \n## 3 8     2ft   gas   30   \n## 4 7     3ft   gas   40   \n## 5 6     2ft   elec  40   \n## 6 7     2ft   hyb   35\n\nIf we were working with any other type of model, we could plug this data into the newdata argument of predictions() and get predicted values. That doesn’t work here though. There were 200 respondents in the original data, and {mlogit}-predictions need to happen on a dataset with a multiple of that many rows. We can’t just feed it 6 values.\n\npredictions(model_minivans_mlogit, newdata = example_product_mix)\n## Error: The `newdata` argument for `mlogit` models must be a data frame with a number of rows equal to a multiple of the number of choices: 200.\n\nInstead, following Chapman and Feit (2019) (and this Stan forum post), we can manually multiply the covariates in example_product_mix with the model coefficients to calculate “utility” (or predicted vales on the logit scale), which we can then exponentiate and divide to calculate market shares.\n\n\n\n\n\n\nLimits of {marginaleffects} and {mlogit}\n\n\n\nFrom what I can tell, this is not possible with {marginaleffects} because that package can’t work with the coefficients from {mlogit} models and can only really work with predictions. {mlogit} predictions are forced to be on the response/probability scale since they’re, like, predictions, so there’s no way to get them on the log/logit link scale to calculate utilities and shares.\n\n\n\n# Create a matrix of 0s and 1s for the values in `example_product_mix`, omitting\n# the first column (seat6)\nexample_product_dummy_encoded &lt;- model.matrix(\n  update(model_minivans_mlogit$formula, 0 ~ .),\n  data = example_product_mix\n)[, -1]\nexample_product_dummy_encoded\n##   seat7 seat8 cargo3ft enghyb engelec price35 price40\n## 1     1     0        0      1       0       0       0\n## 2     0     0        0      0       0       0       0\n## 3     0     1        0      0       0       0       0\n## 4     1     0        1      0       0       0       1\n## 5     0     0        0      0       1       0       1\n## 6     1     0        0      1       0       1       0\n\n# Matrix multiply the matrix of 0s and 1s with the model coefficients to get\n# logit-scale predictions, or utility\nutility &lt;- example_product_dummy_encoded %*% coef(model_minivans_mlogit)\n\n# Divide each exponentiated utility by the sum of the exponentiated utilities to\n# get the market share\nshare &lt;- exp(utility) / sum(exp(utility))\n\n# Stick all of these in one final dataset\nbind_cols(share = share, logits = utility, example_product_mix)\n## # A tibble: 6 × 6\n##   share[,1] logits[,1] seat  cargo eng   price\n##       &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1    0.113      -1.35  7     2ft   hyb   30   \n## 2    0.433       0     6     2ft   gas   30   \n## 3    0.319      -0.306 8     2ft   gas   30   \n## 4    0.0728     -1.78  7     3ft   gas   40   \n## 5    0.0167     -3.26  6     2ft   elec  40   \n## 6    0.0452     -2.26  7     2ft   hyb   35\n\n\n\n\n\n\n\nFunction version of this kind of prediction\n\n\n\n\n\nOn p. 375 of Chapman and Feit (2019) (and at this Stan forum post), there’s a function called predict.mnl() that does this utility and share calculation automatically. Because this post is more didactic and because I’m more interested in the Bayesian approach, I didn’t use it earlier, but it works well.\n\npredict.mnl &lt;- function(model, data) {\n  # Function for predicting shares from a multinomial logit model \n  # model: mlogit object returned by mlogit()\n  # data: a data frame containing the set of designs for which you want to \n  #       predict shares. Same format at the data used to estimate model. \n  data.model &lt;- model.matrix(update(model$formula, 0 ~ .), data = data)[ , -1]\n  utility &lt;- data.model %*% model$coef\n  share &lt;- exp(utility) / sum(exp(utility))\n  cbind(share, data)\n}\n\npredict.mnl(model_minivans_mlogit, example_product_mix)\n##     share seat cargo  eng price\n## 1 0.11273    7   2ft  hyb    30\n## 2 0.43337    6   2ft  gas    30\n## 3 0.31918    8   2ft  gas    30\n## 4 0.07281    7   3ft  gas    40\n## 5 0.01669    6   2ft elec    40\n## 6 0.04521    7   2ft  hyb    35\n\n\n\n\nThis new predicted share column sums to one, and it shows us the predicted market share assuming these are the only six products available. The $30,000 six-seater 2ft gas van and the $30,000 eight-seater 2ft gas van would comprise more than 75% (0.43337 + 0.31918) of a market consisting of these six products. Because we did this calculation by hand, we lose all of {marginaleffects}’s extra features like standard errors and hypothesis tests. Alas.\n\n\nBayesian predictions\nIf we use the categorical multinomial {brms} model we run into the same issue of getting weird predictions. Here it shows that 2/3rds of predictions are 0, which makes sense—if a respondent is offered 10 iterations of 3 possible choices, that would be 30 total choices, but they can only choose one option per iteration, so 20 choices (or 20/30 or 2/3) wouldn’t be selected. The other three groups are each 11%, since that’s the remaining 33% divided evenly across three options. Neat, I guess, but still not super helpful.\n\navg_predictions(model_minivans_categorical_brms)\n## \n##  Group Estimate 2.5 % 97.5 %\n##      0    0.667 0.657  0.676\n##      1    0.109 0.103  0.115\n##      2    0.112 0.105  0.118\n##      3    0.113 0.106  0.119\n## \n## Columns: group, estimate, conf.low, conf.high\n\nInstead of going through the manual process of matrix-multiplying a dataset of some mix of products with a single set of coefficients, we can use predictions(..., type = \"link\") to get predicted values on the log-odds scale, or that utility value that we found before.\n\n\n\n\n\n\nmarginaleffects::predictions() vs. {tidybayes} functions\n\n\n\nWe can actually use either marginaleffects::predictions() or {tidybayes}’s *_draw() functions for these posterior predictions. They do the same thing, with slightly different syntax:\n\n# Logit-scale predictions with marginaleffects::predictions()\nmodel_minivans_categorical_brms %&gt;% \n  predictions(newdata = example_product_mix, re_formula = NA, type = \"link\") %&gt;% \n  posterior_draws()\n\n# Logit-scale predictions with tidybayes::add_linpred_draws()\nmodel_minivans_categorical_brms %&gt;% \n  add_linpred_draws(newdata = example_product_mix, re_formula = NA)\n\nEarlier in the chocolate example, I used marginaleffects::predictions() with the Bayesian {brms} model. Here I’m going to switch to the {tidybayes} prediction functions instead, in part because these multinomial models with the categorical() family are a lot more complex (though {marginaleffects} can handle them nicely), but mostly because in the actual paper I’m working on with real conjoint data, our MCMC results were generated with raw Stan code through rstan, and {marginaleffects} doesn’t support raw Stan models.\nCheck out this guide for the differences between {tidybayes}’s three general prediction functions: predicted_draws(), epred_draws(), and linpred_draws().\n\n\nAdditionally, we now actually have 4,000 draws in 3 categories (option 1, option 2, and option 3), so we actually have 12,000 sets of coefficients (!). To take advantage of the full posterior distribution of these coefficients, we can calculate shares within each set of draws within each of the three categories, resulting in a distribution of shares rather than single values.\n\ndraws_df &lt;- example_product_mix %&gt;% \n  add_linpred_draws(model_minivans_categorical_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes\n  group_by(.draw, .category) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nWe can summarize this huge dataset of posterior shares to get medians and credible intervals, but we need to do one extra step first. Right now, we have three predictions for each mix type, one for each of the categories (i.e. option 1, option 2, and option 3.\n\nshares_df %&gt;% \n  group_by(mix_type, .category) %&gt;% \n  median_qi(share)\n## # A tibble: 18 × 8\n##    mix_type      .category  share .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 1         0.0161 0.0123 0.0208   0.95 median qi       \n##  2 6 2ft elec 40 2         0.0238 0.0183 0.0302   0.95 median qi       \n##  3 6 2ft elec 40 3         0.0216 0.0168 0.0275   0.95 median qi       \n##  4 7 2ft hyb 35  1         0.0513 0.0400 0.0647   0.95 median qi       \n##  5 7 2ft hyb 35  2         0.0485 0.0379 0.0605   0.95 median qi       \n##  6 7 2ft hyb 35  3         0.0532 0.0424 0.0660   0.95 median qi       \n##  7 7 3ft gas 40  1         0.0693 0.0543 0.0876   0.95 median qi       \n##  8 7 3ft gas 40  2         0.0891 0.0705 0.111    0.95 median qi       \n##  9 7 3ft gas 40  3         0.0775 0.0615 0.0967   0.95 median qi       \n## 10 7 2ft hyb 30  1         0.117  0.0976 0.139    0.95 median qi       \n## 11 7 2ft hyb 30  2         0.107  0.0891 0.128    0.95 median qi       \n## 12 7 2ft hyb 30  3         0.124  0.104  0.146    0.95 median qi       \n## 13 8 2ft gas 30  1         0.326  0.292  0.363    0.95 median qi       \n## 14 8 2ft gas 30  2         0.314  0.282  0.348    0.95 median qi       \n## 15 8 2ft gas 30  3         0.299  0.267  0.331    0.95 median qi       \n## 16 6 2ft gas 30  1         0.418  0.380  0.457    0.95 median qi       \n## 17 6 2ft gas 30  2         0.416  0.379  0.454    0.95 median qi       \n## 18 6 2ft gas 30  3         0.423  0.387  0.462    0.95 median qi\n\nSince those options were all randomized, we can lump them all together as a single choice. To do this we’ll take the average share across the three categories (this is also called “marginalizing”) within each posterior draw.\n\nshares_marginalized &lt;- shares_df %&gt;% \n  # Marginalize across categories within each draw\n  group_by(mix_type, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ungroup()\n\nshares_marginalized %&gt;% \n  group_by(mix_type) %&gt;% \n  median_qi(share)\n## # A tibble: 6 × 7\n##   mix_type       share .lower .upper .width .point .interval\n##   &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6 2ft elec 40 0.0206 0.0173 0.0242   0.95 median qi       \n## 2 7 2ft hyb 35  0.0512 0.0435 0.0600   0.95 median qi       \n## 3 7 3ft gas 40  0.0788 0.0673 0.0915   0.95 median qi       \n## 4 7 2ft hyb 30  0.116  0.103  0.131    0.95 median qi       \n## 5 8 2ft gas 30  0.313  0.291  0.337    0.95 median qi       \n## 6 6 2ft gas 30  0.419  0.394  0.446    0.95 median qi\n\nAnd we can plot them:\n\nshares_marginalized %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(fill = clrs[10], normalize = \"xy\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\n\nThis is great because (1) it includes the uncertainty in the estimated shares, and (2) it lets us do neat Bayesian inference and say things like “there’s a 93% chance that in this market of 6 options, a $30,000 6-passenger gas minivan with 2 feet of storage would reach at least 40% market share”:\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  summarize(prop_greater_40 = sum(share &gt;= 0.4) / n())\n## # A tibble: 1 × 1\n##   prop_greater_40\n##             &lt;dbl&gt;\n## 1           0.931\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt;= 0.4)), fill = clrs[10]) +\n  geom_vline(xintercept = 0.4) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[10], 0.4), guide = \"none\") +\n  labs(x = \"Predicted market share\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nAs explained in the AMCEs section for the chocolate data, in the social sciences we’re less concerned about predicted market shares and more concerned about causal effects. Holding all other features constant, what is the effect of a $5,000 increase in price or moving from 2 feet → 3 feet of storage space on the probability (or favorability) of selecting a minivan?\nIn the chocolate example, we were able to use marginaleffects::avg_comparisons() with the Bayesian model and get categorical contrasts automatically. This was because we cheated and used a Poisson model, since those can secretly behave like multinomial models. For the frequentist {mlogit}-based model, we had to use marginaleffects::predictions() instead and specify a special by argument to collapse the predictions into the different contrasts we were interested in.\nIn this case, since both the frequentist and Bayesian models for minivans are true multinomial models, we have to return to {marginaleffects}’s special syntax that lets us pass a data frame as the by argument.\n\nFrequentist comparisons/contrasts\nTo help with the intuition behind this, since it’s more complex this time, we’ll first create a data frame with all 54 combinations of all the feature levels (3 seats × 2 cargos × 3 engines × 3 prices) and create a by column label that concatenates all the labels together so there’s a single unique label for each row:\n\nby_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price) %&gt;% \n  mutate(by = paste(seat, cargo, eng, price, sep = \"_\"))\nby_all_combos\n## # A tibble: 54 × 5\n##    seat  cargo eng   price by           \n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;        \n##  1 6     2ft   gas   30    6_2ft_gas_30 \n##  2 6     2ft   gas   35    6_2ft_gas_35 \n##  3 6     2ft   gas   40    6_2ft_gas_40 \n##  4 6     2ft   hyb   30    6_2ft_hyb_30 \n##  5 6     2ft   hyb   35    6_2ft_hyb_35 \n##  6 6     2ft   hyb   40    6_2ft_hyb_40 \n##  7 6     2ft   elec  30    6_2ft_elec_30\n##  8 6     2ft   elec  35    6_2ft_elec_35\n##  9 6     2ft   elec  40    6_2ft_elec_40\n## 10 6     3ft   gas   30    6_3ft_gas_30 \n## # ℹ 44 more rows\n\nWe can then feed this by_all_combos data frame into predictions(), which will generate predictions for all these levels collapsed by all three of the possible groups (i.e. option 1, option 2, and option 3). We can then split the by column back into separate columns for each of the feature levels so that we have those original columns back.\n\nall_preds_mlogit &lt;- predictions(\n  model_minivans_mlogit,\n  # predictions.mlogit() weirdly gets mad when working with tibbles :shrug:\n  by = as.data.frame(by_all_combos)\n) %&gt;%\n  # Split the `by` column up into separate columns\n  separate(by, into = c(\"seat\", \"cargo\", \"eng\", \"price\"))\nas_tibble(all_preds_mlogit)\n## # A tibble: 54 × 10\n##    estimate std.error statistic   p.value conf.low conf.high seat  cargo eng   price\n##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n##  1   0.348    0.0135       25.8 1.89e-146   0.321     0.374  6     2ft   elec  30   \n##  2   0.173    0.00907      19.0 7.82e- 81   0.155     0.190  6     2ft   elec  35   \n##  3   0.0872   0.00565      15.4 1.04e- 53   0.0761    0.0983 6     2ft   elec  40   \n##  4   0.695    0.0122       57.0 0           0.671     0.719  6     2ft   gas   30   \n##  5   0.491    0.0147       33.3 1.08e-243   0.462     0.520  6     2ft   gas   35   \n##  6   0.311    0.0130       23.8 1.40e-125   0.285     0.336  6     2ft   gas   40   \n##  7   0.499    0.0138       36.3 3.76e-288   0.472     0.526  6     2ft   hyb   30   \n##  8   0.300    0.0126       23.8 8.97e-125   0.275     0.324  6     2ft   hyb   35   \n##  9   0.161    0.00952      16.9 3.90e- 64   0.142     0.180  6     2ft   hyb   40   \n## 10   0.430    0.0147       29.2 2.13e-187   0.402     0.459  6     3ft   elec  30   \n## # ℹ 44 more rows\n\nThere are a lot of predicted probabilities here, so we need to collapse and average these by groups to make any sense of them. For instance, suppose we’re interested in the AMCE of cargo space. We can first find the average predicted probability of selection with some grouping and summarizing:\n\nmanual_cargo_example &lt;- all_preds_mlogit %&gt;% \n  group_by(cargo) %&gt;% \n  summarize(avg_pred = mean(estimate))\nmanual_cargo_example\n## # A tibble: 2 × 2\n##   cargo avg_pred\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 2ft      0.292\n## 2 3ft      0.375\n\ndiff(manual_cargo_example$avg_pred)\n## [1] 0.08326\n\nHolding all other features constant, the average probability (or average favorability, or average market share, or whatever we want to call it) of selecting a minivan with 2 feet of storage space is 0.292 (this is the average of the 27 predictions from all_preds_mlogit where cargo = 2ft); the average probability for a minivan with 3 feet of storage space is 0.375 (again, this is the average of the 27 predictions from all_preds_mlogit where cargo = 3ft). There’s an 8.3 percentage point difference between these groups. This is the causal effect or AMCE: switching from 2 feet to 3 feet increases minivan favorability by 8 percentage points on average.\nThis manual calculation works, but it’s tedious and doesn’t include anything about standard errors. So instead, we can do it automatically with predictions(..., by = data.frame(...)).\n\npreds_minivan_cargo_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    cargo = levels(minivans$cargo),\n    by = levels(minivans$cargo)\n  )\n)\npreds_minivan_cargo_mlogit\n## \n##  Estimate Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %  By\n##     0.291    0.00440 66.2   &lt;0.001 0.283  0.300 2ft\n##     0.375    0.00441 85.2   &lt;0.001 0.367  0.384 3ft\n## \n## Columns: estimate, std.error, statistic, p.value, conf.low, conf.high, by\n\nAnd if we use the hypothesis argument (or the standalone hypotheses() function), we can get the difference between these average predictions, or the AMCE we care about—moving from 2 feet → 3 feet causes an 8 percentage point increase in favorability.\n\nhypotheses(preds_minivan_cargo_mlogit, hypothesis = \"revpairwise\")\n## \n##       Term Estimate Std. Error   z Pr(&gt;|z|)  2.5 % 97.5 %\n##  3ft - 2ft   0.0837    0.00881 9.5   &lt;0.001 0.0664  0.101\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nThe hypothesis functionality works with more than 2 levels too. If calculate average predictions for the 3 seat configurations and specify pariwise or revpairwise, {marginaleffects} will give three differences: row 2 − row 1; row 3 − row 1; and row 3 − row 2. If we specify reference or revreference, it’ll only give two, all based on the first row.\n\npreds_minivan_seat_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    seat = levels(minivans$seat),\n    by = levels(minivans$seat)\n  )\n)\n\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"revpairwise\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6  -0.0996     0.0107 -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6  -0.0557     0.0109 -5.11   &lt;0.001 -0.0771 -0.0343\n##  8 - 7   0.0439     0.0106  4.13   &lt;0.001  0.0230  0.0647\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"reference\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6  -0.0996     0.0107 -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6  -0.0557     0.0109 -5.11   &lt;0.001 -0.0771 -0.0343\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can specify any other comparisons with bN, where N stands for the row number from predictions(). For instance, if we just want the difference between 6 (row 1) and 8 (row 2), we can do this:\n\nhypotheses(preds_minivan_seat_mlogit, hypothesis = \"b2 = b1\")\n## \n##   Term Estimate Std. Error     z Pr(&gt;|z|)  2.5 %  97.5 %\n##  b2=b1  -0.0996     0.0107 -9.29   &lt;0.001 -0.121 -0.0786\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nWe can make a big data frame with all the AMCEs we’re interested in. I’ve hidden the code here because it’s really repetitive.\n\n\nMake separate datasets of predictions and combine them in one data frame\npreds_minivan_seat_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    seat = levels(minivans$seat),\n    by = levels(minivans$seat)\n  )\n)\n\npreds_minivan_cargo_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    cargo = levels(minivans$cargo),\n    by = levels(minivans$cargo)\n  )\n)\n\npreds_minivan_eng_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    eng = levels(minivans$eng),\n    by = levels(minivans$eng)\n  )\n)\n\npreds_minivan_price_mlogit &lt;- predictions(\n  model_minivans_mlogit, \n  by = data.frame(\n    price = levels(minivans$price),\n    by = levels(minivans$price)\n  )\n)\n\namces_minivan_mlogit &lt;- bind_rows(\n  seat = hypotheses(preds_minivan_seat_mlogit, hypothesis = \"reference\"),\n  cargo = hypotheses(preds_minivan_cargo_mlogit, hypothesis = \"reference\"),\n  eng = hypotheses(preds_minivan_eng_mlogit, hypothesis = \"reference\"),\n  # Need to specify these two tests manually because `hypotheses()` reeeeallly\n  # wants to use 35 as the reference level because it's the first value that\n  # appears in the original data. See\n  # https://github.com/vincentarelbundock/marginaleffects/issues/861\n  price = bind_rows(\n    hypotheses(preds_minivan_price_mlogit, hypothesis = \"b1 = b2\") %&gt;% mutate(term = \"35 - 30\"),\n    hypotheses(preds_minivan_price_mlogit, hypothesis = \"b3 = b2\") %&gt;% mutate(term = \"40 - 30\")\n  ),\n  .id = \"variable\"\n)\n\n\n\namces_minivan_mlogit\n## \n##        Term Estimate Std. Error      z Pr(&gt;|z|)   2.5 %  97.5 %\n##  7 - 6       -0.0996    0.01072  -9.29   &lt;0.001 -0.1206 -0.0786\n##  8 - 6       -0.0557    0.01091  -5.11   &lt;0.001 -0.0771 -0.0343\n##  3ft - 2ft    0.0837    0.00881   9.50   &lt;0.001  0.0664  0.1010\n##  gas - elec   0.2785    0.01021  27.28   &lt;0.001  0.2585  0.2985\n##  hyb - elec   0.1156    0.01032  11.20   &lt;0.001  0.0954  0.1358\n##  35 - 30      0.1767    0.01117  15.82   &lt;0.001  0.1548  0.1986\n##  40 - 30     -0.1333    0.01014 -13.14   &lt;0.001 -0.1532 -0.1134\n## \n## Columns: variable, term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\n\n\nBayesian comparisons/contrasts\nUnlike the chocolate example, where the outcome variable was binary, we have to do similar by-like shenanigans with the Bayesian minivan model here. We could theoretically work with things like marginaleffects::comparisons() or marginaleffects::slopes() to extract the AMCEs from the model, but as I’ll show below, there are some weird mathy things we have to deal with because of the multinomial outcome, and I think it’s beyond what {marginaleffects} is designed to easily do.\nSo instead we can use epred_draws() from {tidybayes} and calculate posterior predictions ourselves (see this guide for an overview of all of {tidybayes}’s different prediction functions).\nTo illustrate why predicting things with this multinomial model is so weird, we’ll first predict the probability that someone chooses a $30,000 6-seater electric van with 2 feet of storage space. For this combination of minivan characteristics, there’s a 66% chance that someone does not select it, shown as category 0. That means there’s a 33% chance that someone does select it. Because options 1, 2 and 3 were randomized, that 33% is split evenly across categories 1, 2, and 3 in the predictions here.\n\none_prediction &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = data.frame(\n    seat = \"6\", cargo = \"2ft\", eng = \"elec\", price = \"30\", resp.id = 1)\n  )\n\none_prediction %&gt;% \n  group_by(.category) %&gt;% \n  median_qi(.epred)\n## # A tibble: 4 × 7\n##   .category .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0          0.661 0.629   0.694   0.95 median qi       \n## 2 1          0.104 0.0843  0.128   0.95 median qi       \n## 3 2          0.112 0.0894  0.138   0.95 median qi       \n## 4 3          0.121 0.0990  0.147   0.95 median qi\n\nWe could add the predictions for categories 1, 2, and 3 together, but that would take a bit of extra data manipulation work. Instead, we can rely on the the fact that the prediction for category 0 is actually the inverse of the sum of categories 1+2+3, so we can instead just use 1 - .epred and only look at category 0. Even though the category column here says 0, it’s really the combined probability of choosing options 1, 2, or 3:\n\none_prediction %&gt;% \n  mutate(.epred = 1 - .epred) %&gt;%\n  filter(.category == 0) %&gt;% \n  median_qi(.epred)\n## # A tibble: 1 × 13\n##   seat  cargo eng   price resp.id  .row .category .epred .lower .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     2ft   elec  30          1     1 0          0.339  0.306  0.371   0.95 median qi\n\nWith {mlogit}, we found AMCEs by essentially calculating marginal means for specific contrasts of predicted probabilities. We created a data frame of all 54 combinations of feature levels and then grouped and summarized that data frame as needed (e.g., the average of the 27 predictions for 2 feet of cargo space and the average of the 27 predictions for 3 feed of cargo space).\nWe can do the same thing with the {brms} model, but selecting only the 0 category and reversing the predicted value:\n\nnewdata_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price) %&gt;% \n  mutate(resp.id = 1)\n\nall_preds_brms &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = newdata_all_combos) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nTo make sure it worked, here are the posterior medians for all the different levels. It’s roughly the same as what we found with in all_preds_mlogit:\n\nall_preds_brms %&gt;% \n  group_by(seat, cargo, eng, price) %&gt;% \n  median_qi(.epred)\n## # A tibble: 54 × 10\n##    seat  cargo eng   price .epred .lower .upper .width .point .interval\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6     2ft   gas   30    0.682  0.651   0.713   0.95 median qi       \n##  2 6     2ft   gas   35    0.485  0.451   0.520   0.95 median qi       \n##  3 6     2ft   gas   40    0.305  0.275   0.338   0.95 median qi       \n##  4 6     2ft   hyb   30    0.502  0.466   0.537   0.95 median qi       \n##  5 6     2ft   hyb   35    0.306  0.276   0.338   0.95 median qi       \n##  6 6     2ft   hyb   40    0.171  0.150   0.194   0.95 median qi       \n##  7 6     2ft   elec  30    0.339  0.306   0.371   0.95 median qi       \n##  8 6     2ft   elec  35    0.183  0.161   0.207   0.95 median qi       \n##  9 6     2ft   elec  40    0.0952 0.0819  0.110   0.95 median qi       \n## 10 6     3ft   gas   30    0.769  0.743   0.795   0.95 median qi       \n## # ℹ 44 more rows\n\nTo pull out specific group-level averages, we can group and summarize. For example, here are the posterior median predictions for the two levels of cargo space:\n\nall_preds_brms %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi(.epred)\n## # A tibble: 2 × 7\n##   cargo .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft    0.256 0.0606  0.676   0.95 median qi       \n## 2 3ft    0.348 0.0905  0.763   0.95 median qi\n\nThe medians here are correct and basically what we found with {mlogit}, but the credible intervals are wildly off (5% to 75% favorability?!). If we plot this we can see why:\n\nall_preds_brms %&gt;% \n  ggplot(aes(x = .epred, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\n\nhahahaha check out those mountain ranges. All those peaks come from combining the 27 different 2ft- and 3ft- posterior distributions for all the different combinations of other feature levels.\nTo get the actual marginal mean for cargo space, we need to marginalize out (or average out) all those other covariates. To do this, we need to group by the cargo column and the .draw column so that we find the average within each set of MCMC draws. To help with the intuition, look how many rows are in each of these groups of cargo and .draw—there are 27 different estimates for each of the 4,000 draws for 2 feet and 27 different estimates for each of the 4,000 draws for 3 feet. We want to collapse (or marginalize) those 27 rows into just one average.\n\nall_preds_brms %&gt;% \n  group_by(cargo, .draw) %&gt;% \n  summarize(nrows = n())\n## # A tibble: 8,000 × 3\n## # Groups:   cargo [2]\n##    cargo .draw nrows\n##    &lt;fct&gt; &lt;int&gt; &lt;int&gt;\n##  1 2ft       1    27\n##  2 2ft       2    27\n##  3 2ft       3    27\n##  4 2ft       4    27\n##  5 2ft       5    27\n##  6 2ft       6    27\n##  7 2ft       7    27\n##  8 2ft       8    27\n##  9 2ft       9    27\n## 10 2ft      10    27\n## # ℹ 7,990 more rows\n\nTo do that, we can find the average predicted value in those groups, then work with that as our main estimand. Check out these marginalized-out posteriors now—the medians are the same as before, but the credible intervals make a lot more sense:\n\npreds_cargo_marginalized &lt;- all_preds_brms %&gt;% \n  # Marginalize out the other covariates\n  group_by(cargo, .draw) %&gt;%\n  summarize(avg = mean(.epred))\n\npreds_cargo_marginalized %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi()\n## # A tibble: 2 × 7\n##   cargo   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft   0.292  0.275  0.309   0.95 median qi       \n## 2 3ft   0.374  0.356  0.393   0.95 median qi\n\nWe can confirm that marginalizing out the other covariates worked by plotting it:\n\npreds_cargo_marginalized %&gt;% \n  ggplot(aes(x = avg, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\n\nheck. yes.\nFinally, we’re actually most interested in the AMCE, or the difference between these two cargo sizes. The compare_levels() function from {tidybayes} can calculate this automatically:\n\npreds_cargo_marginalized %&gt;%\n  compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n  median_qi(avg)\n## # A tibble: 1 × 7\n##   cargo        avg .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 3ft - 2ft 0.0830 0.0643  0.100   0.95 median qi\n\nThat’s it! The causal effect of moving from 2 feet → 3 feet of storage space, holding all other features constant, is 8 percentage points (with a 95% credible interval of 6.5 to 10 percentage points).\nWe can combine all these AMCEs into a huge data frame. The marginalization process + compare_levels() has to happen with one feature at a time, so we need to create several separate data frames:\n\n# I could probably do this with purrr::map() to reduce all this repetition, but\n# whatever, it works\namces_minivan_brms &lt;- bind_rows(\n  seat = all_preds_brms %&gt;% \n    group_by(seat, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms %&gt;% \n    group_by(cargo, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms %&gt;% \n    group_by(eng, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms %&gt;% \n    group_by(price, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_brms %&gt;% \n  group_by(term, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 7 × 8\n##   term  contrast       avg  .lower  .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo 3ft - 2ft   0.0830  0.0643  0.100    0.95 median qi       \n## 2 eng   elec - gas -0.279  -0.300  -0.256    0.95 median qi       \n## 3 eng   hyb - gas  -0.161  -0.184  -0.138    0.95 median qi       \n## 4 price 35 - 30    -0.178  -0.201  -0.155    0.95 median qi       \n## 5 price 40 - 30    -0.309  -0.330  -0.287    0.95 median qi       \n## 6 seat  7 - 6      -0.0995 -0.121  -0.0785   0.95 median qi       \n## 7 seat  8 - 6      -0.0570 -0.0788 -0.0355   0.95 median qi\n\n\n\nPlots\nAgain, plotting these AMCEs so that there’s a reference category at 0 requires some extra data work, so I’ve hidden all that code for the sake of space.\n\n\nExtract variable labels\nminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plot\namces_minivan_mlogit_split &lt;- amces_minivan_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- minivan_var_levels %&gt;%\n  left_join(\n    amces_minivan_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make {brms} plot\nposterior_mfx_minivan_nested &lt;- amces_minivan_brms %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_bayes &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_mfx_minivan_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_minivan_bayes, aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 + p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nWe’ve cheated a little and have already used multilevel structures in the Bayesian models for the chocolate experiment and the minivan experiment. This was because these datasets had a natural panel grouping structure inside them. {mlogit} can work with panel-indexed data frames (that’s the point of that strange dfidx() function). By creating respondent-specific intercepts like we did with the {brms} models, we helped account for some of the variation caused by respondent differences.\nBut we can do better than that and get far richer and more complex models and estimates and predictions. In addition to using respondent-specific intercepts, we can (1) include respondent-level characteristics as covariates, and (2) include respondent-specific slopes for the minivan characteristic.\nIn the minivan data, we have data on feature levels (seat, cargo, eng, price) and on individual characteristics (carpool). The carpool variable indicates if the respondent uses their vehicle for carpooling. This is measured at the respondent level and not the choice level (i.e. someone won’t stop being a carpooler during one set of choices and then resume being a carpooler for another set). We can visualize where these different columns are measured by returning to the hierarchical model diagram:\n\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents, w ith variables measured at different levels\n\n\n\n\nWe can use hierarchical models (or multilevel models, or mixed effects models, or whatever you want to call them) to account for choice-level and respondent-level covariates and incorporate respondent-level heterogeneity and covariance into the model estimates.\n\n\n\nImage by Chelsea Parlett-Pelleriti"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Important sidenote on notation",
    "text": "Important sidenote on notation\nBut before looking at how to incorporate that carpool column into the model, we need to take a quick little detour into the world of notation. There’s no consistent way of writing out multilevel models,1 and accordingly, I thought it was impossible to run fully specified marketing-style hierarchical Bayesian models with {brms}—all because of notation!1 These are not the only approaches—section 12.5 in Gelman and Hill (2007) is called “Five ways to write the same model,” and they don’t include the offset notation as one of their five!\nThere are a couple general ways I’ve seen group-level random effects written out in formal model notation: one with complete random β terms and one with random offsets from a global β term.\n\n\n\n\n\n\n{brms} / {lme4} syntax\n\n\n\nFor the best overview of how to use {brms} and {lme4} with different random group-level intercept and slope specifications, check out this summary table by Ben Bolker.\n\n\n\nRandom intercepts\nIf you want group-specific intercept terms, you can use a formula like this:\nbf(y ~ x + (1 | group))\nIn formal mathy terms, we can write this group-specific intercept as a complete coefficient: \\(\\beta_{0_j}\\). Each group \\(j\\) gets its own intercept coefficient. Nice and straightforward.\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Random group-specific intercepts}\n\\end{aligned}\n\\]\nHowever, I actually like to think of these random effects in a slightly different way, where each group intercept is actually a combination of a global average (\\(\\beta_0\\)) and a group-specific offset from that average (\\(b_{0_j}\\)), like this:\n\\[\n\\beta_{0_j} = \\beta_0 + b_{0_j}\n\\]\nThat offset is assumed to be normally distributed with a mean of 0 (\\(\\mathcal{N}(0, \\sigma_0)\\)):\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random group-specific offsets from global intercept}\n\\end{aligned}\n\\]\nI prefer this offset notation because it aligns with the output of {brms}, which reports population-level coefficients (i.e. the global average \\(\\beta_0\\)) along with group-specific offsets from that average (i.e. \\(b_{0_j}\\)), which you can access with ranef(model_name).\n\n\nRandom slopes\nIf you want group-specific intercepts and slopes, you can use a formula like this:\nbf(y ~ x + (1 + x | group))\nThe same dual syntax applies when using random slopes too. We can either use whole group-specific \\(\\beta_{n_j}\\) terms, or use offsets (\\(b_{n_j}\\)) from a global average slope (\\(\\beta_n\\)). When working with random slopes, the math notation gets a little fancier because the random intercept and slope terms are actually correlated and move together across groups. The \\(\\beta\\) terms come from a multivariate (or joint) normal distribution with shared covariance.\nWith the complete β approach, we’re estimating the joint distribution of \\(\\begin{pmatrix} \\beta_{0_j} \\\\ \\beta_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  \\beta_{0_j} \\\\\n  \\beta_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific slopes and intercepts}\n\\end{aligned}\n\\]\n\nWith the offset approach, we’re estimating the joint distribution of the offsets from the global intercept and slope, or \\(\\begin{pmatrix} b_{0_j} \\\\ b_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  b_{0_j} \\\\\n  b_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    0 \\\\\n    0 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific offsets from global intercept and slope}\n\\end{aligned}\n\\]\n\n\n\nSummary table\nAnd here’s a helpful little table summarizing these two types of notation (mostly for future me).\n\n\n\n\n\n\n\n\n\n\n\nFormula syntax\nFull \\(\\beta\\) notation\nOffset notation\n\n\n\n\nRandom intercept\ny ~ x + (1 | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                       \\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                      \\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0)                                                                                                                                                                                                                                              \n                                                                                                                                       \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            b_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0)                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]\n\n\nRandom intercept + slope\ny ~ x + (1 + x | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                       \\begin{array}{c}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                                                        \\beta_{0_j} \\\\                                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                         \\beta_{1_j}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                           &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                            \n                                                                                                                                                                                                                                                                                                                           \\beta_0 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\beta_1 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                          \\end{array}                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\right)                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                         , \\, \\Sigma                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\right) \\\\                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                        \\Sigma &\\sim                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\begin{array}{cc}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                       \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                   \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\end{aligned}                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} \\\\                                                                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           b_{0_j} \\\\                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             b_{1_j}                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\left(                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\begin{array}{c}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\end{array}                                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\right)                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           , \\, \\Sigma                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\right) \\\\                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\Sigma &\\sim                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{cc}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\end{aligned}                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Translating from marketing-style Stan notation to {brms} syntax",
    "text": "Translating from marketing-style Stan notation to {brms} syntax\nIn Chapman and Feit (2019) and in all the marketing papers I’ve seen that use hierarchical Bayesian models—and even one I coauthored! (Chaudhry, Dotson, and Heiss 2021) (see the appendix)—they define their models using notation like this:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nFor the longest time this threw me off because it’s slightly different from the two different notations we just reviewed (full βs vs. offsets from global β), and I figured that specifying a model like this with {brms} was impossible. The main reason for my confusion is that there are two different datasets involved in this model, and {brms} can only really work with one dataset.\nIn raw Stan (like in this tutorial on conjoint hierarchical Bayes models, or in this example of a different conjoint hierarchical model), you’d typically work with two different datasets or matrices: one \\(X\\) with feature levels and one \\(Z\\) with respondent-level characteristics. (This is actually the recommended way to write hierarchical models in raw Stan!).\nHere’s what separate \\(X\\) and \\(Z\\) matrices would look like with the minivan data—X contains the full data without respondent-level covariates like carpool and it has 9,000 rows; Z contains only respondent-level characteristics like carpool and it only has 200 rows (one per respondent).\n\nX &lt;- minivans %&gt;% select(-carpool)\nX\n## # A tibble: 9,000 × 8\n##    resp.id  ques   alt seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 6     2ft   gas   35         0\n##  2       1     1     2 8     3ft   hyb   30         0\n##  3       1     1     3 6     3ft   gas   30         1\n##  4       1     2     1 6     2ft   gas   30         0\n##  5       1     2     2 7     3ft   gas   35         1\n##  6       1     2     3 6     2ft   elec  35         0\n##  7       1     3     1 8     3ft   gas   35         1\n##  8       1     3     2 7     3ft   elec  30         0\n##  9       1     3     3 8     2ft   elec  40         0\n## 10       1     4     1 7     3ft   elec  40         1\n## # ℹ 8,990 more rows\n\nZ &lt;- minivans %&gt;% \n  # Only keep the first row of each respondent\n  group_by(resp.id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% \n  # Only keep the respondent-level columns\n  select(resp.id, carpool)\nZ\n## # A tibble: 200 × 2\n##    resp.id carpool\n##      &lt;dbl&gt; &lt;fct&gt;  \n##  1       1 yes    \n##  2       2 no     \n##  3       3 no     \n##  4       4 no     \n##  5       5 yes    \n##  6       6 no     \n##  7       7 no     \n##  8       8 yes    \n##  9       9 no     \n## 10      10 no     \n## # ℹ 190 more rows\n\nX and Z are then passed to Stan as separate matrices and used at different places in the model fitting process. Here’s what that looks like in pseudo-Stan code. The matrix of individual characteristics Z is matrix-multiplied with a bunch of estimated \\(\\gamma\\) coefficients (Gamma here) to generate individual-specific \\(\\beta\\) coefficients (Beta here). The matrix of choices X is then matrix-multiplied with the individual-specific \\(\\beta\\) coefficients to generate predicted outcomes (Y here).\nfor (r in 1:n_respondents) {\n  // All the individual-specific slopes and intercepts\n  Beta[,r] ~ multi_normal(Gamma * Z[,r], ...);\n\n  // All the question-level outcomes, using individual-specific slopes and intercepts\n  for (s in 1:n_questions) {\n     Y[r,s] ~ categorical_logit( X [r,s] * Beta[,r]);\n  }\n}\nThat’s a neat way of working with multilevel models, but it’s different from how I’ve always worked with them (and it requires working with raw Stan). As seen throughout this post, I’m a fan of {brms}’s formula-style syntax for specifying multilevel models, but {brms} can only work with one dataset at a time—you can’t pass it both X and Z like you’d do with raw Stan. So I (naively) figured that this went beyond {brms}’s abilities and was only possible with raw Stan.\nHowever, if we use {brms}’s special formula syntax, we can actually specify an identical model with only one dataset (again, see this for a fantastic overview of the syntax).\nFirst, let’s look at the marketing-style syntax again:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nThis is actually just a kind of really compact notation. That second line with the \\(\\beta \\sim \\operatorname{Multivariate}\\, \\mathcal{N}(\\cdot)\\) distribution is a shorthand version of the full-β syntax from earlier. To illustrate this, let’s expand this out to a more complete formal definition of the model. Instead of using \\(X\\) to stand in for all the feature levels and \\(Z\\) for all the individual characteristics, we’ll expand those to include all the covariates we’re using. And instead of calling the distribution “Multinomial logit” we’ll call it “Categorical” so it aligns with Stan. It’ll make for a really massive formula, but it shows what’s really going on.\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right]\n\\end{aligned}\n\\]\n\n \nImportantly, pay attention to where the choice-level and respondent-level variables show up in this expanded version. All the choice-level variables have respondent-specific \\(\\beta\\) coefficients, while the respondent-level variable (carpool) is down in that massive multivariate normal matrix with its own \\(\\gamma\\) coefficients, helping determine the respondent-specific \\(\\beta\\) coefficients. That’s great and exactly what we want, and we can do that with raw Stan, but raw Stan is no fun.\nWe can create this exact same model structure with {brms} like this:\nbf(choice_alt ~\n  # Choice-level predictors that are nested within respondents...\n  (seat + cargo + eng + price) *\n  # ...interacted with all respondent-level predictors...\n  (carpool) +\n  # ... with random respondent-specific slopes for the\n  # nested choice-level predictors\n  (1 + seat + cargo + eng + price | resp.id))\nWe can confirm that this worked by using the miraculous {equatiomatic} package, which automatically converts model objects into LaTeX code. {equatiomatic} doesn’t work with {brms} models, but it does work with frequentist {lme4} models, so we can fit a throwaway frequentist model with this syntax (it won’t actually converge and it’ll give a warning, but that’s fine—we don’t actually care about this model) and then feed it to equatiomatic::extract_eq() to see what it looks like in formal notation.\n(This is actually how I figured out the correct combination of interactions and random slopes—I kept trying different combinations that I thought were right until the math matched the huge full model above, with the \\(\\beta\\) and \\(\\gamma\\) terms in the right places.)\n\nlibrary(lme4)\nlibrary(equatiomatic)\n\nmodel_throwaway &lt;- lmer(\n  choice ~ (seat + cargo + eng + price) * (carpool) +\n    (1 + seat + cargo + eng + price | resp.id),\n  data = minivans\n)\n\nprint(extract_eq(model_throwaway))\n\\[\n\\begin{aligned}\n  \\operatorname{choice}_{i}  &\\sim N \\left(\\mu, \\sigma^2 \\right) \\\\\n    \\mu &=\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{seat}_{\\operatorname{7}}) + \\beta_{2j[i]}(\\operatorname{seat}_{\\operatorname{8}}) + \\beta_{3j[i]}(\\operatorname{cargo}_{\\operatorname{3ft}}) + \\beta_{4j[i]}(\\operatorname{eng}_{\\operatorname{hyb}}) + \\beta_{5j[i]}(\\operatorname{eng}_{\\operatorname{elec}}) + \\beta_{6j[i]}(\\operatorname{price}_{\\operatorname{35}}) + \\beta_{7j[i]}(\\operatorname{price}_{\\operatorname{40}}) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j} \\\\\n      &\\beta_{2j} \\\\\n      &\\beta_{3j} \\\\\n      &\\beta_{4j} \\\\\n      &\\beta_{5j} \\\\\n      &\\beta_{6j} \\\\\n      &\\beta_{7j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}})\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} & \\rho_{\\alpha_{j}\\beta_{2j}} & \\rho_{\\alpha_{j}\\beta_{3j}} & \\rho_{\\alpha_{j}\\beta_{4j}} & \\rho_{\\alpha_{j}\\beta_{5j}} & \\rho_{\\alpha_{j}\\beta_{6j}} & \\rho_{\\alpha_{j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{2j}\\alpha_{j}} & \\rho_{\\beta_{2j}\\beta_{1j}} & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{3j}\\alpha_{j}} & \\rho_{\\beta_{3j}\\beta_{1j}} & \\rho_{\\beta_{3j}\\beta_{2j}} & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{4j}\\alpha_{j}} & \\rho_{\\beta_{4j}\\beta_{1j}} & \\rho_{\\beta_{4j}\\beta_{2j}} & \\rho_{\\beta_{4j}\\beta_{3j}} & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{5j}\\alpha_{j}} & \\rho_{\\beta_{5j}\\beta_{1j}} & \\rho_{\\beta_{5j}\\beta_{2j}} & \\rho_{\\beta_{5j}\\beta_{3j}} & \\rho_{\\beta_{5j}\\beta_{4j}} & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{6j}\\alpha_{j}} & \\rho_{\\beta_{6j}\\beta_{1j}} & \\rho_{\\beta_{6j}\\beta_{2j}} & \\rho_{\\beta_{6j}\\beta_{3j}} & \\rho_{\\beta_{6j}\\beta_{4j}} & \\rho_{\\beta_{6j}\\beta_{5j}} & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{7j}\\alpha_{j}} & \\rho_{\\beta_{7j}\\beta_{1j}} & \\rho_{\\beta_{7j}\\beta_{2j}} & \\rho_{\\beta_{7j}\\beta_{3j}} & \\rho_{\\beta_{7j}\\beta_{4j}} & \\rho_{\\beta_{7j}\\beta_{5j}} & \\rho_{\\beta_{7j}\\beta_{6j}} & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for resp.id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nDifferent variations of group-level interactions\nThis syntax is a special R shortcut for interacting carpool with each of the feature variables:\n# Short way\n(seat + cargo + eng + price) * (carpool)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool\nIf we had other respondent-level columns like age (age) and education (ed), the shortcut syntax is really helpful:\n# Short way\n(seat + cargo + eng + price) * (carpool + age + ed)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool +\nseat*age + cargo*age + eng*age + price*age +\nseat*ed + cargo*ed + eng*ed + price*ed\nWe don’t necessarily need to fully interact everything. For instance, if we have theoretical reasons to think that carpool status is associated with seat count preferences, but not other features, we can only interact seat and carpool:\nbf(choice ~ \n    (seat * carpool) + cargo + eng + price + \n  (1 + seat + cargo + eng + price | resp.id))\n\n\n\n\n\n\nModel running times\n\n\n\nThe more individual-level interactions you add, the longer it will take for the model to run. As we’ll see below, interacting carpool with the four feature levels takes ≈30 minutes to fit. As you add more individual-level interactions, the running time blows up.\nIn the replication code for Jensen et al. (2021), where they model a ton of individual variables, they say it takes several days to run. Our models in Chaudhry, Dotson, and Heiss (2021) take hours and hours to run."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "mlogit model",
    "text": "mlogit model\n{mlogit} can estimate hierarchical models with something like this:\n\n# Define the random parameters\nmodel_mlogit_rpar &lt;- rep(\"n\", length = length(model_minivans_mlogit$coef))\nnames(model_mlogit_rpar) &lt;- names(model_minivans_mlogit$coef)\n\n# This means these random terms are all normally distributed\nmodel_mlogit_rpar\n#&gt; seat7    seat8 cargo3ft   enghyb  engelec  price35  price40 \n#&gt;   \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\" \n\n# Run the model with carpool as an individual-level covariate\nmodel_mlogit_hierarchical &lt;- mlogit(\n  choice ~ 0 + seat + eng + cargo + price | carpool,\n  data = minivans_idx,\n  panel = TRUE, rpar = model_mlogit_rpar, correlation = TRUE\n)\n\n# Show the results\nsummary(model_mlogit_hierarchical)\n\nHowever, I’m not a frequentist and I’m already not a huge fan of extracting the predictions and AMCEs from these {mlogit} models. Running and interpreting and working with the results of that object is left as an exercise for the reader :). (See p. 381 in Chapman and Feit (2019) for a worked example of how to do it.)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Finally, the full {brms} model",
    "text": "Finally, the full {brms} model\nThis is a really complex model with a ton of moving parts, but it’s also incredibly powerful. It lets us account for individual-specific differences across each of the minivan features. For instance, whether an individual carpools probably influences their preferences for the number of seats, and maybe cargo space, but probably doesn’t influence their preferences for engine type. If we had other individual-level characteristics, we could also let those influence feature preferences. Like, the number of kids an individual has probably influences seat count preferences; the individual’s income probably influences their price preferences; and so on.\nLet’s define the model more formally again, this time with priors for the parameters we’ll be estimating:\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right] \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for choice-level coefficients}] \\\\\n\\gamma^{\\beta_{0 \\dots 7}}_0 \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for individual-level coefficients}] \\\\\n\\sigma_{\\beta_{0 \\dots 7}} \\sim&\\ \\operatorname{Exponential}(1) \\qquad [\\text{Prior for between-respondent intercept and slope variability}] \\\\\n\\rho \\sim&\\ \\operatorname{LKJ}(1) \\qquad\\qquad [\\text{Prior for correlation between random slopes and intercepts}]\n\\end{aligned}\n\\]\n\n \nHere it is in code form. There are a couple new things here in the Stan settings. First, we’re going to create 4 MCMC chains with 4,000 draws rather than 2,000—there are so many parameters to be estimated that we need to let the simulation run longer. Second, we’ve modified the adapt_delta setting to 0.99. Conceptually, this adjusts the size of the steps the MCMC algorithm takes as it traverses the posterior space for each parameter—higher numbers make the steps smaller and more granular. This slows down the MCMC simulation, but it also helps avoid divergent transitions (or failed out-of-bounds draws).\nOn my 2021 M1 MacBook Pro, running through cmdstanr with 2 CPU cores per chain, it took about 30 minutes to fit. If you’re following along with this post, start running this and go get some lunch or go for a walk or something.\n\n\n\n\n\n\nPre-run model\n\n\n\nAlternatively, you can download an .rds file of this completed. This brm() code load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\n\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\n\n\n\n\nmodel_minivans_mega_mlm_brms &lt;- brm(\n  bf(choice_alt ~\n    # Choice-level predictors that are nested within respondents...\n    (seat + cargo + eng + price) *\n    # ...interacted with all respondent-level predictors...\n    (carpool) +\n    # ... with random respondent-specific slopes for the\n    # nested choice-level predictors\n    (1 + seat + cargo + eng + price | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3),\n    prior(lkj(1), class = cor)\n  ),\n  chains = 4, cores = 4, warmup = 1000, iter = 5000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), # refresh = 0,\n  control = list(adapt_delta = 0.9),\n  file = \"models/model_minivans_mega_mlm_brms\"\n)\n\nThis model is incredibly rich. We just estimated more than 5,000 parameters (!!!)—we have three sets of coefficients for each of the three options, and those are all interacted with carpool, plus we have individual-specific offsets to each of those coefficients, plus all the \\(\\rho\\) terms in that massive correlation matrix.\n\nlength(get_variables(model_minivans_mega_mlm_brms))\n## [1] 5156\n\nSince we’re dealing with interaction terms, these raw log odds coefficients are even less helpful on their own. It’s nearly impossible to interpret these coefficients in any meaningful way—there’s no point in even trying to combine each of the individual parts of each effect (random parts, interaction parts, etc.). The only way we’ll be able to interpret these things is by making predictions.\n\nminivans_mega_marginalized &lt;- model_minivans_mega_mlm_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_mega_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi(.value)\n## # A tibble: 16 × 7\n##    .variable            .value  .lower  .upper .width .point .interval\n##    &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 Intercept           -0.127  -0.281   0.0321   0.95 median qi       \n##  2 cargo3ft             0.405   0.290   0.521    0.95 median qi       \n##  3 cargo3ft:carpoolyes  0.162  -0.0488  0.380    0.95 median qi       \n##  4 carpoolyes          -0.718  -1.00   -0.437    0.95 median qi       \n##  5 engelec             -1.59   -1.77   -1.42     0.95 median qi       \n##  6 engelec:carpoolyes   0.0852 -0.211   0.375    0.95 median qi       \n##  7 enghyb              -0.774  -0.910  -0.640    0.95 median qi       \n##  8 enghyb:carpoolyes   -0.0543 -0.307   0.196    0.95 median qi       \n##  9 price35             -0.812  -0.947  -0.675    0.95 median qi       \n## 10 price35:carpoolyes  -0.164  -0.414   0.0845   0.95 median qi       \n## 11 price40             -1.58   -1.74   -1.43     0.95 median qi       \n## 12 price40:carpoolyes  -0.248  -0.528   0.0320   0.95 median qi       \n## 13 seat7               -0.879  -1.03   -0.735    0.95 median qi       \n## 14 seat7:carpoolyes     1.14    0.872   1.41     0.95 median qi       \n## 15 seat8               -0.646  -0.794  -0.501    0.95 median qi       \n## 16 seat8:carpoolyes     1.13    0.859   1.40     0.95 median qi"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the non-hierarchical model earlier, we made marketing-style predictions by thinking of a product mix and figuring out the predicted utility and market share of each product in that mix. We can do the same thing here, but now we can incorporate individual-level characteristics too.\nHere’s our example product mix again, but this time we’ll repeat it twice—once with carpool set to “yes” and once with it set to “no”. This will let us see the predicted market share for each mix of products for a market of only carpoolers and only non-carpoolers.\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n)\n\nproduct_mix_carpool &lt;- bind_rows(\n  mutate(example_product_mix, carpool = \"yes\"),\n  mutate(example_product_mix, carpool = \"no\")\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\n\nWe can now go through the same process from earlier where we get logit-scale predictions for this smaller dataset and find the shares inside each draw + category (options 1, 2, and 3) + carpool status group.\n\ndraws_df &lt;- product_mix_carpool %&gt;% \n  add_linpred_draws(model_minivans_mega_mlm_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes across both levels of carpooling\n  group_by(.draw, .category, carpool) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nThis new dataset contains 576,000 (!!) rows: 6 products × 2 carpool types × 3 \\(\\mu\\)-specific sets of coefficients × 16,000 MCMC draws. We can summarize this to get posterior medians and credible intervals, making sure to find the average share across the three outcomes (choice 1, 2, and 3), or marginalizing across the outcome.\n\nshares_df %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  median_qi(share)\n## # A tibble: 12 × 8\n##    mix_type      carpool   share  .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 no      0.0217  0.0173  0.0272   0.95 median qi       \n##  2 6 2ft elec 40 yes     0.00946 0.00647 0.0136   0.95 median qi       \n##  3 7 2ft hyb 35  no      0.0434  0.0350  0.0532   0.95 median qi       \n##  4 7 2ft hyb 35  yes     0.0573  0.0425  0.0761   0.95 median qi       \n##  5 7 3ft gas 40  no      0.0655  0.0533  0.0798   0.95 median qi       \n##  6 7 3ft gas 40  yes     0.0976  0.0716  0.130    0.95 median qi       \n##  7 7 2ft hyb 30  no      0.0972  0.0824  0.114    0.95 median qi       \n##  8 7 2ft hyb 30  yes     0.148   0.118   0.183    0.95 median qi       \n##  9 8 2ft gas 30  no      0.265   0.239   0.293    0.95 median qi       \n## 10 8 2ft gas 30  yes     0.423   0.367   0.479    0.95 median qi       \n## 11 6 2ft gas 30  no      0.506   0.472   0.540    0.95 median qi       \n## 12 6 2ft gas 30  yes     0.261   0.223   0.303    0.95 median qi\n\nAnd we can plot them:\n\nshares_df %&gt;% \n  mutate(carpool = case_match(carpool, \"no\" ~ \"Non-carpoolers\", \"yes\" ~ \"Carpoolers\")) %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ggplot(aes(x = share, y = mix_type, slab_alpha = carpool)) +\n  stat_halfeye(normalize = \"groups\", fill = clrs[10]) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(1, 0.4),\n    guide = \"none\"\n  ) +\n  facet_wrap(vars(carpool)) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\n\nThis is so cool! In general, the market share for these six hypothetical products is roughly the same across carpoolers and non-carpoolers, with one obvious exception—among non-carpoolers, the $30,000 8-passenger gas minivan with 2 feet of space has 26% of the market, while among carpoolers it has 42%. Individuals who carpool apparently really care about the number of passengers their vehicle can carry."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nTo find the average marginal component effects (AMCEs), or the causal effect of moving one of these features to another value, holding all other variables constant, we can go through the same process as before. We’ll calculate the predicted probabilities of choosing option 0, 1, 2, and 3 across a full grid of all the combinations of feature levels and carpool status. We’ll then filter those predictions to only look at option 0 and reverse the predicted probabilities. Again, that feels weird, but it’s a neat little trick—if there’s a 33% chance that someone will select a specific combination of features, that would imply a 66% chance of not selecting it and an 11% chance of selecting it when it appears in option 1, option 2, and option 3. Rather than adding the probabilities within those three options together, we can do 100% − 66% to get the same 33% value, only it’s automatically combined.\nEarlier we had 54 combinations—now we have 108 (54 × 2). We’ll set resp.id to one that’s not in the dataset (201) so that these effects all deal with a generic hypothetical respondent (we could also do some fancy “integrating out” work and find population-level averages; see here for more about that).\n\nnewdata_all_combos_carpool &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price, carpool) %&gt;% \n  mutate(resp.id = 201)\nnewdata_all_combos_carpool\n## # A tibble: 108 × 6\n##    seat  cargo eng   price carpool resp.id\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;\n##  1 6     2ft   gas   30    no          201\n##  2 6     2ft   gas   30    yes         201\n##  3 6     2ft   gas   35    no          201\n##  4 6     2ft   gas   35    yes         201\n##  5 6     2ft   gas   40    no          201\n##  6 6     2ft   gas   40    yes         201\n##  7 6     2ft   hyb   30    no          201\n##  8 6     2ft   hyb   30    yes         201\n##  9 6     2ft   hyb   35    no          201\n## 10 6     2ft   hyb   35    yes         201\n## # ℹ 98 more rows\n\nNext we can plug this grid into the model, filter to only keep option 0, and reverse the predictions:\n\nall_preds_brms_carpool &lt;- model_minivans_mega_mlm_brms %&gt;% \n  epred_draws(\n    newdata = newdata_all_combos_carpool,\n    re_formula = NULL, allow_new_levels = TRUE\n  ) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nThis thing has 1.7 million rows in it, so we need to group and summarize to do anything useful with it. We also need to marginalize across all the other covariates when grouping (i.e. if we want the estimates for passenger seat count across carpool status, we need to average out all the other covariates).\nTo test that this worked, here are the posterior marginal means for seat count:\n\npreds_seat_carpool_marginalized &lt;- all_preds_brms_carpool %&gt;% \n  # Marginalize out the other covariates in each draw\n  group_by(seat, carpool, .draw) %&gt;% \n  summarize(avg = mean(.epred))\n\npreds_seat_carpool_marginalized %&gt;% \n  group_by(seat, carpool) %&gt;% \n  median_qi(avg)\n## # A tibble: 6 × 8\n##   seat  carpool   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     no      0.425  0.377  0.483   0.95 median qi       \n## 2 6     yes     0.287  0.245  0.339   0.95 median qi       \n## 3 7     no      0.262  0.205  0.337   0.95 median qi       \n## 4 7     yes     0.335  0.267  0.419   0.95 median qi       \n## 5 8     no      0.305  0.225  0.409   0.95 median qi       \n## 6 8     yes     0.378  0.288  0.488   0.95 median qi\n\nThose credible intervals all look reasonable (i.e. not ranging from 5% to 80% or whatever), but it’s hard to see any trends from just this table. Let’s plot it:\n\npreds_seat_carpool_marginalized %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) + \n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\n\nNeat! The average posterior predicted probability of choosing six seats is substantially higher for carpoolers than for non-carpoolers, while the probability for seven and eight seats is bigger for carpoolers.\nWe’re most interested in the AMCE though, and not the marginal means, so we’ll use compare_levels() to find the carpool-specific differences between the effect of moving from 6 → 7 and 6 → seats:\n\namces_seat_carpool &lt;- preds_seat_carpool_marginalized %&gt;% \n  group_by(carpool) %&gt;% \n  compare_levels(variable = avg, by = seat, comparison = \"control\") \n\namces_seat_carpool %&gt;% \n  median_qi(avg)\n## # A tibble: 4 × 8\n##   carpool seat      avg  .lower   .upper .width .point .interval\n##   &lt;fct&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 no      7 - 6 -0.162  -0.231  -0.0877    0.95 median qi       \n## 2 no      8 - 6 -0.120  -0.219  -0.00669   0.95 median qi       \n## 3 yes     7 - 6  0.0478 -0.0247  0.129     0.95 median qi       \n## 4 yes     8 - 6  0.0912 -0.0114  0.207     0.95 median qi\n\nAmong carpoolers, the causal effect of moving from 6 → 7 passengers, holding all other features constant, is a 5ish percentage point increase in the probability of selecting the vehicle. The effect is bigger (9ish percentage points) when moving from 6 → 8.\nAmong non-carpoolers, the causal effect is reversed. Moving from 6 → 7 passengers causes a 16 percentage point decrease in the probability of selection, while moving from 6 → 8 causes a 12 percentage point decrease, holding all other features constant.\nThese effects are “significant” and have a 90–97% probability of being greater than zero for carpoolers and 98–99% probability of being less than zero for the non-carpoolers.\n\n# Calculate probability of direction\namces_seat_carpool %&gt;% \n  group_by(seat, carpool) %&gt;% \n  summarize(p_gt_0 = sum(avg &gt; 0) / n()) %&gt;% \n  mutate(p_lt_0 = 1 - p_gt_0)\n## # A tibble: 4 × 4\n## # Groups:   seat [2]\n##   seat  carpool   p_gt_0 p_lt_0\n##   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n## 1 7 - 6 no      0.000625 0.999 \n## 2 7 - 6 yes     0.908    0.0916\n## 3 8 - 6 no      0.0208   0.979 \n## 4 8 - 6 yes     0.961    0.0387\n\n\namces_seat_carpool %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\n\nHere are all the AMCEs across carpool status:\n\namces_minivan_carpool &lt;- bind_rows(\n  seat = all_preds_brms_carpool %&gt;% \n    group_by(seat, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms_carpool %&gt;% \n    group_by(cargo, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms_carpool %&gt;% \n    group_by(eng, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms_carpool %&gt;% \n    group_by(price, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_carpool %&gt;% \n  group_by(term, carpool, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 14 × 9\n##    term  carpool contrast       avg  .lower   .upper .width .point .interval\n##    &lt;chr&gt; &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 cargo no      3ft - 2ft   0.0750  0.0358  0.117     0.95 median qi       \n##  2 cargo yes     3ft - 2ft   0.102   0.0555  0.152     0.95 median qi       \n##  3 eng   no      elec - gas -0.288  -0.400  -0.147     0.95 median qi       \n##  4 eng   no      hyb - gas  -0.160  -0.208  -0.108     0.95 median qi       \n##  5 eng   yes     elec - gas -0.273  -0.389  -0.125     0.95 median qi       \n##  6 eng   yes     hyb - gas  -0.166  -0.223  -0.104     0.95 median qi       \n##  7 price no      35 - 30    -0.167  -0.223  -0.107     0.95 median qi       \n##  8 price no      40 - 30    -0.294  -0.354  -0.230     0.95 median qi       \n##  9 price yes     35 - 30    -0.203  -0.270  -0.131     0.95 median qi       \n## 10 price yes     40 - 30    -0.342  -0.412  -0.272     0.95 median qi       \n## 11 seat  no      7 - 6      -0.162  -0.231  -0.0877    0.95 median qi       \n## 12 seat  no      8 - 6      -0.120  -0.219  -0.00669   0.95 median qi       \n## 13 seat  yes     7 - 6       0.0478 -0.0247  0.129     0.95 median qi       \n## 14 seat  yes     8 - 6       0.0912 -0.0114  0.207     0.95 median qi\n\nAnd finally, here’s a polisci-style plot of all these AMCEs, which is so so neat. An individual’s carpooling behavior interacts with seat count (increasing the seat count causes carpoolers to select the minivan more often), and it also interacts a bit with cargo space (increasing the cargo space makes both types of individuals more likely to select the minivan, but moreso for carpoolers) and also with price (increasing the price makes both types of individuals less likely to select the minivan, but moreso for carpoolers). Switching from gas → hybrid and gas → electric has a negative effect on both types of consumers, and there’s no carpooling-based difference.\n\n\nExtract variable labels\nminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make plot\nposterior_amces_minivan_carpool_nested &lt;- amces_minivan_carpool %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_carpool &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_amces_minivan_carpool_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  # Make the missing carpool values be \"yes\" for the reference category\n  mutate(carpool = replace_na(carpool, \"yes\"))\n\nplot_data_minivan_carpool %&gt;% \n  ggplot(aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(slab_alpha = carpool), normalize = \"groups\") + \n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    title = \"AMCEs across respondent carpool status\",\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "August 15, 2023\n        \n        \n            Manually generate predicted values for logistic regression with matrix multiplication in R\n            \n                \n                    r\n                \n                \n                    statistics\n                \n                \n                    regression\n                \n            \n            This is like basic stats stuff, but I can never remember how to do it—here's how to use matrix multiplication to replicate the results of `predict()`\n             10.59350/qba9a-b3561\n        \n        \n            \n        \n    \n    \n                  \n            August 12, 2023\n        \n        \n            The ultimate practical guide to multilevel multinomial conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models\n             10.59350/2mz75-rrc46\n        \n        \n            \n        \n    \n    \n                  \n            July 28, 2023\n        \n        \n            How to fill maps with density gradients with R, {ggplot2}, and {sf}\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Fix overplotted points on maps by creating bins or filled desntiy gradients using R, {ggplot2}, and {sf}\n             10.59350/bsctw-0a955\n        \n        \n            \n        \n    \n    \n                  \n            July 25, 2023\n        \n        \n            The ultimate practical guide to conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, and {marginaleffects} to analyze conjoint data and find causal and descriptive quantities of interest, both frequentistly and Bayesianly\n             10.59350/xgwjy-dyj66\n        \n        \n            \n        \n    \n    \n                  \n            July 3, 2023\n        \n        \n            Road trip analysis! How to use and play with Google Location History in R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Learn how to use R to load and clean and play with all the location history data Google keeps about you and look at some neat plots and tables about our 5,000-mile summer road trip along the way\n             10.59350/24rwv-k9n62\n        \n        \n            \n        \n    \n    \n                  \n            June 1, 2023\n        \n        \n            How to make fancy road trip maps with R and OpenStreetMap\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Use R to get geocoded location and routing data from OpenStreetMap and explore our family's impending 5,000 mile road trip around the USA\n             10.59350/rgwda-0tv16\n        \n        \n            \n        \n    \n    \n                  \n            May 15, 2023\n        \n        \n            A guide to Bayesian proportion tests with R and {brms}\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    bayes\n                \n                \n                    brms\n                \n                \n                    stan\n                \n                \n                    surveys\n                \n                \n                    categorical data\n                \n            \n            Use R, Stan, and {brms} to calculate differences between categorical proportions in a principled Bayesian way\n             10.59350/kw2gj-kw740\n        \n        \n            \n        \n    \n    \n                  \n            April 26, 2023\n        \n        \n            Making Middle Earth maps with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n                \n                    nerdery\n                \n            \n            Explore Tolkien's Middle Earth with R-based GIS tools, including {ggplot2} and {sf}\n             10.59350/ccrtd-z3s22\n        \n        \n            \n        \n    \n    \n                  \n            March 21, 2023\n        \n        \n            How old was Aragorn in regular human years?\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    simulations\n                \n                \n                    brms\n                \n                \n                    nerdery\n                \n            \n            Use statistical simulation and a hidden table of Númenórean ages from Tolkien's unpublished works to convert Aragorn's Dúnedan years to actual human years\n             10.59350/e0855-b1171\n        \n        \n            \n        \n    \n    \n                  \n            January 9, 2023\n        \n        \n            One Simple Trick™ to create inline bibliography entries with Markdown and pandoc\n            \n                \n                    writing\n                \n                \n                    markdown\n                \n                \n                    citations\n                \n                \n                    pandoc\n                \n                \n                    zotero\n                \n            \n            By default, pandoc doesn't include full bibliographic references inline in documents, but with one tweak to a CSL file, you can create syllabus-like lists of citations with full references\n             10.59350/hwwgk-v9636\n        \n    \n    \n                  \n            January 8, 2023\n        \n        \n            How to migrate from BibDesk to Zotero for pandoc-based writing\n            \n                \n                    writing\n                \n                \n                    markdown\n                \n                \n                    citations\n                \n                \n                    pandoc\n                \n                \n                    zotero\n                \n            \n            Tips, tricks, and rationale for converting from a single big BibTeX file to a Zotero database\n             10.59350/cwrq4-m7h10\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "August 15, 2023\n        \n        \n            Manually generate predicted values for logistic regression with matrix multiplication in R\n            \n                \n                    r\n                \n                \n                    statistics\n                \n                \n                    regression\n                \n            \n            This is like basic stats stuff, but I can never remember how to do it—here's how to use matrix multiplication to replicate the results of `predict()`\n             10.59350/qba9a-b3561\n        \n        \n            \n        \n    \n    \n                  \n            August 12, 2023\n        \n        \n            The ultimate practical guide to multilevel multinomial conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models\n             10.59350/2mz75-rrc46\n        \n        \n            \n        \n    \n    \n                  \n            July 28, 2023\n        \n        \n            How to fill maps with density gradients with R, {ggplot2}, and {sf}\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Fix overplotted points on maps by creating bins or filled desntiy gradients using R, {ggplot2}, and {sf}\n             10.59350/bsctw-0a955\n        \n        \n            \n        \n    \n    \n                  \n            July 25, 2023\n        \n        \n            The ultimate practical guide to conjoint analysis with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    statistics\n                \n                \n                    brms\n                \n                \n                    stan\n                \n            \n            Learn how to use R, {brms}, and {marginaleffects} to analyze conjoint data and find causal and descriptive quantities of interest, both frequentistly and Bayesianly\n             10.59350/xgwjy-dyj66\n        \n        \n            \n        \n    \n    \n                  \n            July 3, 2023\n        \n        \n            Road trip analysis! How to use and play with Google Location History in R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Learn how to use R to load and clean and play with all the location history data Google keeps about you and look at some neat plots and tables about our 5,000-mile summer road trip along the way\n             10.59350/24rwv-k9n62\n        \n        \n            \n        \n    \n    \n                  \n            June 1, 2023\n        \n        \n            How to make fancy road trip maps with R and OpenStreetMap\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n            \n            Use R to get geocoded location and routing data from OpenStreetMap and explore our family's impending 5,000 mile road trip around the USA\n             10.59350/rgwda-0tv16\n        \n        \n            \n        \n    \n    \n                  \n            May 15, 2023\n        \n        \n            A guide to Bayesian proportion tests with R and {brms}\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    bayes\n                \n                \n                    brms\n                \n                \n                    stan\n                \n                \n                    surveys\n                \n                \n                    categorical data\n                \n            \n            Use R, Stan, and {brms} to calculate differences between categorical proportions in a principled Bayesian way\n             10.59350/kw2gj-kw740\n        \n        \n            \n        \n    \n    \n                  \n            April 26, 2023\n        \n        \n            Making Middle Earth maps with R\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    gis\n                \n                \n                    maps\n                \n                \n                    nerdery\n                \n            \n            Explore Tolkien's Middle Earth with R-based GIS tools, including {ggplot2} and {sf}\n             10.59350/ccrtd-z3s22\n        \n        \n            \n        \n    \n    \n                  \n            March 21, 2023\n        \n        \n            How old was Aragorn in regular human years?\n            \n                \n                    r\n                \n                \n                    tidyverse\n                \n                \n                    ggplot\n                \n                \n                    simulations\n                \n                \n                    brms\n                \n                \n                    nerdery\n                \n            \n            Use statistical simulation and a hidden table of Númenórean ages from Tolkien's unpublished works to convert Aragorn's Dúnedan years to actual human years\n             10.59350/e0855-b1171\n        \n        \n            \n        \n    \n    \n                  \n            January 9, 2023\n        \n        \n            One Simple Trick™ to create inline bibliography entries with Markdown and pandoc\n            \n                \n                    writing\n                \n                \n                    markdown\n                \n                \n                    citations\n                \n                \n                    pandoc\n                \n                \n                    zotero\n                \n            \n            By default, pandoc doesn't include full bibliographic references inline in documents, but with one tweak to a CSL file, you can create syllabus-like lists of citations with full references\n             10.59350/hwwgk-v9636\n        \n    \n    \n                  \n            January 8, 2023\n        \n        \n            How to migrate from BibDesk to Zotero for pandoc-based writing\n            \n                \n                    writing\n                \n                \n                    markdown\n                \n                \n                    citations\n                \n                \n                    pandoc\n                \n                \n                    zotero\n                \n            \n            Tips, tricks, and rationale for converting from a single big BibTeX file to a Zotero database\n             10.59350/cwrq4-m7h10\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2022",
    "text": "2022\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2021",
    "text": "2021\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2018",
    "text": "2018\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-6",
    "href": "blog/index.html#section-6",
    "title": "Blog",
    "section": "2017",
    "text": "2017\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-7",
    "href": "blog/index.html#section-7",
    "title": "Blog",
    "section": "2016",
    "text": "2016\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-8",
    "href": "blog/index.html#section-8",
    "title": "Blog",
    "section": "2013",
    "text": "2013\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-9",
    "href": "blog/index.html#section-9",
    "title": "Blog",
    "section": "2012",
    "text": "2012\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-10",
    "href": "blog/index.html#section-10",
    "title": "Blog",
    "section": "2011",
    "text": "2011\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-11",
    "href": "blog/index.html#section-11",
    "title": "Blog",
    "section": "2010",
    "text": "2010\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-12",
    "href": "blog/index.html#section-12",
    "title": "Blog",
    "section": "2009",
    "text": "2009\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-13",
    "href": "blog/index.html#section-13",
    "title": "Blog",
    "section": "2007",
    "text": "2007\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I’m an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University. I received a PhD in public policy and political science from Duke University’s Sanford School of Public Policy in 2017.\nI study how international NGOs work in authoritarian countries, and I received the 2016–2018 Emerging Scholar Dissertation Award from the International Society for Third Sector Research (ISTR). I do research in public administration and policy, nonprofit management, international relations, and comparative politics.\nI teach courses on program evaluation and causal inference, statistics and data science, data visualization, economics, and science communication. I’m also a certified RStudio instructor and a Posit Academy mentor and absolutely love teaching how to use R and the tidyverse."
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-links",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-links",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration (research question #2)"
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#abstract",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#abstract",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Abstract",
    "text": "Abstract\nThe phenomenon of closing civic space has adversely impacted INGO funding. We argue that individual private donors can be important in sustaining the operations of INGOs working in repressive contexts. Individual donors do not use the same performance-based metrics as official aid donors. Rather, trust can be an important component of individual donor support for nonprofits working towards difficult goals. How does trust in charitable organizations influence individuals’ preferences to donate, especially when these groups face crackdown? Using a simulated market for philanthropic donations based on data from a nationally representative sample of individuals in the United States who regularly donate to charity, we find that trust in INGOs matters substantially in shaping donor preferences. Donor profiles with high levels of social trust are likely to donate to INGOs with friendly relationships with host governments. This support holds steady if INGOs face criticism or crackdown. In contrast, donor profiles with lower levels of social trust prefer to donate to organizations that do not face criticism or crackdown abroad. The global crackdown on NGOs may thus possibly sour NGOs’ least trusting individual donors. Our findings have practical implications for INGOs raising funds from individuals amid closing civic space."
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships\n\n\n\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships"
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Data and code",
    "text": "Data and code\nThe project is reproducible with R code available at GitHub. Follow the instructions there to install all supporting files and R packages.\nThis project includes the following data files:\n\ndata/raw_data/final_data.rds: Original results from the Qualtrics survey. This is hosted at OSF because of its size. Running targets::tar_make(survey_results_file) will download the .rds file from OSF and place it in data/raw_data. The code for cleaning and processing this data is part of a separate project, “Why Donors Donate”.\ndata/derived_data/survey_results.csv: CSV version of the survey data.\ndata/derived_data/survey_results.yaml: YAML metadata describing the syntax of the survey data.\ndata/raw_data/posterior_draws/public_political_social_charity_demo.rds: Gamma (Γ) coefficients from our multilevel Bayesian model. This is hosted at OSF because of its size. Running targets::tar_make(gamma_draws_file) will download the .rds file from OSF and place it in data/raw_data/posterior_draws. The code for running this model is part of a separate project, “Why Donors Donate”.\ndata/raw_data/Market Simulator Version 01.xlsx: An interactive Excel version of the market simulator to help demonstrate the intuition behind all the moving parts of the simulation."
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#citation",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#citation",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryDotsonHeiss:2021,\n    Author = {Suparna Chaudhry and Marc Dotson and Andrew Heiss},\n    Doi = {10.1111/1758-5899.12984},\n    Journal = {Global Policy},\n    Month = {7},\n    Number = {S5},\n    Pages = {45--58},\n    Title = {Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy},\n    Volume = {12},\n    Year = {2021}}"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Abstract",
    "text": "Abstract\nAn increasing number of countries have recently cracked down on non-governmental organizations (NGOs). Much of this crackdown is sanctioned by law and represents a bureaucratic form of repression that could indicate more severe human rights abuses in the future. This is especially the case for democracies, which unlike autocracies, may not aggressively attack civic space. We explore if crackdowns on NGOs predict broader human rights repression. Anti-NGO laws are among the most subtle means of repression and attract lesser domestic and international condemnation compared to the use of violence. Using original data on NGO repression, we test whether NGO crackdown is a predictor of political terror, and violations of physical integrity rights and civil liberties. We find that while de jure anti-NGO laws provide little information in predicting future repression, their patterns of implementation—or de facto civil society repression—predicts worsening respect for physical integrity rights and civil liberties."
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 6: Marginal effects of changing levels of civil society repression on the probability of specific levels of political terror and predicted latent human rights values\n\n\n\n\n\nFigure 7: The disconnect between Egypt’s de jure 2002 law and the widespread de facto repression of civil society a decade later"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2022,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1080/14754835.2022.2030205},\n    Journal = {Journal of Human Rights},\n    Number = {2},\n    Pages = {123--140},\n    Title = {NGO Repression as a Predictor of Worsening Human Rights Abuses},\n    Volume = {21},\n    Year = {2022}}"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html",
    "href": "research/articles/heiss-2019-taking-control/index.html",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#important-links",
    "href": "research/articles/heiss-2019-taking-control/index.html#important-links",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#abstract",
    "href": "research/articles/heiss-2019-taking-control/index.html#abstract",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Abstract",
    "text": "Abstract\nA wave of legislative and regulatory crackdown on international nongovernmental organizations (INGOs) has constricted the legal environment for foreign advocacy groups interested in influencing domestic and global policy. Although the legal space for advocacy is shrinking, many INGOs have continued their work and found creative ways to adapt to these restrictions, sometimes even reshaping the regulatory environments of their target countries in their favor. In this article, I explore what enables INGOs to cope with and reshape their regulatory environments. I bridge international relations and interest group literatures to examine the interaction between INGO resource configurations and institutional arrangements. I argue that specific resource and managerial characteristics provide organizations with ‘programmatic flexibility’ that enables groups to adjust their strategies without changing their core mission. I illustrate and test this argument with case studies of Article 19 and AMERA International and demonstrate how organizations with high programmatic flexibility can navigate regulations and shape policy in their target country, while those without this flexibility are shut out of policy discussions and often the target country itself. I conclude by exploring how the interaction between internal characteristics and institutional environments shape and constrain the effects of interest groups in global governance."
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#important-figures",
    "href": "research/articles/heiss-2019-taking-control/index.html#important-figures",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects\n\n\n\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#citation",
    "href": "research/articles/heiss-2019-taking-control/index.html#citation",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{Heiss:2019,\n    Author = {Andrew Heiss},\n    Doi = {10.1057/s41309-019-00061-0},\n    Journal = {Interest Groups and Advocacy},\n    Month = {9},\n    Number = {3},\n    Pages = {356--375},\n    Title = {Taking Control of Regulations: How International Advocacy {NGOs} Shape the Regulatory Environments of their Target Countries},\n    Volume = {8},\n    Year = {2019}}"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html",
    "href": "research/articles/heiss-kelley-2017/index.html",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#abstract",
    "href": "research/articles/heiss-kelley-2017/index.html#abstract",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#figure",
    "href": "research/articles/heiss-kelley-2017/index.html#figure",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Figure",
    "text": "Figure\nFigure 2: The dual environmental constraints confronting INGOs\n\n\n\nFigure 2: The dual environmental constraints confronting INGOs"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#books-reviewed",
    "href": "research/articles/heiss-kelley-2017/index.html#books-reviewed",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah Sunn Bush, The Taming of Democracy Assistance: Why Democracy Promotion Does Not Confront Dictators (Cambridge, UK: Cambridge University Press, 2015), doi: 10.1017/cbo9781107706934.\nJessica C. Teets, Civil Society under Authoritarianism: The China Model (New York: Cambridge University Press, 2014), doi: 10.1017/cbo9781139839396.\nAmanda Murdie, Help or Harm: The Human Security Effects of International NGOs (Stanford: Stanford University Press, 2014), doi: 10.2307/j.ctvqsdpq8."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#citation",
    "href": "research/articles/heiss-kelley-2017/index.html#citation",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n    Author = {Andrew Heiss and Judith G. Kelley},\n    Doi = {10.1086/691218},\n    Journal = {Journal of Politics},\n    Month = {4},\n    Number = {2},\n    Pages = {732--41},\n    Title = {Between a Rock and a Hard Place: International {NGOs} and the Dual Pressures of Donors and Host Governments},\n    Volume = {79},\n    Year = {2017}}"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html",
    "href": "research/articles/peck-heiss-2021/index.html",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#important-links",
    "href": "research/articles/peck-heiss-2021/index.html#important-links",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#abstract",
    "href": "research/articles/peck-heiss-2021/index.html#abstract",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Abstract",
    "text": "Abstract\nEcological theorists have generated several yet unresolved disputes that try to untangle the difficulty in understanding the nature of complex ecological communities. In this paper, we combine two recent theoretical approaches that used together suggest a promising way to consider how evolutionary and ecological processes may be used to frame a general theory of community ecology and its functional stability. First, we consider the theoretical proposal by Mark Vellend (2016) to focus on a small set of higher-level evolutionary and ecological processes that act on species within an ecological community. These processes provide a basis for ecological theory similar to the way in which theoretical population genetics has focused on a small set of mathematical descriptions to undergird its theory. Second, we explore ideas that might be applied to ecosystem functioning developed by Alvaro Moreno and Matteo Mossio’s (2015) work on how biologically autonomous systems emerge from closure of relevant constraints. To explore the possibility that combining these two ideas may provide a more general theoretical understanding of ecological communities, we have developed a stochastic, agent-based model, with agents representing species, that explores the potential of using evolutionary and ecological processes as a constraint on the flow of species through an ecosystem. We explore how these ideas help illuminate aspects of stability found in many ecological communities. These agent-based modeling results provide in-principle arguments that suggest that constraint closure, using evolutionary and ecological processes, explains general features of ecological communities. In particular, we find that our model suggests a perspective useful in explaining repeated patterns of stability in ecological evenness, species turnover, species richness, and in measures of fitness."
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#important-figure",
    "href": "research/articles/peck-heiss-2021/index.html#important-figure",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent, and with uniform random values of parameters pulled from their possible values in the model iterations. The whiskers in the box plot show the range of variation. The actual data used to construct the boxplots are shown as fine gray dots to the right of each box plot. (a) Effect of the number of constraints on landscape fitness. (b) Effect of the number of constraints on ecological evenness. (c) Effect of the number of constraints on the average cell species richness (number of species defined as functional groups) present in the cell-niche. (d) Effect of the number of constraints on the difference between turnover in mutualistic linked and unlinked species. The turnover rate is the number of new species created at each time step in each cell.\n\n\n\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#citation",
    "href": "research/articles/peck-heiss-2021/index.html#citation",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{PeckHeiss:2021,\n    Author = {Steven L. Peck and Andrew Heiss},\n    Doi = {10.1111/oik.07621},\n    Journal = {Oikos},\n    Month = {9},\n    Number = {9},\n    Pages = {1425--1439},\n    Title = {Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?},\n    Volume = {130},\n    Year = {2021}}"
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html",
    "href": "research/articles/witesman-heiss-2016/index.html",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#abstract",
    "href": "research/articles/witesman-heiss-2016/index.html#abstract",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#figure",
    "href": "research/articles/witesman-heiss-2016/index.html#figure",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#citation",
    "href": "research/articles/witesman-heiss-2016/index.html#citation",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{WitesmanHeiss:2016,\n    Author = {Eva Witesman and Andrew Heiss},\n    Doi = {10.1007/s11266-016-9684-5},\n    Journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    Month = {8},\n    Number = {4},\n    Pages = {1500--1528},\n    Title = {Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives},\n    Volume = {28},\n    Year = {2016}}"
  },
  {
    "objectID": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html",
    "href": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#important-links",
    "href": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#important-links",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#abstract",
    "href": "research/working-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#abstract",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "Abstract",
    "text": "Abstract\nAnalysts often transform their models’ parameter estimates to report more meaningful and interpretable quantities of interest. This article presents a simple conceptual framework to describe a vast array of estimands which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, an R package which offers a simple and powerful interface to compute all of those quantites, and to conduct hypothesis tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R packages; and it supports over 70 classes of models, including Generalized Linear, Generalized Additive, Mixed Effects, and Bayesian models."
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "",
    "text": "GitHub repository\nExperiment preregistration (research question #1)"
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#important-links",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#important-links",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "",
    "text": "GitHub repository\nExperiment preregistration (research question #1)"
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#abstract",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#abstract",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "Abstract",
    "text": "Abstract\nAs space for civil society has closed around the world, transnational NGOs have faced a crisis of funding. We explore how NGOs have shifted from traditionally Northern funding sources toward grassroots private philanthropic money. How do individual donors in the US feel about donating to legally besieged NGOs abroad? Do legal restrictions on NGOs influence donors’ decision to donate? We use a conjoint survey experiment to argue that domestic political environments of NGO host countries influence preferences of private donors and that legal crackdowns on NGOs serve as a heuristic of organizational deservingness."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#important-links",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#important-links",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#abstract",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#abstract",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "Abstract",
    "text": "Abstract\nForeign donors routinely use nongovernmental organizations (NGOs) to deliver foreign aid. However, states are increasingly relying on repressive legislation to crack down on NGOs within their borders. How have foreign aid donors responded to this legal crackdown on NGOs? Using original data from all countries that received aid from 1981–2012, we assess the impact of anti-NGO laws on total flows of official foreign aid, the nature of projects funded, and the channels used for distributing this aid. Overall, we find that donors scale back their operations in repressive countries. However, rather than completely withdraw, we find that donors redirect funds within restrictive countries by decreasing funds for politically sensitive issues, and channeling more aid through domestic rather than foreign NGOs. While our findings challenge existing notions of foreign aid running on “autopilot,” they also have worrying implications for Western donors and domestic NGOs working on contentious issues."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#figure",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#figure",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "Figure",
    "text": "Figure\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution.\n\n\n\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#bibtex-citation",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#bibtex-citation",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2023,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Note = {Working paper},\n    Title = {Are Donors Really Responding? Analyzing the Impact of Global Restrictions on {NGO}s},\n    Year = {2023}}"
  },
  {
    "objectID": "research/working-papers/heiss-mission-money-environment/index.html",
    "href": "research/working-papers/heiss-mission-money-environment/index.html",
    "title": "‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-mission-money-environment/index.html#important-links",
    "href": "research/working-papers/heiss-mission-money-environment/index.html#important-links",
    "title": "‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-program-capture-ingos/index.html",
    "href": "research/working-papers/heiss-program-capture-ingos/index.html",
    "title": "‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulations",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-program-capture-ingos/index.html#important-links",
    "href": "research/working-papers/heiss-program-capture-ingos/index.html#important-links",
    "title": "‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulations",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html#important-links",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html#important-links",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html#abstract",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html#abstract",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "Abstract",
    "text": "Abstract\nDiscovering causal relationships and testing theoretical mechanisms is a core endeavor of social science. Randomized experiments have long served as a gold standard for making valid causal inferences, but most of the data social scientists work with is observational and non-experimental. However, with newer methodological developments in economics, political science, epidemiology, and other disciplines, an increasing number of studies in social science make causal claims with observational data. As a newer interdisciplinary field, however, nonprofit studies has lagged behind other disciplines in its use of observational causal inference. In this article, we present a hands-on introduction and guide to design-based observational causal inference methods. We first review and categorize all studies making causal claims in top nonprofit studies journals over the past decade to illustrate the field’s current of experimental and observational approaches to causal inference. We then introduce a framework for modeling and identifying causal processes using directed acyclic graphs (DAGs) and provide a walk-through of the assumptions and procedures for making inferences with a range of different methods, including matching, inverse probability weighting, difference-in-differences, regression discontinuity designs, and instrumental variables. We illustrate each approach with synthetic and empirical examples and provide sample R and Stata code for implementing these methods. We conclude by encouraging scholars and practitioners to make more careful and explicit causal claims in their observational empirical research, collectively developing and improving quantitative work in the broader field of nonprofit studies."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Fall 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Fall 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-1",
    "href": "teaching/index.html#section-1",
    "title": "Teaching",
    "section": "2022–23",
    "text": "2022–23\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2023\n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Comparative Public Administration \n            \n            \n                PMAP 8441 | \n                Georgia State University\n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n            \n                 Spring 2023\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Bayesian Statistics Readings \n            \n            \n                PMAP 8911 | \n                Georgia State University\n            \n            Independent readings course on Bayesian statistics with R and Stan\n            \n                 Spring 2023\n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Introduction to Nonprofits \n            \n            \n                PMAP 3210 | \n                Georgia State University\n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n            \n                 Fall 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2023 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2023 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-2",
    "href": "teaching/index.html#section-2",
    "title": "Teaching",
    "section": "2021–22",
    "text": "2021–22\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2022\n                 Fall 2021\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Introduction to Nonprofits \n            \n            \n                PMAP 3210 | \n                Georgia State University\n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n            \n                 Spring 2022\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2022 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2022 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-3",
    "href": "teaching/index.html#section-3",
    "title": "Teaching",
    "section": "2020–21",
    "text": "2020–21\n\n\n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2021 (asynchronous online)\n                 Fall 2020 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 Summer 2021 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2021 (asynchronous online)\n                 Spring 2021 (asynchronous online)\n                 Fall 2020 (asynchronous online)\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-4",
    "href": "teaching/index.html#section-4",
    "title": "Teaching",
    "section": "2019–20",
    "text": "2019–20\n\n\n    \n        \n            \n        \n        \n            \n                Data Visualization with R \n            \n            \n                PMAP 8921 | \n                Georgia State University\n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n            \n                 May 2020 (asynchronous online)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Program Evaluation for Public Service \n            \n            \n                PMAP 8521 | \n                Georgia State University\n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n            \n                 Spring 2020\n                 Fall 2019\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Microeconomics for Public Policy \n            \n            \n                PMAP 8141 | \n                Georgia State University\n            \n            Learn how to understand, speak, and do economics in the public sector\n            \n                 Summer 2020 (asynchronous online)\n                 Fall 2019\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-5",
    "href": "teaching/index.html#section-5",
    "title": "Teaching",
    "section": "2017–19",
    "text": "2017–19\n\n\n    \n        \n            \n        \n        \n            \n                Economy, Society, and Public Policy \n            \n            \n                MPA 612 | \n                Brigham Young University\n            \n            \n            \n                 Winter 2019\n                 Winter 2019 (executive MPA)\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Science and Statistics for Public Management \n            \n            \n                MPA 630 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization \n            \n            \n                MPA 635 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Economic Decision Making for Managers \n            \n            \n                MPA 612 | \n                Brigham Young University\n            \n            \n            \n                 Winter 2018\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Telling Stories with Data \n            \n            \n                Bus M 491R | \n                Brigham Young University\n            \n            \n            \n                 Fall 2017\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Data Visualization \n            \n            \n                MPA 635 | \n                Brigham Young University\n            \n            \n            \n                 Fall 2017\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-6",
    "href": "teaching/index.html#section-6",
    "title": "Teaching",
    "section": "2007–15",
    "text": "2007–15\n\n\n    \n        \n            \n        \n        \n            \n                Stats in Real Life \n            \n            \n                PubPol 590 | \n                Duke University\n                | TA, as PhD student\n            \n            \n            \n                 Fall 2014\n                 Spring 2014\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Policy Analysis \n            \n            \n                PubPol 803/807 | \n                Duke University\n                | TA, as PhD student\n            \n            \n            \n                 Fall 2015\n                 Fall 2013\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Statistical Analysis for Public Administrators \n            \n            \n                PMGT 630 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Summer 2012\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                International Development Field Study \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Winter and Spring 2012\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Decision Modeling and Analysis \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                | TA, as MPA student\n            \n            \n            \n                 Winter 2012\n                 Fall 2011\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Print Publishing \n            \n            \n                CHum 260 | \n                Brigham Young University\n                | Instructor, as undergraduate\n            \n            \n            \n                 Winter 2008\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                First-year Arabic \n            \n            \n                Arabic 101 | \n                Brigham Young University\n                | Instructor, as undergraduate\n            \n            \n            \n                 Winter 2007\n                 Fall 2006\n            \n        \n    \n\n\nNo matching items"
  }
]